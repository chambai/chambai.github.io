<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小白的网志空间</title>
  
  <subtitle>云计算|编程|读书|思维|认知</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chambai.github.io/"/>
  <updated>2019-02-21T14:34:11.022Z</updated>
  <id>https://chambai.github.io/</id>
  
  <author>
    <name>bike</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes 笔记 11 Pod 扩容与缩容 双十一前后的忙碌</title>
    <link href="https://chambai.github.io/2018/09/29/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_11_Pod_%E6%89%A9%E5%AE%B9%E4%B8%8E%E7%BC%A9%E5%AE%B9_%E5%8F%8C%E5%8D%81%E4%B8%80%E5%89%8D%E5%90%8E%E7%9A%84%E5%BF%99%E7%A2%8C/"/>
    <id>https://chambai.github.io/2018/09/29/云计算/Kubernetes_笔记_11_Pod_扩容与缩容_双十一前后的忙碌/</id>
    <published>2018-09-29T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:11.022Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/13.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 11 篇。</p><p>资源的伸缩在云计算环境中是至关重要的，云计算的动机就是企图提高资源的利用率，在用户请求高峰期的时候能够对资源进行横向扩容，反之，当用户请求回落低谷的时候，能够及时缩减资源，避免资源的浪费。</p><a id="more"></a><p>这就像双十一的时候，随着用户不断地涌入，阿里后台需要不断调配更多的资源来支撑用户大量的请求，当过了双十一当天，再慢慢缩减资源的使用。</p><p>Kubernetes 作为一个集群管理系统，提供了两种资源伸缩的方式：手动和自动。本文先来看手动方式。</p><p>Kubernetes 的资源伸缩本质上指的是 Pod 的扩容和缩容（scale up/down），也就是增加或减少 Pod 的副本数。</p><p>手动的方式是使用 <code>kubectl scale</code> 命令手动进行，或者基于 YAML 配置来实现。</p><p>首先，定义一个 <code>nginx-deployment.yaml</code> 配置文件：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web_server</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        images: nginx:1.12.1</span><br></pre></td></tr></table></figure><p>其中定义了 3 个副本，执行 <code>kubectl create -f nginx-deployment.yaml</code> 创建 Pod。</p><center><img src="/images/k8s/pod_scale.png" alt=""></center><p>如果现在遇到高峰请求，我们急需进行扩容，执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment nginx-deployment --replicas 5</span><br></pre></td></tr></table></figure></p><p>将 Pod 扩增到 5 个。</p><center><img src="/images/k8s/pod_scale_up.png" alt=""></center><p>其中，用 <code>--replicas</code> 来指示增缩的数量，对于缩容，将 <code>--replicas</code> 设置为比当前 Pod 副本数量更小的数字即可，比如缩容到 2 个如下：</p><center><img src="/images/k8s/pod_scale_down.png" alt=""></center><p>可以看到，Pod 销毁会经历一个 <code>Terminating</code> 的过程，最终 3 个副本被删除，只保留了 2 个副本。</p><p>以上是通过命令的形式来实现手动的扩容和缩容，我们也可以修改 YAML 配置文件中的 <code>replicas</code> 来实现，只要修改完之后执行 <code>kubectl apply</code> 即可。</p><p>OK，本文到此为止，下文我们再来 Pod 伸缩的另一种方式——自动扩容和缩容。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/13.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;



&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 11 篇。&lt;/p&gt;
&lt;p&gt;资源的伸缩在云计算环境中是至关重要的，云计算的动机就是企图提高资源的利用率，在用户请求高峰期的时候能够对资源进行横向扩容，反之，当用户请求回落低谷的时候，能够及时缩减资源，避免资源的浪费。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 10 Job 机器人加工厂</title>
    <link href="https://chambai.github.io/2018/09/26/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_10_Job_%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A0%E5%B7%A5%E5%8E%82/"/>
    <id>https://chambai.github.io/2018/09/26/云计算/Kubernetes_笔记_10_Job_机器人加工厂/</id>
    <published>2018-09-26T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:15.878Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/12.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 10 篇。</p><p>通常，我们在执行任务时，会启用多个服务，有些任务需要长时间运行，全天 24 小时不中断，所以一般会启用 Daemon 类的 服务；而有些任务则只需要短暂执行，任务执行完，服务就没有存在的必要了。</p><a id="more"></a><p>容器提供服务运行的环境，根据任务持续运行的时间，将容器分为两类：服务类容器和工作类容器。</p><p>服务类容器需要一直运行来提供持续性的服务，而工作类容器则是运行一次性的任务，任务完成后便会退出。</p><p>前面学习的 Deployment、ReplicaSet 和 DaemonSet 都用于管理服务类容器，而工作类容器则由本文要讲得 Job 来管理。</p><p>Job 多用于执行一次性的任务，批处理任务等，Job 就像是现代化机械加工厂的机器人，当有任务来的时候，便会启动，按照预先设定好的程序执行任务，直至任务执行完，便会进入休眠状态。</p><p>进一步，Job 根据任务的类型和执行的动作又分为以下几类：   </p><ul><li>单 Job 单任务：只启动一个 Job 来完成任务，同时 Job 只启用一个 Pod ，适用于简单的任务。</li><li>多 Job 多任务：启动多个 Job 来处理批量任务，每个任务对应一个 Job，Pod 的数量可以自定义。</li><li>单 Job 多任务：采用一个任务队列来存放任务，启动一个 Job 作为消费者来处理这些任务，Job 会启动多个 Pod，Pod 的数量可以自定义。</li><li>定时 Job：也叫 CronJob，启动一个 Job 来定时执行任务，类似 Linux 的 Crontab 程序。</li></ul><p>上述 Job 的分类需要注意两点：</p><p>1）Job 执行失败的重启策略；Job 执行的是一次性的任务，但也不保证一定能执行成功，如果执行失败，应该怎么处理？这个是由前面所讲的 Pod 重启策略来决定的。在 Job Controller 中，只允许定义两种策略：  </p><ul><li>Never：Pod 执行失败，不会重启该 Pod，但会根据 Job 定义的期望数重新创建 Pod。</li><li>OnFailure：Pod 执行失败，则会尝试重启该 Pod。</li></ul><p>两种策略尝试的次数由 <code>spec.backoffLimits</code> 来限制，默认是 6 次（K8S 1.8.0 新加的特性）。</p><p>2）批量任务的多次并行处理的限制；对于批量任务，通常是一个 Pod 对应一个任务，但有时为了加快任务的处理，会启动多个 Pod 来并行处理单个任务。可以通过下面两个参数来设置并行度：</p><ul><li><code>spec.completions</code>：总的启动 Pod 数，只有当所有 Pod 执行成功结束，任务才结束。</li><li><code>spec.parallelism</code>：每个任务对应的 Pod 的并行数，当有一个 Pod 执行成功结束，该任务就执行结束。</li></ul><p>下面通过几个例子来实践一下上面的几种 Job 类别。</p><h3 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h3><h4 id="单-Job-单-Pod-执行一次性任务"><a href="#单-Job-单-Pod-执行一次性任务" class="headerlink" title="单 Job 单 Pod 执行一次性任务"></a>单 Job 单 Pod 执行一次性任务</h4><p>首先，定义 Job 的 yaml 配置文件 myjob.yaml：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: myjob</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: myjob</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: hello</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;echo&quot;, &quot;hello, I&apos;m CloudDeveloper, Welcome&quot;]</span><br><span class="line">      restartPolicy: Never</span><br></pre></td></tr></table></figure><p>执行 <code>kubectl create -f myjob.yaml</code> 创建 job 对象：</p><center><img src="/images/k8s/job-yaml.png" alt=""></center><p>可以看到期望创建的 Job 数为 1，成功执行的 Job 数也为 1，这表明该 Job 已经执行完任务退出了。这个 Job 执行的任务就是创建一个 Pod，Pod 中创建一个 busybox 容器，并进入容器输出一段字符串：<strong>“hello, I’m CloudDeveloper, Welcome”</strong>。</p><p>查看一下 Pod 的状态：   </p><center><img src="/images/k8s/job-pod.png" alt=""></center><p>可以看到，该 Pod 的状态为 <code>Completed</code>，表示它已经执行完任务并成功退出了。那怎么看该任务的执行结果呢？可以执行 <code>kubectl logs myjob</code> 调出该 Pod 的历史执行信息进行查看：</p><center><img src="/images/k8s/job-logs.png" alt=""></center><p>看到历史输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hello, I&apos;m CloudDeveloper, Welcome</span><br></pre></td></tr></table></figure><p>以上是执行成功的情况，如果执行失败，会根据 <code>restartPolicy</code> 进行重启，重启的方式上面也说了。大家可以自己实践下。</p><h4 id="多-Job-多-Pod-执行批量任务"><a href="#多-Job-多-Pod-执行批量任务" class="headerlink" title="多 Job 多 Pod 执行批量任务"></a>多 Job 多 Pod 执行批量任务</h4><p>首先，定义 Job 的 yaml 模板文件 job.yaml.txt，然后再根据这个模板文件创建多个 Job yaml 文件。模板文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: work-item-$ITEM</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: job</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: c</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo work item $ITEM &amp;&amp; sleep 2&quot;]</span><br><span class="line">      restartPolicy: Never</span><br></pre></td></tr></table></figure><p>其中，<code>$ITEM</code> 作为各个 Job 项的标识。接着，使用以下脚本，根据 Job 模板创建三个 Job 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">for i in app book phone</span><br><span class="line">do</span><br><span class="line">  cat myjob_tmp.yaml | sed &quot;s/\$ITEM/$i/g&quot; &gt; ./jobs/job-$i.yaml</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>最后，创建三个 Job 对象，如下：</p><center><img src="/images/k8s/job-multi.png" alt=""></center><h4 id="单-Job-多-Pod-执行批量任务"><a href="#单-Job-多-Pod-执行批量任务" class="headerlink" title="单 Job 多 Pod 执行批量任务"></a>单 Job 多 Pod 执行批量任务</h4><p>这种方式是用一个队列来存放任务，然后启动一个 Job 来执行任务，Job 可以根据需求启动多个 Pod 来承载任务的执行。定义下面的配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: myjob</span><br><span class="line">spec:</span><br><span class="line">  completions: 6</span><br><span class="line">  parallelism: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: myjob</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: hello</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;echo&quot;, &quot;hello CloudDeveloper&quot;]</span><br><span class="line">      restartPolicy: OnFailure</span><br></pre></td></tr></table></figure><p>这里用到了上面说的两个参数：<code>completions</code> 和 <code>parallelism</code>，表示每次并行运行两个 Pod，直到总共 6 个 Pod 成功运行完成。如下：</p><center><img src="/images/k8s/job-para.png" alt=""></center><p>可以看到 DESIRED 和 SUCCESSFUL 最终均为 6，符合预期，实际上也有 6 个 Pod 成功运行并退出，呈 <code>Completed</code> 状态。</p><p>随便查看其中一个 Pod 的历史执行情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl logs myjob-5lfnp</span><br><span class="line">hello CloudDeveloper</span><br></pre></td></tr></table></figure><h4 id="定时任务-CronJob"><a href="#定时任务-CronJob" class="headerlink" title="定时任务 CronJob"></a>定时任务 CronJob</h4><p>定义一个 CronJob 配置文件，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  schedule: &quot;*/1 * * * *&quot;</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: hello</span><br><span class="line">            images: busybox</span><br><span class="line">            command: [&quot;echo&quot;, &quot;Hello CloudDeveloper&quot;]</span><br><span class="line">          restartPolicy: OnFailure</span><br></pre></td></tr></table></figure><p>kind 类型为 CronJob，<code>spec.schedule</code> 表示定时调度，指定什么时候运行 Job，格式与 Linux 的 Crontab 命令是一样的，这里 <code>*/1 * * * *</code> 的含义是每一分钟启动一次。</p><p>创建 CronJob 对象，通过 <code>kubectl get cronjob</code> 查看 CronJob 的状态：  </p><center><img src="/images/k8s/cronjob-get.png" alt=""></center><p>过一段时间再查看 Pod 的状态：</p><center><img src="/images/k8s/cronjob.png" alt=""></center><p>可以看到，此时产生了 3 个 Pod，3 个 Jobs，这是每隔一分钟就会启动一个 Job。执行 <code>kubectl logs</code> 查看其中一个的历史执行情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl logs hello-1536764760-lm5kt</span><br><span class="line">Hello CloudDeveloper</span><br></pre></td></tr></table></figure><p>到此，本文就结束了。我们从理论结合实践，梳理了 Job 的几种类型，下文我们开始看一种有状态的 Controller——StatefulSet。</p><p>同样，需要学习资料的后台回复“K8S” 和 “K8S2”，想加群学习回复“加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/12.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 10 篇。&lt;/p&gt;
&lt;p&gt;通常，我们在执行任务时，会启用多个服务，有些任务需要长时间运行，全天 24 小时不中断，所以一般会启用 Daemon 类的 服务；而有些任务则只需要短暂执行，任务执行完，服务就没有存在的必要了。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 09 DaemonSet 我是一只看门狗</title>
    <link href="https://chambai.github.io/2018/09/19/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_09_DaemonSet_%E6%88%91%E6%98%AF%E4%B8%80%E5%8F%AA%E7%9C%8B%E9%97%A8%E7%8B%97/"/>
    <id>https://chambai.github.io/2018/09/19/云计算/Kubernetes_笔记_09_DaemonSet_我是一只看门狗/</id>
    <published>2018-09-19T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:20.405Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/11.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 9 篇。</p><p>和上文中的 Deployment 一样，DaemonSet 也是一种副本管理机制，和 Deployment 可以在每个 Node 上运行好几个 Pod 副本不同的是，DaemonSet 始终保证每个 Node 最多只会运行一个副本，就像它的名称一样，作为一只看门狗（Daemon）守护在主人家里。</p><a id="more"></a><p>那么，哪些应用适合用 DaemonSet 的方式来部署呢？</p><p>主要有以下几类：</p><ul><li>监控类的，比如 Prometheus，collectd，New Relic agent，Ganglia gmond 等。</li><li>系统管理类的，比如 kube-proxy, kube-flannel 等。</li><li>日志收集类的，比如 fluentd，logstash 等。</li><li>数据存储类的，比如 glusterd, ceph 等。</li><li>……</li></ul><p>其中，系统管理类的应用主要是 K8S 自身的一些系统组件，我们可以通过 <code>kubectl get daemonset --namespace=kube-system</code> 查看到：  </p><center><img src="/images/k8s/daemon-sys.png" alt=""></center><p>DaemonSet <code>kube-proxy</code> 和 <code>kube-flannel-ds</code> 有 3 个副本，分别负责在每个节点上运行 kube-proxy 和 flannel 组件。</p><p>kube-proxy 前面的文章讲过，它有负载均衡的功能，主要将外部对 Service 的访问导向后端的 Pod 上。显然，一个 Node 运行一个负载均衡器足矣。</p><p>我们可以通过 <code>kubectl edit daemonset kube-proxy --namespace=kube-system</code> 来查看 kube-proxy 的 yaml 配置文件。</p><center><img src="/images/k8s/kube-proxy-dae.png" alt=""></center><p>可以看到它的 kind 是 DaemonSet。</p><p>接着再来看 kube-flannel-ds，这是一个网络插件组件，主要用于构建 K8S 的集群网络，这里大家不懂可以跳过，不影响本文的理解，后面在讲到 K8S 网络的时候会重点讲这个网络方案。</p><p>这里我们只需要知道，各个 Pod 间的网络连通就是 flannel 来实现的。</p><p>这是一个第三方的插件，我们可以直接下载它的 yaml 文件进行安装，执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><p>得到 kube-flannel.yml 文件：</p><center><img src="/images/k8s/kube-flannel-dae.png" alt=""></center><p>这里只列出了一部分内容，kind 类型是 DaemonSet。</p><p>其实 DaemonSet 配置文件的语法和结构和 Deployment 几乎完全一样，不同就在于将 kind 设为 DaemonSet。</p><p>OK，DaemonSet 的探讨就到这里，下文我们继续讨论另外一种 Controller：Job。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/11.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 9 篇。&lt;/p&gt;
&lt;p&gt;和上文中的 Deployment 一样，DaemonSet 也是一种副本管理机制，和 Deployment 可以在每个 Node 上运行好几个 Pod 副本不同的是，DaemonSet 始终保证每个 Node 最多只会运行一个副本，就像它的名称一样，作为一只看门狗（Daemon）守护在主人家里。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 08 Deployment 副本管理 重新招一个员工来填坑</title>
    <link href="https://chambai.github.io/2018/09/16/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_08_Deployment_%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86_%E9%87%8D%E6%96%B0%E6%8B%9B%E4%B8%80%E4%B8%AA%E5%91%98%E5%B7%A5%E6%9D%A5%E5%A1%AB%E5%9D%91/"/>
    <id>https://chambai.github.io/2018/09/16/云计算/Kubernetes_笔记_08_Deployment_副本管理_重新招一个员工来填坑/</id>
    <published>2018-09-16T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:25.260Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/10.png" alt=""></center><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 8 篇。</p><p>Deployment 是 K8S v1.2 引入的概念，与之一起引入还有 ReplicaSet。这两个概念是等同的，准确说是 Deployment 内部调用 ReplicaSet 来实现。</p><a id="more"></a><p>之前这个概念是由 Replication Controller 来实现的，但由于和 K8S 代码中的模块重名，所以就改成 Deployment + ReplicaSet 的组合。</p><p>Deployment 实现了 Pod 的副本管理，使得应用的表现形态和用户期望的状态保持一致。比如用户期望应用部署为 3 副本，如果在运行过程中有一个副本挂了，那么 Deployment 会自动拉起一个副本。</p><p>Deployment 对于应用的编排、自动扩容和缩容、升级回滚等功能都是至关重要的。</p><p>下面我们通过一个例子来看看 Deployment 是如何工作的。</p><p>定义一个 <code>nginx.yaml</code> 文件（对 yaml 文件不熟悉的可以查阅这篇文章）：   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1   </span><br><span class="line">kind: Deployment </span><br><span class="line">metadata:  </span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2  </span><br><span class="line">  template:  </span><br><span class="line">    metadata:</span><br><span class="line">      labels:  </span><br><span class="line">        app: web-server</span><br><span class="line">    spec: </span><br><span class="line">      containers:  </span><br><span class="line">      - name: nginx  </span><br><span class="line">        images: nginx:1.12.1     </span><br><span class="line">        ports:  </span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure><p>这个文件定义了一个 nginx 容器应用，两个 Pod 副本。也就是每个 Pod 中会跑一个 nginx 应用。</p><p>执行<code>kubectl create -f nginx.yaml</code>创建 Deployment 对象，在执行 <code>kubectl get deploy</code> 查看创建的 Deployment。</p><center><img src="/images/k8s/deploy-yaml.png" alt=""></center><p>可以看到，其中两个参数 <code>desired</code>（期待副本数）和 <code>current</code>（当前副本数）都为 2，保持一致，我们再执行 <code>kubectl get pod -o wide</code> 查看当前 Pod 的情况：</p><center><img src="/images/k8s/deploy-pod.png" alt=""></center><p>可以看到，创建了两个 Pod 自动调度到了 Node1 和 Node2 上。这说明每个 Pod 副本是由 Deployment 统一创建并维护的。</p><p>为了一探究竟，我们继续深挖 Deployment。</p><p>执行 <code>kubectl describe deployment nginx-deployment</code> 查看该 Deployment 的详细信息。</p><center><img src="/images/k8s/deploy-replica.png" alt=""></center><p>图中圈住的地方告诉我们，这里创建了一个 ReplicaSet，也就是说 Deployment 内部是调用 ReplicaSet 来完成 Pod 副本的创建的。是否是这样，我们继续验证。</p><p>执行 <code>kubeclt get replicaset</code> 显示创建的 ReplicaSet 对象：</p><center><img src="/images/k8s/deploy-get-replica.png" alt=""></center><p>可以看到这里的 ReplicaSet 名称和上面 Deployment 信息里显示的是一样的，同样，执行 <code>kubectl describe replicaset xxx</code> 显示该 ReplicaSet 的详细信息。</p><center><img src="/images/k8s/deploy-replica-events.png" alt=""></center><p>图中，有两处地方值得注意。一处是 <code>Controlled By</code>，表明 ReplicaSet 是由谁创建并控制的，显然这里显示是 Deployment。第二处是 <code>Events</code>，Events 记录了 K8S 中每一种对象的日志信息，这里的信息有助于排错查问题。我们可以看到这里记录了两个 Pod 副本的创建，Pod 的名称和我们在上面执行 <code>kubectl get pod</code> 看到的结果是一样的。</p><p>继续执行 <code>kubectl describe pod xxx</code> 查看其中一个 Pod 的详细信息：</p><center><img src="/images/k8s/deploy-pod-des.png" alt=""></center><p>可以看到这个 Pod 是由 ReplicaSet 创建的。</p><p>到此，我们不难得出下面这幅图：</p><center><img src="/images/k8s/deploy-pod1.png" alt=""></center><p>用户通过 kubeclt 创建 Deployment，Deployment 又创建 ReplicaSet，最终由 ReplicaSet 创建 Pod。</p><p>从命名上我们也可以看出，子对象的名字 = 父对象的名字 + 随机字符串。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们从实践上剖析了 Deployment 创建 Pod，实际上经过 ReplicaSet 进行创建。Deployment 最主要是对 Pod 进行副本管理，这样可以进行很多自动化管理的复杂操作，后面我们逐步从实践上去剖析 Pod 的各种操作。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/10.png&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 8 篇。&lt;/p&gt;
&lt;p&gt;Deployment 是 K8S v1.2 引入的概念，与之一起引入还有 ReplicaSet。这两个概念是等同的，准确说是 Deployment 内部调用 ReplicaSet 来实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>历史上那些具有跨时代意义的云计算创业公司</title>
    <link href="https://chambai.github.io/2018/09/13/%E4%BA%91%E8%AE%A1%E7%AE%97/%E5%8E%86%E5%8F%B2%E4%B8%8A%E9%82%A3%E4%BA%9B%E5%85%B7%E6%9C%89%E8%B7%A8%E6%97%B6%E4%BB%A3%E6%84%8F%E4%B9%89%E7%9A%84%E4%BA%91%E8%AE%A1%E7%AE%97%E5%88%9B%E4%B8%9A%E5%85%AC%E5%8F%B8/"/>
    <id>https://chambai.github.io/2018/09/13/云计算/历史上那些具有跨时代意义的云计算创业公司/</id>
    <published>2018-09-13T05:16:14.000Z</published>
    <updated>2019-02-22T15:24:05.056Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/cloud/8.jpg" alt=""></center><p>这里所说的不是像 Google、VMware、Cisco、Intel 等等这些巨头公司，而是在巨头的夹缝中冉冉升起的那些新星。由于知识面有限，列举不尽完全，大家可以留言说说你心目中还有哪些值得铭记的公司。</p><a id="more"></a><h3 id="Rackspace"><a href="#Rackspace" class="headerlink" title="Rackspace"></a>Rackspace</h3><hr><p>Rackspace 理应不算新星了，但说到“跨时代意义”，它是绝对的名副其实。原因就在于它和 NASA（美国国家航空航天局）合作开发了 OpenStack——一个让云计算产业进入井喷时代的项目。OpenStack 一经开源，短短两年时间就力压群雄成为开源社区仅次于 Linux 的全球第二大开源项目。</p><center><img src="/images/cloud/rackspace.jpg" alt=""></center><p>可以说是 OpenStack 拯救了 Rackspace。Rackspace 于 1998 成立，是最早期的一批云计算提供商，OpenStack 出来之前，一直不温不火，2010 年 OpenStack 出来之后，名声大震，一步跃居为全球仅次于 Amazon 和 VMware 的三大云计算中心之一。</p><p>如今 OpenStack 已经更新到 Rocky 版本，在众多竞争者和效仿者的围追堵截之下，依然很强势，这和它优秀的架构是分不开的。我很好奇它更新到 Z 版本之后会发生什么？</p><h3 id="dotCloud"><a href="#dotCloud" class="headerlink" title="dotCloud"></a>dotCloud</h3><hr><p>OpenStack 带动了 IaaS 的发展，在这个节骨眼上，人们开始意识到，只有 IaaS 已经无法满足用户变态的需求了。</p><p>时间也落在 2010 年，这个时候几个大胡子年轻人在旧金山成立了一家做 PaaS 的公司，起名 dotCloud，开始杀入 PaaS 领域。</p><center><img src="/images/cloud/dotcloud.png" alt=""></center><p>年轻一辈都意识到了商机，大佬们（Google、Microsoft、Amazon 等）能不意识到吗？于是大佬们也纷纷涉足 PaaS 领域，最终 dotCloud 寡不敌众，不得不缴械投降。</p><p>dotCloud 的工程师于心不甘啊，辛辛苦苦画下的大饼就这样被割分完了。但也没办法，谁让人家是大佬呢。</p><p>鉴于工程师们血液里都流淌着一股热爱分享的劲儿，他们决定将 dotCloud 的核心技术开源给世人。</p><p>谁能想到，无心插柳柳成荫。这门技术瞬间风靡全球，开启了又一个新的时代。</p><p>这门技术就是 Docker。</p><center><img src="/images/cloud/docker.jpg" alt=""></center><p>dotCloud 又火了，但为了纪念这个神圣的时刻，dotCloud 改名为 Docker Inc，全身心投入 Docker 的研发中。至于原来的 PaaS 业务，Docker 将其卖给了德国人的 cloudControl。但好景不长，cloudControl 于 2016 年就关闭了。</p><p>如今，Docker 技术依然可圈可点，虽然很多人在 Kubernetes 出来之后，叫衰 Docker，但我却不以为然。</p><h3 id="Nicira"><a href="#Nicira" class="headerlink" title="Nicira"></a>Nicira</h3><hr><p>2007 年，斯坦福大学的 Nick McKeown 教授和他的天才学生 Martin Casado 博士根据他们的研究成果创办了 Nicira。这是 SDN 网络的鼻祖公司，Nick 教授和 Martin 博士也因此被人们称为 SDN 之父。</p><p>Nicira 公司做了很多牛逼的事：发明了世界上第一个 SDN 控制器 NOX，第一个分布式交换机 Open vSwitch 及其配套协议 OpenFlow 协议，领导研发 OpenStack 网络驱动模块 Quantum/Neutron，开源了业界第一个与硬件无关，支持多种 X86 虚拟化平台的分布式网络虚拟化架构（DVNI），等等等等。</p><p>Nicira 当时那套 SDN 的解决方案，放在今天来说都是很超前的东西。很快，Nicira 就被 VMware 和 Cisco 这些巨头盯上了。巨头们看上的不仅是 Nicira 的技术，更是 Nicira 那帮工程师天才般的智慧和对技术敏锐的洞察力。</p><p>巨头们开始了疯狂的收购战，最终 VMware 以 12.6 亿美元拔得头筹。Martin 博士也带领他的一般众将归入了 VMware 的麾下。</p><center><img src="/images/cloud/nicira.jpg" alt=""></center><p>至此，Nicira 的舞台也算退出了。Open vSwitch 如今仍然占据 SDN 数据面的头把交椅。</p><h3 id="Palo-Alto-Networks"><a href="#Palo-Alto-Networks" class="headerlink" title="Palo Alto Networks"></a>Palo Alto Networks</h3><hr><p>2005 年，以色列的一个天才少年 Nir Zuk 在一间简陋的办公室里，一手创办了 PAN，这就是当下极富盛名的下一代防火墙的初创者（很多公司都宣称自己的防火墙是下一代防火墙，众说纷坛，其实争论这个没多大意义，人家 PAN 都还没说话呢）。</p><center><img src="/images/cloud/palo.jpg" alt=""></center><p>回看 Nir 的一生，也是颇具传奇，经历像极了乔布斯，但没乔布斯那么惨是被老东家赶出的，Nir 是因为老东家小气不肯给他资源做项目而无奈出走。谁能想到，PAN 能在短时间之内就超越了老东家。</p><blockquote><p>小公司要在大公司的夹缝中生长，需要天时地利人和，以及运气，就像 Docker，上帝为你关上一扇门，就会为你打开一扇窗。</p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/cloud/8.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;这里所说的不是像 Google、VMware、Cisco、Intel 等等这些巨头公司，而是在巨头的夹缝中冉冉升起的那些新星。由于知识面有限，列举不尽完全，大家可以留言说说你心目中还有哪些值得铭记的公司。&lt;/p&gt;
    
    </summary>
    
      <category term="01 云计算" scheme="https://chambai.github.io/categories/01-%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="OpenStack" scheme="https://chambai.github.io/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 07 豌豆荚之旅（二）</title>
    <link href="https://chambai.github.io/2018/09/11/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_07_%E8%B1%8C%E8%B1%86%E8%8D%9A%E4%B9%8B%E6%97%85%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>https://chambai.github.io/2018/09/11/云计算/Kubernetes_笔记_07_豌豆荚之旅（二）/</id>
    <published>2018-09-11T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:28.831Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/9.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学习 K8S，这是系列第 7 篇。</p><p>上篇我们简单学习了 Pod 的基础知识，本篇开始讲述一些 Pod 的高阶知识（本文只做理论的简单阐述，后面会针对每个点进行实践）。</p><a id="more"></a><h3 id="Pod-的生命周期管理"><a href="#Pod-的生命周期管理" class="headerlink" title="Pod 的生命周期管理"></a>Pod 的生命周期管理</h3><p>豌豆荚自诞生之日起，便注定要经历生老病死的一生。Pod 是由容器组成的，Pod 生命周期实际上是由容器的生命周期决定的。</p><p>在整个生命周期过程中，Pod 会被定义为各种状态，如下：</p><center><img src="/images/k8s/pod_lifetime.png" alt=""></center><p>这些状态包括正常状态和异常状态，当出现异常状态时，K8S 的监控机制会检测到这种异常，并执行相应的异常处理。</p><h3 id="Pod-的监控机制"><a href="#Pod-的监控机制" class="headerlink" title="Pod 的监控机制"></a>Pod 的监控机制</h3><p>Pod 的监控主要是监控 Pod 内容器的健康状况，并进行相关的异常处理和容错管理。</p><p>当监控到某个容器异常退出或健康检查失败时，Pod 会执行重启策略，使得 Pod 内的容器健康运转。</p><p>如下记录了 Pod 的重启策略和健康检查机制：</p><center><img src="/images/k8s/pod_jk.png" alt=""></center><h3 id="Pod-的调度管理"><a href="#Pod-的调度管理" class="headerlink" title="Pod 的调度管理"></a>Pod 的调度管理</h3><p>K8S Master 上的 Scheduler 服务负责实现 Pod 的调度管理，Pod  是静态的，只有真正被调度到具体的节点上才能发挥它的作用。K8S 根据不同的应用场景，定义了多种不同的调度策略。这些策略可以是根据算法自动完成的，也可以是人为指定的。具体可以看下面这张导图：</p><center><img src="/images/k8s/pod_scheduler.png" alt=""></center><p>笼统来看，有时候为了权衡应用场景和集群资源的需求，需要对 Pod 进行扩容和缩容，这同样属于 Pod 调度管理的范畴。</p><h3 id="Pod-的存储管理"><a href="#Pod-的存储管理" class="headerlink" title="Pod 的存储管理"></a>Pod 的存储管理</h3><p>Pod 和容器的数据存储使用 Volume，K8S Volume 和 Docker 的 Volume 是一样的原理，都是文件系统上的一个目录，只不过在 K8S 中实现了更多的 backend driver。包括 emptyDir、hostPath、GCE Persistent Disk、NFS、Ceph 等。</p><center><img src="/images/k8s/pod_volume.png" alt=""></center><p>Volume 提供了对各种 driver 的抽象，容器在使用 Volume 读写数据的时候不需要关心底层具体存储方式的实现，对它来说，所有类型的 Volume 都是一个目录。</p><p>当 Volume 被挂载到 Pod 中时，这个 Pod 中的容器都会共享这个 Volume，当其中的容器销毁时，Volume 中的数据也不会丢失，当 Pod 销毁时，根据不同的 driver 实现，数据也可以保存下来。</p><p>Volume 提高了 Pod 内数据的持久化管理，延长了 Pod 和容器的生命周期。</p><h3 id="Pod-的网络管理"><a href="#Pod-的网络管理" class="headerlink" title="Pod 的网络管理"></a>Pod 的网络管理</h3><p>在 K8S 中，定义了多种资源对象，很多对象本身就是一个通信的实体，比如容器、Pod、Service、Node。</p><p>K8S 维护这多种对象之间的通信关系，比如：Pod 内容器之间的通信、Pod 与容器之间的通信、Pod 之间的通信、Pod 与 Service 之间的通信，以及外部的访问。</p><p>这些通信机制的建立离不开 K8S 建立的完善的网络模型。K8S 使用了 CNI（容器网络规范）来标准化、归一化网络模型。</p><p>第三方的厂商或开发者可以根据自身网络需求，遵从 CNI 的规范，实现各种网络方案，并以插件的形式提供给 K8S 使用。目前比较知名的网络方案有：flannel、calico、weave、canal 等。</p><center><img src="/images/k8s/pod_net.png" alt=""></center><p>这些网络方案各有千秋、虽然实现方式各有区别，但殊途同归，最终都是满足 K8S 中各种实体间的通信需求。</p><p>OK，本文就到这里，我们通过两篇文章大致梳理了豌豆荚从出生到死亡要面临的多种人生的关卡。跨过去了，就成熟了，希望我们都能跨过自己人生的关卡。</p><p>下文我们开始进入实践的部分。</p><p>为了给大家更多的福利，这个系列的每一篇文章我都会送一些电子书，可能有重的，也有一些新书，之前送了《K8S 指南》和《容器与容器云》，这次送一本新书《》，大家有需要的后台回复“K8S2”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/9.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学习 K8S，这是系列第 7 篇。&lt;/p&gt;
&lt;p&gt;上篇我们简单学习了 Pod 的基础知识，本篇开始讲述一些 Pod 的高阶知识（本文只做理论的简单阐述，后面会针对每个点进行实践）。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 06 豌豆荚之旅（一）</title>
    <link href="https://chambai.github.io/2018/09/09/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_06_%E8%B1%8C%E8%B1%86%E8%8D%9A%E4%B9%8B%E6%97%85%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://chambai.github.io/2018/09/09/云计算/Kubernetes_笔记_06_豌豆荚之旅（一）/</id>
    <published>2018-09-09T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:33.462Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/8.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学习 K8S，这是系列第 6 篇。</p><p>Pod 中文译为豌豆荚，很形象，豌豆荚里面包裹的多颗小豌豆就是容器，小豌豆和亲密无间的老伙计壳荚子自出生之日起就得面对各种各样的人生大事：  </p><a id="more"></a><ul><li>容器、Pod、Node 之间的关系</li><li>Pod 的生命周期管理</li><li>Pod 的调度管理</li><li>Pod 的监控</li><li>Pod 的升级与回滚</li><li>Pod 的扩容与缩容</li><li>Pod 的存储管理</li><li>Pod 的网络管理</li><li>……</li></ul><h3 id="为什么需要-Pod？"><a href="#为什么需要-Pod？" class="headerlink" title="为什么需要 Pod？"></a>为什么需要 Pod？</h3><p>我们假设没有 Pod，应用部署的最小单元就是容器，会有什么问题？首先，应用调度粒度太细，不便于管理。想象一下淘宝网站运行着海量应用，每个应用又拆分成多个服务，每个服务部署在一个容器里，一个集群管理系统要管理庞大的容器集群，既要顾忌不同应用之间的隔离性，又要考虑相同应用之间的关联性，这在管理上将会是灾难性的难题。</p><p>其次，资源利用率低。有很多应用之间存在某种强关联关系，它们需要彼此能共享对方的资源，双方的交互需要快捷有效，如果把它们部署到单独的容器中，资源利用和通信将成为最主要的系统瓶颈。</p><p>Pod 的提出改变了这种局面，它将强关联的应用整合在一起，作为一个整体对外提供服务，既简化了管理的难度，又提高了资源的利用率。</p><p>那哪些应用是强关联，适合放到一个 Pod 中呢？举个例子，比如下面这个 Pod 包含两个容器，一个 File Puller，一个是 Web Server。</p><center><img src="/images/k8s/pod.png" alt=""></center><p>File Puller 会定期从外部的 Content Manager 中拉取最新的文件，将其存放在 Volume 中。然后 Web Server 从 Volume 中读取文件，来响应 Consumer 的请求。</p><p>这两个容器通过 Volume 来共享实时的数据，协作处理一个 Consumer 的请求，把它们放到同一个 Pod 中就是合适的。</p><p>如果有应用和任何应用之间都不存在联系，那么它们就单独部署在一个 Pod 中，称为<code>one-container-per-pod</code>。即便只有一个容器，K8S 管理的也是 Pod 而不是直接管理容器。</p><p>综上，Pod 在设计的时候，主要动机有以下两点：</p><ol><li>方便管理</li></ol><p>Pod 提供了比容器更高一层的抽象，K8S以 Pod 为最小单元进行应用的部署、调度、扩展、共享资源和管理周期。</p><ol start="2"><li>资源共享和通信</li></ol><p>Pod 内的所有容器共享同一个网络空间，它们之间可以通过 <code>localhost</code> 相互通信。同样，所有容器共享 Volume，一个容器挂载一个 Volume，其余容器都可以访问这个 Volume。</p><h3 id="容器、Pod、Node-之间的关系"><a href="#容器、Pod、Node-之间的关系" class="headerlink" title="容器、Pod、Node 之间的关系"></a>容器、Pod、Node 之间的关系</h3><p>容器是 Pod 的一个属性，定义了应用的类型及共享的资源。每个容器会分配一个 Port，Pod 内的容器通过 localhost:Port 的形式来通信。</p><p>一个 Pod 包含一个或多个容器，每个 Pod 会分配一个唯一的 IP 地址，Pod 内的多个容器共享这个 IP 地址，每个容器的 Port 加上 Pod IP 共同组成一个 <code>Endpoint</code>，共同对外提供服务。</p><p>在部署应用的时候，Pod 会被 Master 作为一个整体调度到一个 Node 上。如果开启多副本管理，则多个 Pod 会根据调度策略调度到不同的 Node 上。如果 Node 宕机，则该 Node 上的所有 Pod 会被自动调度到其他 Node 上。</p><p>下面是容器、Pod、Node 三者之间的关系图：  </p><center><img src="/images/k8s/pod_docker_node.png" alt=""></center><h3 id="Pod-根容器"><a href="#Pod-根容器" class="headerlink" title="Pod 根容器"></a>Pod 根容器</h3><p>Pod 中有一个特殊的容器，叫 Pod 的根容器——Pause 容器，这是一个很小的容器，镜像大小大概为 200KB。</p><center><img src="/images/k8s/pod_pause.png" alt=""></center><p>Pause 容器存在的意义是: <strong>维护 Pod 的状态信息</strong>。</p><p>由于 Pod 是作为一个整体进行调度，我们难以对这个整体的信息进行简单的判断和有效地进行行动。</p><p>想象一下，假如 Pod 内一个容器死亡了，是算整体死亡呢还是 N/M 死亡率，如果 Pod 内所有容器都死亡了，那是不是该 Pod 也就死亡了，如果加入新的容器或原有容器故障恢复呢，如何让新成员快速融入环境？</p><p>理论上，虽然 Pod 是由一组容器组成的，但 Pod 和容器是彼此独立的，也就是容器的故障不应该影响 Pod 的存在，Pod 有相应的手段来保证容器的健康状况。</p><p>引入与业务无关的，并且不易死亡的 Pause 容器就可以很好的解决这个问题，Pause 容器的状态就代表了 Pod 的状态，只要 Pause 不死，那么不管应用容器发生什么变化，Pod 的状态信息都不会改变。</p><p>这样，Pod 内的多个应用容器共享 Pause 容器的 IP 和 Volume，当加入新的容器或者原有的容器因故障重启后就可以根据 Pause 保存的状态快速学习到当前 Pod 的状态。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文简单学习了 Pod 的初级知识，包括 Pod 的设计动机，容器、Pod 和 Node 之间的关系，以及 Pod 的守护者——Pause 容器。</p><p>容器的 Port + Pod IP = Endpoint，构成一个 Pod 的通信实体，Pod 中的容器共享网络和存储，这些共享信息是由 Pause 容器来维护的。</p><p>下文继续豌豆荚之旅的第二个部分，学习 Pod 的管理哲学。</p><p>为了给大家更多的福利，这个系列的每一篇文章我都会送一些电子书，可能有重的，也有一些新书，之前送了《K8S 指南》和《容器与容器云》，这次送一本由 K8S 中文社区主编的《K8S 中文手册》，大家有需要的后台回复“K8S2”</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/8.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学习 K8S，这是系列第 6 篇。&lt;/p&gt;
&lt;p&gt;Pod 中文译为豌豆荚，很形象，豌豆荚里面包裹的多颗小豌豆就是容器，小豌豆和亲密无间的老伙计壳荚子自出生之日起就得面对各种各样的人生大事：  &lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 05 使用产品前请先阅读说明书</title>
    <link href="https://chambai.github.io/2018/09/07/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_05_%E4%BD%BF%E7%94%A8%E4%BA%A7%E5%93%81%E5%89%8D%E8%AF%B7%E5%85%88%E9%98%85%E8%AF%BB%E8%AF%B4%E6%98%8E%E4%B9%A6/"/>
    <id>https://chambai.github.io/2018/09/07/云计算/Kubernetes_笔记_05_使用产品前请先阅读说明书/</id>
    <published>2018-09-07T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:37.109Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/7.png" alt=""></center><hr><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 5 篇。</p><p>生活中，随处可见，几乎每一款产品都会附带一份说明书，说明书可以记录产品的使用方法，也可以记录产品的配方。有了说明书，我们完全可以窥探一款产品的全貌。</p><a id="more"></a><p>在 K8S 中，<code>yaml</code> 配置文件就是 K8S 资源对象的说明书，定义了对象包含的元素及采取的动作，每种对象都可以通过 yaml 配置文件来创建。</p><center><img src="/images/k8s/yaml.jpg" alt=""></center><h3 id="yaml-是什么"><a href="#yaml-是什么" class="headerlink" title="yaml 是什么"></a>yaml 是什么</h3><p>yaml 是一种用来写配置文件的语言，没错，它是一门语言。如果你用过 json，那么对它就不会陌生，yaml 又被称为是 json 的超集，使用起来比 json 更方便。</p><p>结构上它有两种可选的类型：Lists 和 Maps。</p><p>List 用 -（破折号） 来定义每一项，Map 则是一个 key:value 的键值对来表示。如下是一个 json 文件到 yaml 文件的转换：  </p><p>json:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">    &quot;kind&quot;: &quot;Pod&quot;,</span><br><span class="line">    &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;xx&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;containers&quot;: [&#123;</span><br><span class="line">            &quot;name&quot;: &quot;front-end&quot;,</span><br><span class="line">            &quot;images&quot;: &quot;nginx&quot;,</span><br><span class="line">            &quot;ports&quot;: [&#123;</span><br><span class="line">                &quot;containerPort&quot;: &quot;80&quot;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;, &#123;</span><br><span class="line">            &quot;name&quot;: &quot;flaskapp-demo&quot;,</span><br><span class="line">            &quot;images&quot;: &quot;jcdemo/flaskapp&quot;,</span><br><span class="line">            &quot;ports&quot;: [&#123;</span><br><span class="line">                &quot;containerPort&quot;: &quot;5000&quot;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>yaml:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">    name: xx</span><br><span class="line">spec:</span><br><span class="line">    containers:</span><br><span class="line">        - name: front-end</span><br><span class="line">        images: nginx</span><br><span class="line">        ports:</span><br><span class="line">            - containerPort: 80</span><br><span class="line">    - name: flaskapp-demo</span><br><span class="line">        images: jcdemo/flaskapp</span><br><span class="line">        ports: 8080</span><br></pre></td></tr></table></figure></p><p>这个文件简单地定义了一个 Pod 对象，包含两个容器，我们可以很清晰地看到两者是如何转换的。</p><h3 id="K8S-创建资源的两种方式"><a href="#K8S-创建资源的两种方式" class="headerlink" title="K8S 创建资源的两种方式"></a>K8S 创建资源的两种方式</h3><p>在 K8S 中，有两种创建资源的方式：kubectl 命令和 yaml 配置文件。</p><p>两种方式各有各的好处。命令行的方式最为简单，一条命令就万事大吉，但缺点也很明显，你并不知道这条命令背后到底做了哪些事，配置文件就提供了一种让你知其然更知其所以然的方式。总的来说，它有以下好处：   </p><ul><li>完整性：配置文件描述了一个资源的完整状态，可以很清楚地知道一个资源的创建背后究竟做了哪些事；</li><li>灵活性：配置文件可以创建比命令行更复杂的结构；</li><li>可维护性：配置文件提供了创建资源对象的模板，能够重复使用；</li><li>可扩展性：适合跨环境、规模化的部署。</li><li>……</li></ul><p>当然，复杂的东西对用户就难以做到友好，我们需要熟悉它的配置文件的语法，有一定难度。下面举几个例子，让你对 yaml 配置文件有一个基本的认识。</p><h3 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h3><p>下面，我们分别来看看 <code>deployment</code>、<code>pod</code>、<code>service</code> 这三种资源的说明书都长啥样。</p><p>由于 K8S 对每种资源的定义非常庞杂，限于篇幅，我们只看一些必选的参数，目的是通过这几个例子，读懂 yaml 配置文件。</p><h4 id="deployment"><a href="#deployment" class="headerlink" title="deployment"></a>deployment</h4><p>定义 deployment 配置文件，命名为：nginx-deployment.yaml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1   # 1.9.0 之前的版本使用 apps/v1beta2，可通过命令 kubectl api-versions 查看</span><br><span class="line">kind: Deployment     #指定创建资源的角色/类型</span><br><span class="line">metadata:      #资源的元数据/属性</span><br><span class="line">    name: nginx-deployment    #资源的名字，在同一个namespace中必须唯一</span><br><span class="line">spec:</span><br><span class="line">    replicas: 2      #副本数量2</span><br><span class="line">    selector:      #定义标签选择器</span><br><span class="line">        matchLabels:</span><br><span class="line">            app: web-server</span><br><span class="line">    template:      #这里Pod的定义</span><br><span class="line">        metadata:</span><br><span class="line">            labels:      #Pod的label</span><br><span class="line">                app: web-server</span><br><span class="line">    spec:         # 指定该资源的内容  </span><br><span class="line">        containers:  </span><br><span class="line">        - name: nginx      #容器的名字  </span><br><span class="line">          images: nginx:1.12.1  #容器的镜像地址    </span><br><span class="line">          ports:  </span><br><span class="line">          - containerPort: 80  #容器对外的端口</span><br></pre></td></tr></table></figure></p><p>执行<code>kubectl create -f nginx.yaml</code>创建 deployment 资源：</p><center><img src="/images/k8s/deploy-yaml.png" alt=""></center><h4 id="pod"><a href="#pod" class="headerlink" title="pod"></a>pod</h4><p>定义 pod 配置文件，命名为 redis-pod.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod  </span><br><span class="line">metadata:  </span><br><span class="line">    name: pod-redis</span><br><span class="line">    labels:</span><br><span class="line">        name: redis</span><br><span class="line">spec: </span><br><span class="line">    containers:</span><br><span class="line">    - name: pod-redis</span><br><span class="line">        images: docker.io/redis  </span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80    #容器对外的端口</span><br></pre></td></tr></table></figure><p>执行<code>kubectl create -f pod-redis.yaml</code>创建 pod 资源：  </p><center><img src="/images/k8s/pod-yaml.png" alt=""></center><p>可以看到，成功创建一个 Pod，<code>ContainerCreating</code>表示 Pod 中的容器正在执行镜像的下载和安装过程，过一会儿，就显示<code>Running</code>了，表明 Pod 应用部署完成。</p><h4 id="service"><a href="#service" class="headerlink" title="service"></a>service</h4><p>定义 service 配置文件，命名为 httpd-svc.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1  </span><br><span class="line">kind: Service  # 指明资源类型是 service</span><br><span class="line">metadata:  </span><br><span class="line">    name: httpd-svc # service 的名字是 httpd-svc</span><br><span class="line">    labels:  </span><br><span class="line">        name: httpd-svc </span><br><span class="line">spec:  </span><br><span class="line">    ports:  # 将 service 8080 端口映射到 pod 的 80 端口，使用 TCP 协议</span><br><span class="line">    - port: 8080</span><br><span class="line">        targetPort: 80  </span><br><span class="line">        protocol: TCP  </span><br><span class="line">    selector:  </span><br><span class="line">        run: httpd # 指明哪些 label 的 pod 作为 service 的后端</span><br></pre></td></tr></table></figure><p>执行<code>kubectl create -f httpd-svc.yaml</code>创建 service 资源：  </p><center><img src="/images/k8s/svc-yaml.png" alt=""></center><p>可以看到，service httpd-svc 分配到一个 Cluster-IP 10.96.0.1，我们可以通过该 IP 访问 service 所维护的后端 Pod。</p><p>另外，还有一个 service kubernetes，这个是 Kubernetes API Server 的 service，Cluster 内部的各组件就是通过这个 service 来访问 API Server。 </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>yaml 是 K8S 资源对象的说明书，每个对象拥有哪些属性都可以在 yaml 中找到详尽的说明，初学者建议多写 yaml 文件，少用命令行。</p><p>以上三个例子只是对 yaml 文件做个简单说明，更详细的信息还是参考官网。</p><p>OK，本文就到此为止，下文我们开始进入豌豆荚之旅。觉得不错，别忘了转发分享给你的朋友哦。</p><blockquote><p>参考：<br><a href="https://www.kubernetes.org.cn/1414.html" target="_blank" rel="noopener">https://www.kubernetes.org.cn/1414.html</a></p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/7.png&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;hr&gt;
&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 5 篇。&lt;/p&gt;
&lt;p&gt;生活中，随处可见，几乎每一款产品都会附带一份说明书，说明书可以记录产品的使用方法，也可以记录产品的配方。有了说明书，我们完全可以窥探一款产品的全貌。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 04 架构是个好东西</title>
    <link href="https://chambai.github.io/2018/09/05/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_04_%E6%9E%B6%E6%9E%84%E6%98%AF%E4%B8%AA%E5%A5%BD%E4%B8%9C%E8%A5%BF/"/>
    <id>https://chambai.github.io/2018/09/05/云计算/Kubernetes_笔记_04_架构是个好东西/</id>
    <published>2018-09-05T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:40.725Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/6.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 4 篇。</p><p>任何技术的诞生，都会经历从架构设计到开发测试的过程，好的技术，往往也会有一套好的架构。架构是个好东西，它能帮助我们站在高处看清楚事物的整体结构，避免过早地进入细节而迷失方向。</p><a id="more"></a><p>上篇文章扫清了 K8S 的一些基本概念，今天这篇文章我们就来看看 K8S 的架构。</p><p>先上图：</p><center><img src="/images/k8s/architecture.png" alt=""></center><p>图中包括两种类型的节点：Master 和 Node，每个节点上运行着多种 K8S 服务。</p><h3 id="Master-节点"><a href="#Master-节点" class="headerlink" title="Master 节点"></a>Master 节点</h3><p>Master 是 K8S 集群的大脑，运行着如下 Daemon 服务：kube-apiserver、kube-controller-manager、kube-scheduler、etcd 等。</p><h4 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h4><p>如果把 K8S 环境看作是一个公司，那 API Server 就是这个公司的基础平台部，是公司最为核心的技术能力输出出口。它对外提供 HTTP/HTTPS REST API，统称 K8S API，可以供各类客户端工具（CLI 或 WebUI）、K8S 其他组件，以及第三方的平台接入。对内提供了 K8S 各类资源（如 Pod、Deployment、Service等）的增删改查和监控等操作，是集群内各个功能模块之间数据交互和通信的中心枢纽。</p><h4 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h4><p>Controller Manager 更像是公司的人力资源部，负责统筹公司的人员分布。它管理着 K8S 各类资源的生命周期，保证资源处于预期状态，如果现有状态和预期状态不符，它会自动化执行修正。</p><p>Controller Manager 由多种 Controller 组成，包括 Replication Controller、Node Controller、ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Service Controller、Token Controller 及 Endpoint Controller 等。每种 Controller 都负责一种具体的资源管控流程，而 Controller Manager 正是这些 Controller 的核心管理者。</p><h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>Scheduler 则像是公司各个部门的项目经理之类的角色，负责将具体的人力放到他们擅长的位置上，知人善用。具体来说，Scheduler 负责将待调度的 Pod 对象按照特定的调度策略绑定到集群中某个合适的节点上，调度策略会综合考虑集群的拓扑结构、节点的负载情况、以及应用对高可用、性能、数据亲和性的需求。</p><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>etcd 是一个高可用的分布式数据库，负责保存 K8S 的配置信息和各种资源的状态信息。当数据发生变化时，etcd 会及时告知集群中的其他组件。</p><h4 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h4><p>kubectl 是 K8S 的 CLI 工具，这是使用 K8S API 建立的一套命令行工具，使用它，可以非常方便地管理 K8S 集群。</p><h3 id="Node-节点"><a href="#Node-节点" class="headerlink" title="Node 节点"></a>Node 节点</h3><p>Node 是 K8S 集群的具体执行者，也运行着多种服务，如：kubelet、kube-proxy、container runtime、Pod 网络等。Node 可以看作是 Master 的代理，负责处理 Master 下发到本节点的任务，管理 Pod 和 Pod 中的容器，定期向 Master 汇报自身资源的使用情况。</p><h4 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h4><p>kubelet 更像是部门中各个小组的 Leader，对外从 API Server 拿资源，对内负责小组内各种资源的管理，比如从 Master 拿到 Pod 的具体配置信息（images、Volume 等）之后，kubelet 根据这些信息创建和运行容器。</p><h4 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h4><p>kube-proxy 则像穿插在公司各个部门之间的接口人，对内和内部人员沟通，对外协调部门之间的各种事宜，展示部门风采。kube-proxy 作用于 Service，通过前面学习，我们知道 Service 是对一组 Pod 的抽象，统一对外提供某种服务，当外部访问 Service 时，实际上是需要访问 Service 所管辖的 Pod 中的容器，kube-proxy 在这里就是负责将访问 Service 的数据流转发到后端的容器，如果有多个副本，kube-proxy 会实现负载均衡。</p><h4 id="cAdvisor"><a href="#cAdvisor" class="headerlink" title="cAdvisor"></a>cAdvisor</h4><p>cAdvisor 对 Node 上的资源进行实时监控和性能数据采集，包括 CPU 使用情况、内存使用情况、网络吞吐量及文件系统使用情况等。cAdvisor 集成在 kubelet 中，当 kubelet 启动时会自动启动 cAdvisor，一个cAdvisor 仅对一台 Node 机器进行监控。</p><h4 id="container-runtime"><a href="#container-runtime" class="headerlink" title="container runtime"></a>container runtime</h4><p>container runtime 是真正运行容器的地方，为容器提供运行环境，主流的三种 container runtime 是 lxc、runc 和 rkt，K8S 都支持它们，但常用的事 runc，原因是 runc 是 Docker 默认的 runtime。在 K8S 的容器应用中，Docker 是主流。</p><p>OK，K8S 架构介绍就到此为止。</p><p>最后，还是继续送书，容器网络专家倪朋飞写的《K8S 指南》电子书，如有需要后台回复“K8S”（之前回复过就不用回复了）。如需加群学习回复“加群”。</p><p>下文我们开始对 K8S 的说明书一探究竟。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/6.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 4 篇。&lt;/p&gt;
&lt;p&gt;任何技术的诞生，都会经历从架构设计到开发测试的过程，好的技术，往往也会有一套好的架构。架构是个好东西，它能帮助我们站在高处看清楚事物的整体结构，避免过早地进入细节而迷失方向。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 03 扫清概念</title>
    <link href="https://chambai.github.io/2018/09/03/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_03_%E6%89%AB%E6%B8%85%E6%A6%82%E5%BF%B5/"/>
    <id>https://chambai.github.io/2018/09/03/云计算/Kubernetes_笔记_03_扫清概念/</id>
    <published>2018-09-03T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:44.388Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/5.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第三篇。</p><p>每一种技术，为了描述清楚它的设计理念，都会自定义一堆概念或术语。在进入一门技术的研究之前，我们有必要扫清它的基本概念。</p><a id="more"></a><h3 id="资源对象"><a href="#资源对象" class="headerlink" title="资源对象"></a>资源对象</h3><p>K8S 的操作实体，在 K8S 中，有很多的操作对象，比如容器、Pod、Deployment、Service、Node 等，我们统统称它们为资源对象。</p><h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p>K8S 集群，是计算、存储和网络资源的集合，K8S 基于这些资源来承载容器化的应用。</p><h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><p>K8S 集群的大脑，负责整个集群的管控、资源调度。可以部署在普通物理机或虚拟机上，为了实现高可用，可以部署多个 Master。</p><h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><p>K8S 集群的执行者，受 Master 指挥，负责运行和监控容器应用、管理容器的生命周期，并向 Master 定期汇报容器的状态。同样，Node 也可以部署在物理机或虚拟机之上，也可以部署多个。</p><h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><p>在 K8S 集群中，Pod 是资源调度的最小单位，一个 Pod 可以包含一个或多个容器应用，这些容器应用彼此之间存在某种强关联。Pod 内的所有容器应用共享计算、存储、网络资源。</p><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><p>Controller 是 K8S 中负责管理 Pod 的资源对象，它定义 Pod 的部署属性，比如有几个副本，副本异常怎么处理等，如果把 Pod 副本看做是一个公司职员，那么 Controller 就像是 HR，会不断根据人员的变动来招人满足公司的发展需求。</p><p>为了满足多种业务场景，K8S 提供了多种 Controller，包括 Deployment、ReplicaSet、DaemonSet、StatefulSet、Job 等。</p><h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><p>Deployment 是最常用的 Controller，定义了用户的期望场景，实现了 Pod 的多副本管理，如果运行过程中有一个副本挂了（员工离职），那么 K8S 会根据 Deployment 的定义重新拉起一个副本继续工作（招一个新员工），始终保证 Pod 按照用户期望的状态运行。</p><h4 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h4><p>ReplicaSet 和 Deployment 实现了同样的功能，确切的说是 Deployment 通过 ReplicaSet 来实现 Pod 的多副本管理。我们通常不需要直接使用 ReplicaSet。</p><h4 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h4><p>DaemonSet 用于每个 Node 最多只运行一个副本的场景，通常用于运行 Daemon。</p><h4 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h4><p>Job 用于运行结束就删除的应用，而其他 Controller 则是会长期保持运行。</p><h4 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h4><p>以上 Controller 都是无状态的，也就是说副本的状态信息会改变，比如当某个 Pod 副本异常重启时，其名称会改变。StatefulSet 提供有状态的服务，能够保证 Pod 的每个副本在其生命周期中名称保持不变。这是通过持久化的存储卷来实现的。</p><h3 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h3><p>Label 定义了其他资源对象所属的标签，类似于你在公司被分到 A 小组、B 小组。有了标签，就可以针对性地对每个小组进行管理。比如把某个小组搬到哪个办公区（把某个 Pod 部署到哪个 Node 上）。给指定的资源对象定义一个或多个不同的标签能够实现多维度的资源分组管理，方便进行资源分配、调度、配置、部署等管理工作。</p><h3 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h3><p>Label 选择器，K8S 通过 Selector 来过滤筛选指定的资源对象，类似于 SQL 语句中的 where 查询条件，Label 实现了简单又通用的对象查询机制。</p><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>在 K8S 中，Service 是对 Pod 对象的抽象，通常，Pod 会以多副本的形式部署，每个 Pod 都有自己的 IP，都可以对外提供服务，但 Pod 是脆弱的，也就是说，它随时都有可能被频繁地销毁和重启，IP 也会随之改变，这样，服务的访问就会出现问题。</p><p>Service 就是提出来解决这个问题的，它定义了一个虚拟 IP（也叫集群 IP），这个 IP 在 Service 的整个生命周期内都不会改变。当有访问到达时，Service 会将请求导向 Pod，如果存在多个 Pod，Service 还能实现负载均衡。</p><h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><p>K8S 的存储卷，定义了一个 Pod 中多个容器可访问的共享目录。和 Docker 的 Volume 不太一样的是，K8S 的 Volume 是以 Pod 为单位的，也就是 Volume 的生命周期和 Pod 相关，和 Pod 内的容器不相关，即使容器终止或重启，Volume 中的数据也不会丢失，只有当 Pod 被删除时，数据才会丢失。</p><h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><p>当有多个用户或租户使用同一个 K8S 集群时，如何区分它们创建的资源呢？答案就是 Namespace。</p><p>Namespace 将一个物理的集群从逻辑上划分成多个虚拟的集群，每个集群就是一个 Namespace，不同 Namespace 里的资源是完全隔离的。每个用户在自己创建的 Namespace 里操作，都不会影响到其他用户。</p><h3 id="Annotation"><a href="#Annotation" class="headerlink" title="Annotation"></a>Annotation</h3><p>Annotation 与 Label 类似，但和 Label 不同 的事，Annotation 不用于过滤筛选，它只是用户定义的某一种资源的附加信息，目的是方便外部查找该资源。有点类似于我们常说的别名，没有它完全可以，但有了它可以很方便查找。</p><p>最后，还是继续送书，容器网络专家倪朋飞写的《K8S 指南》电子书，如有需要后台回复“K8S”（之前回复过就不用回复了）。如需加群学习回复“加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/5.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第三篇。&lt;/p&gt;
&lt;p&gt;每一种技术，为了描述清楚它的设计理念，都会自定义一堆概念或术语。在进入一门技术的研究之前，我们有必要扫清它的基本概念。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 02 demo 初体验</title>
    <link href="https://chambai.github.io/2018/08/31/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_02_demo_%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
    <id>https://chambai.github.io/2018/08/31/云计算/Kubernetes_笔记_02_demo_初体验/</id>
    <published>2018-08-31T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:49.054Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/4.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper！欢迎大家和我一起学习 K8S。</p><p>从前面的文章我们知道，Kubernetes 脱胎于 Google 的 Borg，Borg 在 Kubernetes 诞生之初已经在 Google 内部身经百战 10 余年，且不说它的历史源远流长，就凭它是出自 Google 那帮天才工程师之手，就知道它的学习难度不低。</p><a id="more"></a><p>对于这种有一定学习门槛的技术，最好的入门方式是先玩起来，如果刚开始就沉迷在那些理论中，很容易从入门到放弃。</p><p>可喜的是，Google 已经考虑到了这一点，官方文档提供了一个很小的 demo，麻雀虽小，五脏俱全，这个 demo 基本涵盖了 K8S 的基本概念，通过它，可以轻松构建一个 K8S 集群，玩转 K8S，我们现在就去玩一玩。（PS：下面提到的概念，我们后面会详细讨论，不理解可以暂时跳过）</p><p>打开：<br><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics" target="_blank" rel="noopener">https://kubernetes.io/docs/tutorials/kubernetes-basics</a>  </p><p>映入眼帘的是图文并茂的 6 个步骤： </p><center><img src="/images/k8s/k8sbasic.png" alt=""></center><ol><li>创建一个 K8S 集群</li><li>部署 APP</li><li>探索 APP</li><li>访问 APP</li><li>APP 弹性伸缩</li><li>更新 APP</li></ol><p>在开始每个步骤之前，先来了解个东西——minikube。顾名思义，这是一个迷你版的 K8S，一个轻量级的 K8S 实现，对于平常的学习体验，使用它可以达到和使用 K8S 一样的效果。它的部署方式足够简单，All-In-One，一个集群只有一个节点，K8S 所有组件都部署在这个节点上。</p><p>用户也可以使用 Web UI 和 minikube CLI 的方式来管理 K8S 集群，比如：启动，停止，删除，获取状态等。官方的 demo 就是使用 minikube CLI 来完成的。</p><p>话不多说，下面我们就开始体验下 K8S 之旅吧。</p><p><font color="red" size="5">第一步：</font><strong>创建一个 K8S 集群</strong>  </p><center><img src="/images/k8s/createcluster.png" alt=""></center><p>在交互界面输入 <code>minikube start</code> 就创建了一个 K8S 集群，这个集群创建在一台 VM 上，K8S 所有组件都跑在这台 VM 上。</p><p>接下来我们就可以使用 K8S 命令行工具 <code>kubectl</code> 来操作这个集群了。</p><p><code>kubectl version</code> 查看 K8S 的版本号：</p><center><img src="/images/k8s/kubectlversion.png" alt=""></center><p>看到两个 version，<code>client version</code> 指 kubectl 的 version，<code>server version</code> 就是 K8S 的 version。</p><p><code>kubectl get nodes</code> 获取集群节点数：</p><center><img src="/images/k8s/kubegetnodes.png" alt=""></center><p>可以看到这个 demo 只有一个节点，就是前面创建的 VM。<code>status</code> 是 <code>ready</code>，说明该节点准备好部署 APP 了。</p><p><font color="red" size="5">第二步：</font><strong>部署一个 APP</strong>  </p><p>执行命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run kubernetes-bootcamp --images=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080</span><br></pre></td></tr></table></figure></p><p>就完成了一个 APP 的部署。</p><center><img src="/images/k8s/createdeployment.png" alt=""></center><p>其中，<code>--images</code> 指定 APP 的 Docker 镜像，<code>--port</code> 设置 APP 对外服务的端口，<code>kubectl run</code> 会下载镜像，然后创建 <code>deployment</code>，根据 deployment 创建 APP。deployment 就像是 APP 的说明书，它指导怎么创建 和维护 APP。APP 创建完运行在 Docker 容器中，使用 <code>kubectl get deployments</code> 可以查看 deployment 的信息。</p><p><font color="red" size="5">第三步：</font><strong>探索 APP</strong>  </p><p>上一步创建完 deployment，会接着创建 Pod 来运行 APP 容器，K8S 使用 Pod 来管理容器资源，一个 Pod 可以包含一个或多个容器，在这个例子，一个 Pod 就只有一个 APP 容器。使用 <code>kubectl get pods</code> 查看当前 Pod 信息。</p><center><img src="/images/k8s/getpods.png" alt=""></center><p>更详细信息使用 <code>kubectl describe pods</code> 查看。</p><p>kubectl 工具对于排错很有帮助，下面几个是较为常用的命令：</p><ul><li><strong>kubectl get</strong> - 列出资源</li><li><strong>kubectl describe</strong> - 显示资源的详细信息</li><li><strong>kubectl logs</strong> - 输出 Pod 中容器的日志</li><li><strong>kubectl exec</strong> - 在 Pod 容器中执行命令</li></ul><p><font color="red" size="5">第四步：</font><strong>访问 APP</strong>  </p><p>默认情况下，所有 Pod 都只能在集群内部访问，上面看到每个 Pod 有 IP 和端口，Pod 之间可以直接访问。外部想要访问 Pod， 需要将端口暴露出去，执行如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment/kubernetes-bootcamp --type=&quot;NodePort&quot; --port 8080</span><br></pre></td></tr></table></figure></p><p>将容器的端口（8080）映射到节点的端口。</p><p>执行 <code>kubectl get services</code> 查看映射到的节点的端口。</p><center><img src="/images/k8s/service2.png" alt=""></center><p>可以看到容器的 8080 端口已经映射到节点的 31915 端口。外部可以通过 <code>NodeIP:Port</code> 的方式就可以访问到 Pod 内的容器，如下：</p><center><img src="/images/k8s/curservice.png" alt=""></center><p><code>service</code> 是 K8S 中对 Pod 的进一步抽象，是外部访问 Pod 的入口。如果把 K8S 集群想象成一个组织，那么 service 就是这个组织的接口人，为什么需要 service，这个留作后面的内容再讲，在这里你可以把它暂时理解成端口映射。  </p><p><font color="red" size="5">第五步：</font><strong>APP 的弹性伸缩</strong> </p><p>为了满足高可用，Pod 可以自动扩容和缩容。默认情况下，Pod 只会运行一个副本，这是由 deployment 定义的，可以通过 <code>kubectl get deployments</code> 查看副本数，通过 <code>kubectl scale deployments/app --replicas=num</code> 增加或减少副本数。 </p><p>比如，增加副本数到 4 个： </p><center><img src="/images/k8s/scaleup.png" alt=""></center><p>看到 Pod 数也增加到了 4 个。</p><p>减少副本数为 2 个：  </p><center><img src="/images/k8s/scale_down.png" alt=""></center><p>看到两个副本显示 Terminating，表示正在中止，过段时间再看就只有两个了。</p><p>对于多副本的情况，访问 APP 会实现负载均衡，如下：</p><center><img src="/images/k8s/scaleservice.png" alt=""></center><p>看到每次请求访问都落在不同的 Pod 上，这个功能是由 service 来完成的。</p><p><font color="red" size="5">第六步：</font><strong>更新 APP</strong>   </p><p>当前 APP 使用的镜像版本是 v1，需要升级到 v2，执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl set images deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2</span><br></pre></td></tr></table></figure><center><img src="/images/k8s/update.png" alt=""></center><p>看到升级过程是先中止之前的 4 个副本，再重开 4 个副本。</p><p>如果回退到 v1 版本，只用执行如下命令即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployments/kubernetes-bootcamp</span><br></pre></td></tr></table></figure><center><img src="/images/k8s/rollout.png" alt=""></center><p>至此，我们已经通过官方这个 demo 体验了一把 K8S 的功能和使用方法，下面我会陆陆续续把自己学习 K8S 的笔记整理出来，分享给你，希望对你有帮助。如有可能，请随手转发分享一下，让更多的人也参与进来。</p><p>最后，还是继续送书，容器网络专家倪朋飞写的《K8S 指南》电子书，如有需要后台回复“K8S”（之前回复过就不用回复了）。如需加群学习回复“加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/4.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper！欢迎大家和我一起学习 K8S。&lt;/p&gt;
&lt;p&gt;从前面的文章我们知道，Kubernetes 脱胎于 Google 的 Borg，Borg 在 Kubernetes 诞生之初已经在 Google 内部身经百战 10 余年，且不说它的历史源远流长，就凭它是出自 Google 那帮天才工程师之手，就知道它的学习难度不低。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 01 初识 Kubernetes 新时代的领航者</title>
    <link href="https://chambai.github.io/2018/08/30/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E7%AC%94%E8%AE%B0_01_%E5%88%9D%E8%AF%86_Kubernetes_%E6%96%B0%E6%97%B6%E4%BB%A3%E7%9A%84%E9%A2%86%E8%88%AA%E8%80%85/"/>
    <id>https://chambai.github.io/2018/08/30/云计算/Kubernetes_笔记_01_初识_Kubernetes_新时代的领航者/</id>
    <published>2018-08-30T05:16:14.000Z</published>
    <updated>2019-02-21T14:34:54.485Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/2.jpg" alt=""></center><p>Hi，大家好，我是 CloudDeveloper！欢迎大家和我一起学习 K8S。</p><p>大明王朝时期，明成祖朱棣为了发展海外贸易和建立自己的声望，派郑和七下西洋，创下了这段中国古代规模最大、船只最多（240多艘）、海员最多（2.7 万人）、时间最久的，比欧洲国家航海时间早半个多世纪的远洋航行壮举。</p><a id="more"></a><center><img src="/images/k8s/3.jpg" alt=""></center><p>Kubernetes 这个名字起源于古希腊，是「舵手」的意思，所以它的 Logo 既像一张渔网，又像一个罗盘。如果 Docker 把自己定位为驮着集装箱在大海上遨游的鲸鱼，那么 Kubernetes 就是掌舵大航海时代话语权的舵手，指挥着这条鲸鱼按照主人设定的路线巡游。</p><center><img src="/images/k8s/dockervsk8s.jpg" alt=""></center><p>Kubernetes 脱胎于 Google 老牌的集群管理软件「Borg」，虽然自诞生至今才三年多（第一个正式版本 Kubernetes 1.0 于 2015 年 7 月才正式发布），但要论其历史，却是早已在 Google 内部身经百战 10 余年，Kubernetes 站在 Borg 这个前辈的肩膀上，吸取了它过去十年间的经验和教训，才有了今天的成绩。这也是 Docker 火了之后，Google 迫不及待想推 Kubernetes 的原因。</p><p>Kubernetes 也常被人们称为「K8S」，原因是 K 和 S 之间正好有 8 个字母，读音上也和 8 相似，为了阅读方便，人们都乐于这么称呼。</p><p>有了 Google 的背书，K8S 一经推出就一鸣惊人，迅速称霸容器技术领域。</p><p>可以说，K8S 是以容器技术为基础打造的一个云计算时代的全新分布式系统架构，它的架构设计开放，除了支持 Docker，还支持其他容器技术，比如 Rocket、RKT 等。</p><p>Google 的加持，开放的生态，让它在与其他竞争对手的 博弈中占据上风，轻松拿下容器编排这个市场。</p><p>2017年9月，Mesosphere 宣布支持 K8S，接着，10月，Docker 在 DockerCon EU 2017 大会上也宣布拥抱 K8S，至此，K8S 正式霸占容器技术领域，彻底掌控容器技术的未来。</p><center><img src="/images/k8s/vsplat.jpg" alt=""></center><p>K8S 为了扩大影响力，推出没多久就加入 OpenStack 阵营，目的是希望 K8S 能被 OpenStack 生态圈所容纳，与 KVM 虚拟机一样成为 OpenStack 平台上的一等公民。</p><blockquote><p>这意味着以容器为代表的应用形态和以虚拟机为代表的系统形态将完美融合于 OpenStack 之上，并与软件定义网络和软件定义存储一起统治下一代数据中心。</p></blockquote><p>K8S 在云计算领域刮起了一道强劲之风，但凡跟云计算相关的公司都无法无视它的存在，错过它，也许就错过了未来。我们来看看它从诞生至今的 Google 趋势（和 Docker Swarm 和 Mesos 进行了对比）：</p><center><img src="/images/k8s/k8strend.jpg" alt=""></center><p>可以看到，K8S 从诞生之初便一路飙升，将对手甩开了十几条街，未来也将会以火箭的速度保持上升。</p><p>目前，除了云计算相关的公司，很多互联网公司、甚至传统企业都在纷纷布局自家的 K8S 产品，可以说，K8S 是当前容器行业最炙手可热的明星。</p><p>作为一个 IT 从业人员，你无法忽视它的存在。谁能比别人领先一步掌握新技术，谁就能在竞争中赢得了先机。</p><p>虽然说，现在学习 K8S 并不是最佳时机，但还不算太晚，就像一句话说的：</p><blockquote><p>学习一门技术最好的时间是 10 年前，其次是现在。</p></blockquote><p>后面我会推一个我学习 K8S 的笔记教程，一方面是加深自己对知识的理解，另一方面也是希望能分享给有需要的人。分享是一种美德，你在看到我的分享的同时，也希望你能动动手指把它分享给你的朋友，这样我的分享也没有白费。</p><p>最后，我这里有一份 《K8S 指南》，这是容器网络专家倪朋飞利用自己业余时间写的一本小册子，质量还是挺不错的，有需要的后台回复“K8S”。另外需要加群学习的后台回复“加群”。</p><blockquote><p>文中图片来源于网络，侵权必删<br>参考：k8s 指南</p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/2.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 CloudDeveloper！欢迎大家和我一起学习 K8S。&lt;/p&gt;
&lt;p&gt;大明王朝时期，明成祖朱棣为了发展海外贸易和建立自己的声望，派郑和七下西洋，创下了这段中国古代规模最大、船只最多（240多艘）、海员最多（2.7 万人）、时间最久的，比欧洲国家航海时间早半个多世纪的远洋航行壮举。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 新时代的宠儿</title>
    <link href="https://chambai.github.io/2018/08/28/%E4%BA%91%E8%AE%A1%E7%AE%97/Kubernetes_%E6%96%B0%E6%97%B6%E4%BB%A3%E7%9A%84%E5%AE%A0%E5%84%BF/"/>
    <id>https://chambai.github.io/2018/08/28/云计算/Kubernetes_新时代的宠儿/</id>
    <published>2018-08-28T05:16:14.000Z</published>
    <updated>2019-02-21T14:31:56.751Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="Kubernetes-是什么"><a href="#Kubernetes-是什么" class="headerlink" title="Kubernetes 是什么"></a>Kubernetes 是什么</h3><hr><p>Kubernetes 简称为 K8S。简单说，K8S 是一个用于容器集群的分布式系统架构。首先，它是基于容器技术，容器是和虚拟机并列的一种虚拟化技术，相比虚拟机来说，容器更加轻量，资源利用率更高，更适合于云原生应用。</p><a id="more"></a><p>其次，K8S 掌管的是容器集群，就像它的名字一样，一个舵手指挥着一个个的集装箱航行。容器会被频繁地销毁、重建和调度，为了最大化地利用集群资源和减少人力成本，K8S 在其中以高效的策略，自动化的运维方式指挥着这一切，就像一台永动机一样，管理员可以一劳永逸。</p><center><img src="/images/k8s/k8swhat.jpg" alt=""></center><p>最后，K8S 的架构非常开放，分布式的组件结构，使得它可以轻松地适应大规模的集群环境，Google 庞大的数据中心就是它最好的历练。</p><h3 id="为什么-K8S-能赢？"><a href="#为什么-K8S-能赢？" class="headerlink" title="为什么 K8S 能赢？"></a>为什么 K8S 能赢？</h3><hr><p>随着 2014 年 Docker 大火之后，已经涌现出大量的容器集群管理平台，其中，Docker 自家的 Swarm，在 Twitter 内部久经考验的 Mesos，以及 Google 的 K8S 最为知名，号称容器编排三驾马车。下图是三家的热度走势图：</p><center><img src="/images/k8s/k8strend.jpg" alt=""></center><p>K8S 自诞生日起便一骑绝尘，甩对手十几条街。为什么 K8S 能赢？我自以为是生态。</p><p>K8S 架构开放，向下可以容纳各种 container runtime，便不是没了 Docker 不行。向上可以承载各种 PaaS 平台，还能和 OpenStack、VMware 这些 IaaS 平台和平相处。它由此组建的生态系统，随随便便可以吃下任何一个平台。再加上 Google 的加持，谁能不爱？</p><h3 id="有哪些公司在使用-K8S？"><a href="#有哪些公司在使用-K8S？" class="headerlink" title="有哪些公司在使用 K8S？"></a>有哪些公司在使用 K8S？</h3><hr><p>据不完全统计，除了 AWS、Azure、Google、Microsoft 等巨头在容器领域里多年的博弈外，国内的很多互联网公司，如 BAT、蚂蚁、今日头条、滴滴等技术大厂，也都将容器和 K8S 列入我来的战略重心，无数中小型企业也正走在容器化的道路上。</p><p>从长远角度来看，K8S 将会成为企业服务器端技术栈标准中的一环，并连同它所推崇的容器化理念，成为广大后端技术人员和开发者的一门必修课。</p><h3 id="怎么学-K8S？"><a href="#怎么学-K8S？" class="headerlink" title="怎么学 K8S？"></a>怎么学 K8S？</h3><hr><p>现在快餐时代，如何学习才能更高效？我觉得排在第一位的应该是站在巨人的肩膀上学习。国内有很多研究 K8S 的大牛，其中一批是浙江大学研究所的研究员，他们出了国内第一本深入解读 Docker 和 K8S 原理的书《容器与容器云》。</p><p>看书虽然效果是奇好的，但效率并不高，想要效率高，我觉得学习大牛的知识总结可能才是最有效的。</p><p>正巧，今天我看到那批研究员中的一位作者张磊（现在在微软研究院）在极客时间开了一个 K8S 专栏，我觉得是雨后逢甘露，第一时间就买了。</p><p>这里简单给大家介绍下，有需要的朋友一定不要错过。</p><p>课程有 51 节，原价 99 元，现在优惠 68 元，9月8日恢复原价，如果你扫我下面的二维码买的话，我<font color="red">返你 8 元</font>（注意：这个是我特地给你的福利，别的地方是没有的），也就是说，你只用<font color="red"> 60 元</font>就可以买了，每节课 1 块多一点。另外，你买了之后还可以以同样的方式分享给你的好友，双方都受益。如果你讨厌这样的方式，那么忽略就好。</p><p>我作为一名云计算爱好者，一方面是希望为前辈们宣传一波，另一方面我也是给我的读者们尽量争取一些实打实的福利。希望能帮助到你。</p><p>大家买了之后记得加我微信哈，我返你钱。</p><p>另外，K8S 是用 Go 语言写的，我推荐大家和 Go 一起学效果最好，大家可以看我之前写的这篇文章学习 Go 语言最好的时间是 10 年前，其次是现在。</p><p>下面还有一个目录，大家可以看看，真的很良心。</p><center><img src="/images/k8s/k8scourse.jpg" alt=""></center><p>课程目录：</p><center><img src="/images/k8s/k8sdir.jpg" alt=""></center><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;Kubernetes-是什么&quot;&gt;&lt;a href=&quot;#Kubernetes-是什么&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes 是什么&quot;&gt;&lt;/a&gt;Kubernetes 是什么&lt;/h3&gt;&lt;hr&gt;
&lt;p&gt;Kubernetes 简称为 K8S。简单说，K8S 是一个用于容器集群的分布式系统架构。首先，它是基于容器技术，容器是和虚拟机并列的一种虚拟化技术，相比虚拟机来说，容器更加轻量，资源利用率更高，更适合于云原生应用。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>集群：孙悟空分身术</title>
    <link href="https://chambai.github.io/2018/08/23/%E4%BA%91%E8%AE%A1%E7%AE%97/%E9%9B%86%E7%BE%A4%EF%BC%9A%E5%AD%99%E6%82%9F%E7%A9%BA%E5%88%86%E8%BA%AB%E6%9C%AF/"/>
    <id>https://chambai.github.io/2018/08/23/云计算/集群：孙悟空分身术/</id>
    <published>2018-08-23T05:16:14.000Z</published>
    <updated>2019-02-21T14:33:15.151Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>在孙悟空的七十二变中，我觉得最厉害的非分身能力莫属，这也是他百试不得其爽的终极大招，每每都能打得妖怪摸不着北。</p><a id="more"></a><center><img src="/images/k8s/mon.jpg" alt=""></center><p>集群，学名叫 Cluster，可以翻译为簇、聚类、集群等多种意思，不同的翻译，在技术世界里所表示的意思都不尽相同，但都有一个共同的指向，即群体。集群就是由一组计算机所组成的实体，通常作为一个整体向用户提供资源。<br>集群的研究和发展离不开人们对高性能计算的追求，像我们熟悉的向量机、对称多处理机、工作站、超级计算机等等都是对高性能计算追求下的产物。</p><p>这些系统要么是提高 CPU 的主频和总线带宽来提高系统性能，要么是增加 CPU 个数和内存容量来提高性能，但这些手段对性能的提高都是有限的。有人做过实验，当 CPU 个数超过某一阈值时，系统的性能反而会变差。其主要的瓶颈就在于 CPU 访问内存的带宽并不能随着 CPU 个数的增加而有效增加。</p><center><img src="/images/k8s/clusterperf.jpg" alt=""></center><p>相反，集群系统的性能可扩展能力是线性增长的。我们可以简单通过增加机器数来增加集群的运算能力，相比购买高性能的大型计算机，同等运算能力下，我们可以获得更高的性价比。同时，系统的可靠性也得到了增强。</p><h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><hr><p>早在七十年代计算机厂商和研究机构就开始了对集群系统的研究和开发，首先创造性发明集群的是 Seymour Cray（西摩·克雷）—— 超级计算机之父。</p><center><img src="/images/k8s/cray.jpg" alt=""></center><p>Seymour 是一位美国工程师，在 1960 年代，CDC 公司涉足高性能计算领域，彼时还是大型机的天下，这些大型机设计非常复杂，生产周期漫长，价格还非常昂贵。当时在 CDC 公司担任总设计师的 Seymour 决心建造出心目中的高性能计算机。</p><p>Seymour 出于工程师的直觉，很快想到并行是提高计算机性能的有效方式。他使用廉价的方式来获得跟大型机一样的运算能力。他将多个普通的处理器连接起来，使它们能够协同工作，这就是高性能计算机的原型。</p><p>后来，IBM、HP 等公司学习了 Seymour 的这套架构，高性能计算机开始迅速推广，逐步取代原有的大型机。高性能计算机为当时的登月计划等大型科研项目作出了非常重要的贡献。</p><p>然后进入八十年代，在摩尔定律的指导下，CPU 频率不断提高，芯片不断降价，个人计算机强势崛起。苹果、微软等公司借助这股东风成为个人计算机时代的王者。随之而来的就是高性能计算机市场遭到了吞噬，被迫只能退守公司服务器市场。</p><p>但很快，随着互联网的普及，高性能计算机又迎来新的一波热潮。互联网上用户量庞大，普通 PC 难以应付如此众多的网络请求，必须要依赖由高性能计算机组成的服务器集群。在 2000 年左右的网络泡沫时期，成就了很多像 Sun 这样的服务器生产商。</p><p>如今，IT 行业向云计算冲击，诸如 Google、Apple、Amazon 等很巨头纷纷建立起了自己的数据中心。集群的规模在不断扩大，为海量的数据提高基础设施提供了支撑。根据不同的应用场景，集群也演变出多种形态，比如高性能集群、高可用集群、负载均衡集群等等。</p><h3 id="集群元素"><a href="#集群元素" class="headerlink" title="集群元素"></a>集群元素</h3><hr><p>集群不是简单的硬件堆叠，而是硬件和软件的结合。从软件上说，集群至少需要：</p><p>构建于 TCP/IP 协议上的通信软件，用于集群中节点之间的通信。</p><p>一套中心管理软件，用于统一管理集群中节点的资源、任务和容错等等。</p><p>这两点比较好理解，集群的规模往往是比较庞大的，对于管理员来说，需要随时能够知晓集群中各节点的业务正常与否，出问题了应该怎么保证业务能够不中断，遇到流量高峰和低谷的时候，又该怎么响应，这些操作如果纯靠人工来完成那必将很惨烈。依靠软件和网络来完成自动化的管理方式，可以将管理员解放出来。当然，以上说的两点是比较宽泛的，用户可以根据自身需求来部署不同的集群元素。</p><p>一个比较经典的集群模型当属 Beowulf 集群，它通过一个节点统一将来自网络的请求分配给各个节点进行计算处理。</p><center><img src="/images/k8s/beowulf.jpg" alt=""></center><h3 id="集群与分布式"><a href="#集群与分布式" class="headerlink" title="集群与分布式"></a>集群与分布式</h3><hr><p>集群与分布式像一对孪生兄弟，傻傻分不清楚。在我看来，它们之间没有特别明确的分界线，集群离不开分布式，分布式也需要集群。如果一定要做个区分，可以套用一个比喻来描述两者的区别：</p><p>一家餐厅刚开业，由于成本限制招了一个厨师，慢慢地，餐厅生意越做越好，一个厨师已经很难应付过来，于是又招了一个，这两个厨师水平相当，都能做同样的事，两个厨师之间的关系就是集群。两厨师除了炒菜，还要负责洗菜、配菜等等的活，工作负荷已经严重超标，为了让厨师能专心炒菜，把菜做到极致，餐厅又招了配菜师来辅助厨师，厨师和配菜师之间的关系就是分布式。</p><p>这个例子比较形象，在网站开发中也有类似的关系，两个全栈工程师之间就是集群的关系，前端工程师和后端工程师之间就属于分布式的关系。</p><center><img src="/images/k8s/cluint.jpg" alt=""></center><p>@知乎大闲人柴毛毛</p><p>所以，一定要有区分的话就是：集群是一个业务部署在多个服务器上，而分布式是一个业务拆分成多个子业务部署在不同的服务器上。但在实际部署中，为了高性能，需要分布式部署，为了高可用，需要集群部署，这两者都是业务所必须的指标。所以，集群和分布式之间的关系是相互补充的。</p><h3 id="虚拟化"><a href="#虚拟化" class="headerlink" title="虚拟化"></a>虚拟化</h3><hr><p>随着虚拟化技术的发展，一台服务器可以虚拟出多个虚拟机，对外提供业务，这种方式大大提高了资源的利用率，集群的部署也逐步从物理机过渡到虚拟机，灵活性大大提高。但同时也带来了更多新的研究课题。虚拟化计算、虚拟化存储、虚拟化网络、虚拟化安全等等这些课题共同推动着云计算产业迈出一个又一个的台阶。</p><h3 id="数据中心"><a href="#数据中心" class="headerlink" title="数据中心"></a>数据中心</h3><hr><p>数据中心是集中存放和运行服务器的地方，是规模最大的集群。随着云计算和大数据概念的风起云涌，Google、Amazon 等这些明星公司幕后的数据中心也开始走入大众的视野。数据中心要求有优秀的架构设计、电路设计、空间设计等等，还要有机制能够应对各种各样的意外，否则一点小小的失误，公司的股价恐怕就要跳水。</p><p>地理位置的选择也是数据中心考虑的一个指标，随着绿色数据中心概念的兴起，越来越多人关注数据中心所带来的能源问题和环境问题，选择一个远离市区，并且能利用天然水源和气温的地方，将会为数据中心的建设节约大量的成本。Google 等大公司的数据中心就有意放在高纬度、高海拔的地区，以及有湖泊、河流流经地区，以享受天然的空调和冷却水。</p><center><img src="/images/k8s/datacenter.jpg" alt=""></center><p>云计算之所以能被称之为“云”计算，是因为具有体量庞大的资源池，集群就是用来构建这个资源池的，说集群是云计算的基石一点都不为过。目前有很多集群管理软件，大家比较熟悉的像 Mesos，k8s 都属于这个范畴，后面会带来相关的干货，大家尽情期待。</p><blockquote><p>PS：文中图片均来自于网络，侵权必删<br>参考：<a href="http://dwz.cn/h9lwjvOR" target="_blank" rel="noopener">http://dwz.cn/h9lwjvOR</a></p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在孙悟空的七十二变中，我觉得最厉害的非分身能力莫属，这也是他百试不得其爽的终极大招，每每都能打得妖怪摸不着北。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
      <category term="集群" scheme="https://chambai.github.io/tags/%E9%9B%86%E7%BE%A4/"/>
    
  </entry>
  
  <entry>
    <title>再谈云计算技能图谱</title>
    <link href="https://chambai.github.io/2018/08/15/%E4%BA%91%E8%AE%A1%E7%AE%97/%E5%86%8D%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1/"/>
    <id>https://chambai.github.io/2018/08/15/云计算/再谈云计算技能图谱/</id>
    <published>2018-08-15T05:16:14.000Z</published>
    <updated>2019-02-22T15:21:56.330Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>之前，我写过一篇「云计算技能图谱」的文章，涵盖了云计算领域绝大部分的分支，很多人看了表示不淡定了——学完这个要等到猴年马月！</p><p>其实那份图谱涉及到很多应用场景，比如说大数据，机器学习，这些是基于云计算引申的技术分支，底层用的是云计算的基础设施，但要说可不可以独立于云计算来做，可以，只是一个规模的问题罢了。</p><p>为了能给很多初学者一个好的引导，我重新整理了这份图谱，把一些相关联的技术分支去掉了，只保留了基础设施部分（包括计算、存储、网络、安全这几个部分）。如下：</p><center><img src="/images/cloud/cloud_tupu.jpg" alt=""></center><p>备注：图片为防抄袭迫不得已加水印，想要原图的可以加我微信私信我</p><p>这样来看，就显得精简多了。</p><p>可能你看到这个还是会很焦虑，其实大可不必焦虑，图谱更多告诉你的是这个领域有什么，至于做不做完全根据你自己的情况选择，比如你想做个 T 型人才 ，那就尽可能去学，想做个 I 型人才，那就专注在某一个领域就好了。这两者没有绝对的孰是孰非，最终都是要解决问题。就像一句话说的，不管黑猫白猫，能捉到老鼠的就是好猫。</p><p>我知道关注我的读者当中，什么人才都有，我目前知道的，有学生，有工作了好几年的老司机，也有博士，首先要感谢大家的关注，我相信大家关注我肯定是因为我的哪一篇文章触动了你或者对你有帮助才会关注的。</p><p>我想说，大家关注我肯定是没错的，我这个号专注的内容就是上面这份图谱提到的内容，你可以在这里看到最基础的开发实践内容（比如 Linux、C/C++、Python、Go 技术栈），也可以看到云计算框架的解读（比如 KVM，OpenStack，Docker，Kubernetes），还可以看到最前沿技术的探讨。当然也有一些非技术的内容，比如行业资讯，以及我一些不吐不快的碎碎念。</p><p>其实我进入这个领域也不算早，跟很多读者比起来，是不折不扣的菜鸟，但正因为我是菜鸟，我写出的文章才会通俗易懂，因为我要保证和我一样的菜鸟能听得懂，当然了，质量肯定是第一位的，你们要是看过我以前写的一些文章就知道质量如何了，绝对是很良心的分享。说这个主要是希望大家能多多向你身边的朋友推荐下我这个号，有更多的朋友加入，我的写作动力就越强，就能给你们输出更多更好的文章。</p><p>为了能让大家有一个交流的氛围，我建了一个群，想加入的可以后台回复“加群”。</p><p>另外，我这里还收藏了一套很有价值的技能图谱，包括上面说的很多细分领域，比如 Python、Docker、Kubernetes、DevOps，还有一些其他的分支，比如机器学习，大数据，架构师，运维，嵌入式等等等等，大概就像下面这样子：</p><center><img src="/images/cloud/cloud_tupuinfo.jpg" alt=""></center><p>这些资料是我精心为大家整理的，整理不易，大家如果需要，有一点点要求，只要你乐于分享即可。这里要说明一点，我觉得好的东西，就是要让更多的人看到，你可以说是诱导你分享，但扪心自问，遇到好的东西谁又不乐意分享呢，让你的朋友看到你分享好东西给他们又何尝不是一种快乐呢。</p><p>获取技能图谱方法：</p><ol><li>转发本文到你的朋友圈；</li><li>添加我的微信号：aCloudDeveloper，或长按下方二维码加我微信；</li><li>加好友后发朋友圈截图给我，我看到会发一整套技能图谱给你，或者你也可以回复“加群”，我拉你进我的技术交流群。</li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;之前，我写过一篇「云计算技能图谱」的文章，涵盖了云计算领域绝大部分的分支，很多人看了表示不淡定了——学完这个要等到猴年
      
    
    </summary>
    
      <category term="01 云计算" scheme="https://chambai.github.io/categories/01-%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="技能图谱" scheme="https://chambai.github.io/tags/%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>Linux探秘之 I/O 效率</title>
    <link href="https://chambai.github.io/2018/07/20/%E4%BA%91%E8%AE%A1%E7%AE%97/Linux%E6%8E%A2%E7%A7%98%E4%B9%8B_I:O_%E6%95%88%E7%8E%87/"/>
    <id>https://chambai.github.io/2018/07/20/云计算/Linux探秘之_I:O_效率/</id>
    <published>2018-07-20T05:16:14.000Z</published>
    <updated>2018-10-06T02:52:24.094Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="文章来由"><a href="#文章来由" class="headerlink" title="文章来由"></a>文章来由</h3><hr><p>最近看了《UNIX环境高级编程》，以前一些比较模糊的知识又清晰了一步，特别是前两章讲到不带缓冲的文件 I/O 和带缓冲的标准 I/O，对 read、write、fread、fwrite、printf 等等这些函数又有了新的认识。</p><p>一个很大的感受是，我们很多时候开发都只注重上层逻辑，虽然一个项目接一个项目，看上去做了不少事，但是夜深人静掐指一想，究竟我们是否真正掌握了这些知识点，对于每一个知识点实现的机制我们是否能完整地说出来？这些东西最能体现一个人的基础知识是否扎实，我发现互联网公司的面试中最喜欢问这些基础知识，由一个很基本的函数层层递进引申出很多的问题。</p><p>很多时候我们内心可能会很排斥，甚至不屑于这些基础知识，想着等用到的时候，再来查，现在专注上层逻辑就好了，这样有助于提升我的开发效率。这样的想法貌似也没什么错，但是往往这就是瓶颈的来源，程序员最可怕的就是遇到瓶颈了。因为瓶颈这个东西是很难意识到的，一味追求实践而放弃理论学习，很容易就遇到瓶颈。（个人见解，不喜勿喷）</p><p>本文算是自己看完《UNIX 环境高级编程》文件 I/O 和标准 I/O 两章的读书笔记，文件 I/O 一章说不带缓冲，但后面又出现可带缓冲，搞得人有点晕，特意记下自己对此的理解。如果有什么不对的，欢迎指出，如果觉得本文对你有帮助，就动动手指推荐下，或者是粉我下，你的支持是我分享的最大动力。^_^</p><h3 id="缓冲机制"><a href="#缓冲机制" class="headerlink" title="缓冲机制"></a>缓冲机制</h3><hr><p>众所周知，CPU 和内存的数据交换要远大于磁盘，通过缓存机制，可以减少磁盘读写的次数，提高并发处理程序的效率，因此，缓存是一种提高任务存储和处理效率的有效方法。我们很多时候可以看到，缓存不仅在操作系统方面被采用，更是在 Web 技术、服务器端、分布式系统等领域发挥着及其重要的作用。</p><p>从宏观上看，Linux 操作系统分为用户态和内核态，在处理 I/O 操作的时候，两者都提供了缓存。用户态的称为标准 I/O 缓存，也称为用户空间缓存，而内核态的称为缓冲区高速缓存，也叫页面高速缓存。既然都提供了缓存，那为什么这本书上却分不带 I/O 的缓存和带 I/O 的缓存，原因其实是“不带 I/O缓存”指的是用户空间中不为这些 I/O 操作设有缓冲，而内核是带缓冲的，这样来看，就不会糊涂了。</p><h3 id="系统-I-O-和标准-I-O"><a href="#系统-I-O-和标准-I-O" class="headerlink" title="系统 I/O 和标准 I/O"></a>系统 I/O 和标准 I/O</h3><hr><p>系统 I/O，又称文件 I/O，或是内核态 I/O，引用文件的方式是通过文件描述符 fd，一个文件对应一个文件描述符。一个文件描述符用一个非负整数表示，0、1、2 系统默认表示标准输入、标准输出、标准错误。</p><p>某些 UNIX 系统规定了描述符的上限值 OPEN_MAX，这些常量都定义在头文件&lt;unistd.h&gt;中。当读或写一个文件时，使用 open 或 create 系统调用返回的文件描述符标识该文件，并将其作为参数传递给 read 或 write 系统调用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">ssize_t read(int filedes, void *buf, size_t nbytes);</span><br><span class="line">ssize_t write(int filedes, const void *buf, size_t nbytes);</span><br></pre></td></tr></table></figure><p>标准 I/O，又叫用户态 I/O，引用文件的方式则是通过文件流（stream），一般用 fopen 和 freopen 函数打开一个流，返回一个指向 FILE 对象的指针，其他函数如果要引用这个流，就将 FILE 指针作为参数传递。</p><p>一个进程预定义了三个流，并且这三个流自动被进程使用，它们是标准输入流、标准输出流和标准出错流。这三个流和系统 I/O 所规定的三个文件描述符所引用的文件相同。</p><p>当读或写一个文件时，不像系统 I/O，仅定义了 read 和 write 两个系统调用函数，标准 I/O 定义了多个函数，程序员可以根据自己的需求灵活使用。这些函数可以分为每次一个字符的 I/O，每次一行的 I/O 和直接 I/O（或者二进制 I/O、一次一个对象 I/O、面向记录的 I/O、面向结构的 I/O）。</p><p><strong>1）每次一个字符的 I/O</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;sdio.h&gt;</span><br><span class="line">/* 输入函数 */</span><br><span class="line">int getc(FILE *fp) //-&gt; 宏</span><br><span class="line">int fgetc(FILE *fp) //-&gt; 函数</span><br><span class="line">int getchar(void) //等价于getc(stdin)</span><br><span class="line"></span><br><span class="line">/* 输出函数 */</span><br><span class="line">int putc(int c, FILE *fp)</span><br><span class="line">int fputc(int c, FILE *fp)</span><br><span class="line">int putchar(int c) //等效于putc(c, stdout)</span><br></pre></td></tr></table></figure></p><p><strong>2）每次一行 I/O</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">/* 输入函数 */</span><br><span class="line">char *fgets(char *restrict buf, int n, FILE *restrict fp)</span><br><span class="line">char *gets(char *buf)</span><br><span class="line"></span><br><span class="line">/* 输出函数 */</span><br><span class="line">int fputs(cont char *restrict str, FILE *restrict fp)</span><br><span class="line">int puts(const char *str)</span><br></pre></td></tr></table></figure></p><p><strong>3）直接I/O</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">size_t fread(void *restrict ptr, size_t size, size_t nobj, FILE *restrict fp)</span><br><span class="line">size_t fwrite(const void *restrict ptr, size_t size, size_t nobj, FILE *restrict fp)</span><br></pre></td></tr></table></figure></p><p>到此，我们大概了解了系统 I/O 和标准 I/O 引用文件的方法，以及一些常用的 I/O 函数。下面通过一个图来详细看下当用户调用一个 I/O 函数时，用户态和内核态的一个执行流程是什么样的，进一步了解缓存在 I/O 操作中的作用，以及用户态 I/O 和内核态 I/O 在执行效率上的区别。</p><h3 id="I-O-操作的流程"><a href="#I-O-操作的流程" class="headerlink" title="I/O 操作的流程"></a>I/O 操作的流程</h3><hr><center><img src="/images/linux/linuxio.png" alt=""></center><p>如上图所示，用户进程空间和内核进程空间读写磁盘的操作都要经过缓冲区缓存，缓存的作用前面提到过，是为了减少磁盘读写的次数，提高 I/O 的效率。（来源于公众号：aCloudDeveloper，此处是防那些无脑SB抄袭，如影响阅读，还请大家见谅）当读写一个文件时，首先看系统 I/O 的操作流程。</p><p><strong>1、系统I/O：</strong>  属于内核系统调用，没有涉及用户态的参与。以图中标号为例：</p><p>③ 调用 write 函数向文件中写数据，buf 中存放的就是要写入的数据，如write(fd, ‘abc’, 3)。调用前需要先设置 BUFFSIZE。不同的 BUFFSIZE 会影响I/O 效率，后面再来说这个问题。</p><p>⑤ 延迟写：当缓存区高速缓存满或者内核要重写缓冲区的时候，才将数据写入输出队列，等数据到队列首部的时候，才真正触发磁盘的写操作。</p><p>⑥ 预读：当检测到正进行顺序读取时，内核就试图读入比应用程序所要求更多的数据，并假想应用程序很快就会读到这些数据。这样，当缓冲区没有数据时，能够快速填充下次要读取的数据。</p><p>④ 调用 read 从缓冲区高速缓存读取所需数据到逻辑单元中进行处理。</p><p>以上，就是系统 I/O 所涉及到的四步操作。</p><p><strong>2、标准 I/O：</strong> 属于 ISO C 实现的标准库函数，调用的是底层的系统调用。</p><p>① 将逻辑单元中的数据写入文件，根据需求，有三种函数类型可以调用，以fputc、fputs、fwrite 为例，这些函数不用人为去控制缓冲区的大小，而是系统自动申请的，当用户定义了相应的 I/O 函数之后，根据不同的缓存类型（是全缓冲、行缓冲还是无缓冲），系统自动调用 malloc 等函数申请缓冲区，即标准 I/O 缓存。</p><p>③⑤ 当用户缓冲区满了之后，如系统 I/O 操作一般，此时调用 write 从标准I/O 缓存中复制数据到内核缓冲区，再写入磁盘。</p><p>④⑥ 同系统 I/O 操作，从内核缓冲区调用 read 读入到用户缓冲区。</p><p>② 同样有三种函数类型可以调用，以 fgetc、fgets、fread 为例，读入逻辑单元进行后续的处理。</p><p>可见，标准 I/O 实现的机制就是基于系统 I/O，这样看来，标准 I/O 在效率上肯定不如系统 I/O，但事实是标准 I/O 与系统 I/O 相比并不慢很多，而且还有很多其他的优点，下面一一述说（本篇文章最重要的就是下一小节）。</p><h3 id="I-O-效率"><a href="#I-O-效率" class="headerlink" title="I/O 效率"></a>I/O 效率</h3><hr><center><img src="/images/linux/ioperf.jpg" alt=""></center><p>从图中可以看出，系统 I/O 效率受限于 read、write 系统调用的次数，而系统调用次数又受限于内核缓冲区的大小，即 BUFFSIZE，通过设置不同的 BUFFSIZE，得到的系统 CPU 时间是不同的，其最小值出现在 BUFFSIZE=4096 处，原因是该测试所采用的是 Linux ext2 文件系统，其块长为 4096 字节，也即缓冲区所能申请到的最大缓冲区大小，我们把 4096 字节看做是本次最佳 I/O 长度。</p><p>如果继续扩大缓冲区大小，对此时间几乎没有影响。所以，对于系统 I/O 操作，一个最大的问题就是：需要人为控制缓存的大小及最佳 I/O 长度的选择，另外就是系统调用与普通函数调用相比通常需要花费更多的时间，因为系统调用具体内核要执行这样的操作：1）内核捕获调用，2）检查系统调用参数的有效性，3）在用户空间和内核空间之间传输数据。</p><p>因此，引入标准 I/O 的目的就是为了通过标准 I/O 缓存来避免 BUFFSIZE 选择不当而带来的频繁的系统调用。根据用户不同的需求，选择不同的 I/O 函数，然后根据不同的缓存类型，自动调用 malloc 等缓存分配函数分配合适的缓存，等分配的缓存满之后，再调用系统 I/O 从标准 I/O 缓存向内核缓存拷贝数据，这样就进一步减少了系统调用的次数。</p><center><img src="/images/linux/ionum.png" alt=""></center><p>但是不同的标准 I/O 函数，不同的缓存类型也会带来不同的效率。如上图，当选择系统最佳 I/O 长度，即 BUFFSIZE 的大小和文件系统的块长一致，可以得到最佳的时间。</p><p>当选用标准 I/O 函数时，每次一个字符函数 fgetc、fputc 和每次一行函数 fgets、fputs 函数相比要花费较多的 CPU 时间，而每次单个字节调用系统 I/O 则花费更多的时间，如果是一个 100M 的文件，则要执行大概 2亿 次函数调用，也就引起 2亿 次系统调用（从用户缓冲区到内核缓冲区，再到磁盘），而 fgetc 版本也执行了 2亿 次函数调用，但只引起了大约 25222 次系统调用，所以，时间就大大减少了。</p><p>综合以上，标准 I/O 函数虽然基于系统 I/O 实现，但很大程度上减少了系统调用的次数，而且不用人为关心缓冲区大小的选择，整体上提高了 I/O 的效率。另外，标准 I/O 提供了多种缓存类型，方便程序员根据不同的应用需求选择不同的缓存要求，提高了编程的灵活性，当选择无缓存时，就相当于直接调用系统 I/O。</p><p>OK，大概的内容就以上这些，当然关于 I/O 操作这块还有很多需要注意的点，而且还有很多更加高级的 I/O 函数，这些在后面遇到再来做总结。最后，如果你觉得这篇文章对你有帮助就点赞转发支持下我吧，还是那句话，你的支持是我分享的最大动力。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;文章来由&quot;&gt;&lt;a href=&quot;#文章来由&quot; class=&quot;headerlink&quot; title=&quot;文章来由&quot;&gt;
      
    
    </summary>
    
      <category term="Linux" scheme="https://chambai.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="I/O" scheme="https://chambai.github.io/tags/I-O/"/>
    
  </entry>
  
  <entry>
    <title>Linux探秘之用户态与内核态</title>
    <link href="https://chambai.github.io/2018/07/06/%E4%BA%91%E8%AE%A1%E7%AE%97/Linux%E6%8E%A2%E7%A7%98%E4%B9%8B%E7%94%A8%E6%88%B7%E6%80%81%E4%B8%8E%E5%86%85%E6%A0%B8%E6%80%81/"/>
    <id>https://chambai.github.io/2018/07/06/云计算/Linux探秘之用户态与内核态/</id>
    <published>2018-07-06T05:16:14.000Z</published>
    <updated>2018-10-06T02:52:24.088Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="Unix-Linux的体系架构"><a href="#Unix-Linux的体系架构" class="headerlink" title="Unix/Linux的体系架构"></a>Unix/Linux的体系架构</h3><hr><p>如下图所示，从宏观上来看，Linux 操作系统的体系架构分为用户态和内核态（或者用户空间和内核）。</p><center><img src="/images/linux/linuxlevel.jpg" alt=""></center><p>内核从本质上看是一种软件——控制计算机的硬件资源，并提供上层应用程序运行的环境。用户态即上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源，包括 CPU 资源、存储资源、I/O 资源等。为了使上层应用能够访问到这些资源，内核必须为上层应用提供访问的接口：即系统调用。</p><p>系统调用是操作系统的最小功能单位，这些系统调用根据不同的应用场景可以进行扩展和裁剪，现在各种版本的 Unix 实现都提供了不同数量的系统调用，如 Linux 的不同版本提供了 240-260 个系统调用，FreeBSD 大约提供了 320 个（reference：UNIX 环境高级编程）。</p><p>我们可以把系统调用看成是一种不能再化简的操作（类似于原子操作，但是不同概念），有人把它比作一个汉字的一个“笔画”，而一个“汉字”就代表一个上层应用，我觉得这个比喻非常贴切。因此，有时候如果要实现一个完整的汉字（给某个变量分配内存空间），就必须调用很多的系统调用。如果从实现者（程序员）的角度来看，这势必会加重程序员的负担，良好的程序设计方法是：重视上层的业务逻辑操作，而尽可能避免底层复杂的实现细节。</p><p>库函数正是为了将程序员从复杂的细节中解脱出来而提出的一种有效方法。它实现对系统调用的封装，将简单的业务逻辑接口呈现给用户，方便用户调用，从这个角度上看，库函数就像是组成汉字的“偏旁”。这样的一种组成方式极大增强了程序设计的灵活性，对于简单的操作，我们可以直接调用系统调用来访问资源，如“人”，对于复杂操作，我们借助于库函数来实现，如“仁”。显然，这样的库函数依据不同的标准也可以有不同的实现版本，如ISO C 标准库，POSIX 标准库等。</p><p>Shell 是一个特殊的应用程序，俗称命令行，本质上是一个命令解释器，它下通系统调用，上通各种应用，通常充当着一种“胶水”的角色，来连接各个小功能程序，让不同程序能够以一个清晰的接口协同工作，从而增强各个程序的功能。</p><p>同时，Shell 是可编程的，它可以执行符合 Shell 语法的文本，这样的文本称为 Shell 脚本，通常短短的几行 Shell 脚本就可以实现一个非常大的功能，原因就是这些 Shell 语句通常都对系统调用做了一层封装。为了方便用户和系统交互，一般，一个 Shell 对应一个终端，终端是一个硬件设备，呈现给用户的是一个图形化窗口。我们可以通过这个窗口输入或者输出文本。这个文本直接传递给 Shell 进行分析解释，然后执行。</p><p>总结一下，用户态的应用程序可以通过三种方式来访问内核态的资源：</p><ul><li>系统调用</li><li>库函数</li><li>Shell 脚本</li></ul><p>下图是对上图的一个细分结构，从这个图上可以更进一步对内核所做的事有一个“全景式”的印象。主要表现为：向下控制硬件资源，向内管理操作系统资源：包括进程的调度和管理、内存的管理、文件系统的管理、设备驱动程序的管理以及网络资源的管理，向上则向应用程序提供系统调用的接口。</p><center><img src="/images/linux/linuxarch.jpg" alt=""></center><p>从整体上来看，整个操作系统分为两层：用户态和内核态，这种分层的架构极大地提高了资源管理的可扩展性和灵活性，而且方便用户对资源的调用和集中式的管理，带来一定的安全性。</p><h3 id="用户态和内核态的切换"><a href="#用户态和内核态的切换" class="headerlink" title="用户态和内核态的切换"></a>用户态和内核态的切换</h3><hr><p>因为操作系统的资源是有限的，如果访问资源的操作过多，必然会消耗过多的资源，而且如果不对这些操作加以区分，很可能造成资源访问的冲突。</p><p>所以，为了减少有限资源的访问和使用冲突，Unix/Linux 的设计哲学之一就是：对不同的操作赋予不同的执行等级，就是所谓特权的概念。简单说就是有多大能力做多大的事，与系统相关的一些特别关键的操作必须由最高特权的程序来完成。Intel 的 X86 架构的 CPU 提供了 0 到 3 四个特权级，数字越小，特权越高。</p><p>Linux 操作系统中主要采用了 0 和 3 两个特权级，分别对应的就是内核态和用户态。运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制。</p><p>很多程序开始时运行于用户态，但在执行的过程中，一些操作需要在内核权限下才能执行，这就涉及到一个从用户态切换到内核态的过程。比如C函数库中的内存分配函数 malloc()，它具体是使用 sbrk() 系统调用来分配内存，当malloc() 调用 sbrk() 的时候就涉及一次从用户态到内核态的切换，类似的函数还有 printf()，调用的是 wirte() 系统调用来输出字符串，等等。</p><center><img src="/images/linux/linuxchg.jpg" alt=""></center><p>那到底在什么情况下会发生从用户态到内核态的切换，一般存在以下三种情况：</p><ol><li>当然就是系统调用：原因如上的分析。</li><li>异常事件： 当 CPU 正在执行运行在用户态的程序时，突然发生某些预先不可知的异常事件，这个时候就会触发从当前用户态执行的进程转向内核态执行相关的异常事件，典型的如缺页异常。</li><li>外围设备的中断：当外围设备完成用户的请求操作后，会向 CPU 发出中断信号，此时，CPU 就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换。</li></ol><p><strong>注意：</strong> 系统调用的本质其实也是中断，相对于外围设备的硬中断，这种中断称为软中断，这是操作系统为用户特别开放的一种中断，如 Linux int 80h 中断。所以，从触发方式和效果上来看，这三种切换方式是完全一样的，都相当于是执行了一个中断响应的过程。但是从触发的对象来看，系统调用是进程主动请求切换的，而异常和硬中断则是被动的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>本文仅是从宏观的角度去理解 Linux 用户态和内核态的设计，并没有去深究它们的具体实现方式。从实现上来看，必须要考虑到的一点我想就是性能问题，因为用户态和内核态之间的切换会消耗大量资源。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;Unix-Linux的体系架构&quot;&gt;&lt;a href=&quot;#Unix-Linux的体系架构&quot; class=&quot;hea
      
    
    </summary>
    
      <category term="Linux" scheme="https://chambai.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>一文掌握 Linux 性能分析之网络篇（续）</title>
    <link href="https://chambai.github.io/2018/06/15/%E4%BA%91%E8%AE%A1%E7%AE%97/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1_Linux_%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87%EF%BC%88%E7%BB%AD%EF%BC%89/"/>
    <id>https://chambai.github.io/2018/06/15/云计算/一文掌握_Linux_性能分析之网络篇（续）/</id>
    <published>2018-06-15T05:16:14.000Z</published>
    <updated>2018-10-06T02:52:24.106Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>这是 Linux 性能分析系列的第五篇，前四篇在这里：<br>一文掌握 Linux 性能分析之 CPU 篇<br>一文掌握 Linux 性能分析之内存篇<br>一文掌握 Linux 性能分析之 IO 篇<br>一文掌握 Linux 性能分析之网络篇</p><p>在上篇中，我们已经介绍了几个 Linux 网络方向的性能分析工具，本文再补充几个。总结下来，余下的工具包括但不限于以下几个：</p><ul><li>sar：统计信息历史</li><li>traceroute：测试网络路由</li><li>dtrace：TCP/IP 栈跟踪</li><li>iperf / netperf / netserver：网络性能测试工具</li><li>perf 性能分析神器</li></ul><p>由于篇幅有限，本文会先介绍前面两个，其他工具留作后面介绍，大家可以持续关注。</p><h4 id="sar"><a href="#sar" class="headerlink" title="sar"></a><strong>sar</strong></h4><p>sar 是一个系统历史数据统计工具。统计的信息非常全，包括 CPU、内存、磁盘 I/O、网络、进程、系统调用等等信息，是一个集大成的工具，非常强大。在 Linux 系统上 <code>sar --help</code> 一下，可以看到它的完整用法。</p><ul><li>-A：所有报告的总和</li><li>-u：输出 CPU 使用情况的统计信息</li><li>-v：输出 inode、文件和其他内核表的统计信息</li><li>-d：输出每一个块设备的活动信息</li><li>-r：输出内存和交换空间的统计信息</li><li>-b：显示 I/O和传送速率的统计信息</li><li>-a：文件读写情况</li><li>-c：输出进程统计信息，每秒创建的进程数</li><li>-R：输出内存页面的统计信息</li><li>-y：终端设备活动情况</li><li>-w：输出系统交换活动信息</li><li>-n：输出网络设备统计信息</li></ul><p>在平时使用中，我们常常用来分析网络状况，其他几项的通常有更好的工具来分析。所以，本文会重点介绍 sar 在网络方面的分析手法。</p><p>Linux 系统用以下几个选项提供网络统计信息：</p><ul><li>-n DEV：网络接口统计信息。</li><li>-n EDEV：网络接口错误。</li><li>-n IP：IP 数据报统计信息。</li><li>-n EIP：IP 错误统计信息。</li><li>-n TCP：TCP 统计信息。</li><li>-n ETCP：TCP 错误统计信息。</li><li>-n SOCK：套接字使用。</li></ul><p>我们来看几个示例：</p><p><strong>（1）每秒打印 TCP 的统计信息：</strong></p><p><code>sar -n TCP 1</code></p><center><img src="/images/linux/sarn.png" alt=""></center><p>几个参数了解一下：</p><ul><li>active/s：新的 TCP 主动连接（也就是 socket 中的 connect() 事件），单位是：连接数/s。</li><li>passive/s：新的 TCP 被动连接（也就是 socket 中的 listen() 事件）。</li><li>iseg/s：接收的段（传输层以段为传输单位），单位是：段/s</li><li>oseg/s：发送的段。<br>通过这几个参数，我们基本可以知道当前系统 TCP 连接的负载情况。</li></ul><p><strong>（2）每秒打印感兴趣的网卡的统计信息：</strong></p><p><code>sar -n DEV 1 | awk &#39;NR == 3 || $3 == &quot;eth0&quot;&#39;</code></p><center><img src="/images/linux/sarnd.png" alt=""></center><p>几个参数了解一下：</p><ul><li>rxpck/s / txpck/s：网卡接收/发送的数据包，单位是：数据包/s。</li><li>rxkB/s / txkB/s：网卡接收/发送的千字节，单位是：千字节/s。</li><li>rxcmp/s / txcmp/s：网卡每秒接受/发送的压缩数据包，单位是：数据包/s。</li><li>rxmcst/s：每秒接收的多播数据包，单位是：数据包/s。</li><li>%ifutil：网络接口的利用率。<br>这几个参数对于分析网卡接收和发送的网络吞吐量很有帮助。</li></ul><p><strong>（3）错误包和丢包情况分析：</strong></p><p><code>sar -n EDEV 1</code></p><center><img src="/images/linux/sarne.png" alt=""></center><p>几个参数了解一下：</p><ul><li>rxerr/s / txerr/s：每秒钟接收/发送的坏数据包</li><li>coll/s：每秒冲突数</li><li>rxdrop/s：因为缓冲充满，每秒钟丢弃的已接收数据包数</li><li>txdrop/s：因为缓冲充满，每秒钟丢弃的已发送数据包数</li><li>txcarr/s：发送数据包时，每秒载波错误数</li><li>rxfram/s：每秒接收数据包的帧对齐错误数</li><li>rxfifo/s / txfifo/s：接收/发送的数据包每秒 FIFO 过速的错误数</li></ul><p>当发现接口传输数据包有问题时，查看以上参数能够让我们快速判断具体是出的什么问题。</p><p>OK，这个工具就介绍到这里，以上只是抛砖引玉，更多技巧还需要大家动手去探索，只有动手，才能融会贯通。</p><h4 id="traceroute"><a href="#traceroute" class="headerlink" title="traceroute"></a><strong>traceroute</strong></h4><p>traceroute 也是一个排查网络问题的好工具，它能显示数据包到达目标主机所经过的路径（路由器或网关的 IP 地址）。如果发现网络不通，我们可以通过这个命令来进一步判断是主机的问题还是网关的问题。</p><p>它通过向源主机和目标主机之间的设备发送一系列的探测数据包（UDP 或者 ICMP）来发现设备的存在，实现上利用了递增每一个包的 TTL 时间，来探测最终的目标主机。比如开始 TTL = 1，当到达第一个网关设备的时候，TTL - 1，当 TTL = 0 导致网关响应一个 ICMP 超时报文，这样，如果没有防火墙拦截的话，源主机就知道网关设备的地址。以此类推，逐步增加 TTL 时间，就可以探测到目标主机之间所经过的路径。</p><p>为了防止发送和响应过程出现问题导致丢包，traceroute 默认会发送 3 个探测包，我们可以用 -q x 来改变探测的数量。如果中间设备设置了防火墙限制，会导致源主机收不到响应包，就会显示 * 号。如下是 <code>traceroute baidu</code> 的结果：</p><center><img src="/images/linux/traceroute.png" alt=""></center><p>每一行默认会显示设备名称（IP 地址）和对应的响应时间。发送多少个探测包，就显示多少个。如果只想显示 IP 地址可以用 -n 参数，这个参数可以避免 DNS 域名解析，加快响应时间。</p><p>和这个工具类似的还有一个工具叫 pathchar，但平时用的不多，我就不介绍了。<br>以上就是两个工具的简单介绍，工具虽然简单，但只要能解决问题，就是好工具。当然，性能分析不仅仅依靠工具就能解决的，更多需要我们多思考、多动手、多总结，逐步培养自己的系统能力，才能融会贯通。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是 Linux 性能分析系列的第五篇，前四篇在这里：&lt;br&gt;一文掌握 Linux 性能分析之 CPU 篇&lt;br&gt;一文
      
    
    </summary>
    
      <category term="Linux" scheme="https://chambai.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="性能分析" scheme="https://chambai.github.io/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    
      <category term="网络" scheme="https://chambai.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>一文掌握 Linux 性能分析之网络篇</title>
    <link href="https://chambai.github.io/2018/06/01/%E4%BA%91%E8%AE%A1%E7%AE%97/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1_Linux_%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87/"/>
    <id>https://chambai.github.io/2018/06/01/云计算/一文掌握_Linux_性能分析之网络篇/</id>
    <published>2018-06-01T05:16:14.000Z</published>
    <updated>2018-10-06T02:52:24.111Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>这是 Linux 性能分析系列的第四篇。</p><p>比较宽泛地讲，网络方向的性能分析既包括主机测的网络配置查看、监控，又包括网络链路上的包转发时延、吞吐量、带宽等指标分析。包括但不限于以下分析工具：</p><ul><li>ping：测试网络连通性</li><li>ifconfig：接口配置</li><li>ip：网络接口统计信息</li><li>netsat：多种网络栈和接口统计信息</li><li>ifstat：接口网络流量监控工具</li><li>netcat：快速构建网络连接</li><li>tcpdump：抓包工具</li><li>sar：统计信息历史</li><li>traceroute：测试网络路由</li><li>pathchar：确定网络路径特征</li><li>dtrace：TCP/IP 栈跟踪</li><li>iperf / netperf / netserver：网络性能测试工具</li><li>perf ：性能分析神器</li></ul><p>本文先来看前面 7 个。</p><h4 id="ping"><a href="#ping" class="headerlink" title="ping"></a><strong>ping</strong></h4><hr><p>ping 发送 ICMP echo 数据包来探测网络的连通性，除了能直观地看出网络的连通状况外，还能获得本次连接的往返时间（RTT 时间），丢包情况，以及访问的域名所对应的 IP 地址（使用 DNS 域名解析），比如：</p><center><img src="/images/linux/ping.jpg" alt=""></center><p>我们 <code>ping baidu.com，-c</code>参数指定发包数。可以看到，解析到了 baidu 的一台服务器 IP 地址为 220.181.112.244。RTT 时间的最小、平均、最大和算术平均差分别是 40.732ms、40.762ms、40.791ms 和 0.248。</p><h3 id="ifconfig"><a href="#ifconfig" class="headerlink" title="ifconfig"></a><strong>ifconfig</strong></h3><hr><p>ifconfig 命令被用于配置和显示 Linux 内核中网络接口的统计信息。通过这些统计信息，我们也能够进行一定的网络性能调优。</p><p><strong>1）ifconfig 显示网络接口配置信息</strong></p><center><img src="/images/linux/ifconfig.jpg" alt=""></center><p>其中，RX/TX packets 是对接收/发送数据包的情况统计，包括错误的包，丢掉多少包等。RX/TX bytes 是接收/发送数据字节数统计。其余还有很多参数，就不一一述说了，性能调优时可以重点关注 MTU（最大传输单元） 和 txqueuelen（发送队列长度），比如可以用下面的命令来对这两个参数进行微调：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 txqueuelen 2000</span><br><span class="line">ifconfig eth0 mtu 1500</span><br></pre></td></tr></table></figure></p><p><strong>2）网络接口地址配置</strong></p><p>ifconfig 还常用来配置网口的地址，比如：<br>为网卡配置和删除 IPv6 地址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 add 33ffe:3240:800:1005::2/64    #为网卡eth0配置IPv6地址</span><br><span class="line">ifconfig eth0 del 33ffe:3240:800:1005::2/64    #为网卡eth0删除IPv6地址</span><br></pre></td></tr></table></figure></p><p>修改MAC地址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 hw ether 00:AA:BB:CC:dd:EE</span><br></pre></td></tr></table></figure></p><p>配置IP地址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 192.168.2.10</span><br><span class="line">ifconfig eth0 192.168.2.10 netmask 255.255.255.0</span><br><span class="line">ifconfig eth0 192.168.2.10 netmask 255.255.255.0 broadcast 192.168.2.255</span><br></pre></td></tr></table></figure></p><h4 id="IP"><a href="#IP" class="headerlink" title="IP"></a><strong>IP</strong></h4><hr><p>ip 命令用来显示或设置 Linux 主机的网络接口、路由、网络设备、策略路由和隧道等信息，是 Linux 下功能强大的网络配置工具，旨在替代 ifconfig 命令，如下显示 IP 命令的强大之处，功能涵盖到 ifconfig、netstat、route 三个命令。</p><center><img src="/images/linux/iptool.jpg" alt=""></center><h4 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a><strong>netstat</strong></h4><hr><p>netstat 可以查看整个 Linux 系统关于网络的情况，是一个集多钟网络工具于一身的组合工具。</p><p>常用的选项包括以下几个：</p><ul><li>默认：列出连接的套接字</li><li>-a：列出所有套接字的信息</li><li>-s：各种网络协议栈统计信息</li><li>-i：网络接口信息</li><li>-r：列出路由表</li><li>-l：仅列出有在 Listen 的服务状态</li><li>-p：显示 PID 和进程名称</li></ul><p>各参数组合使用实例如下：</p><ul><li>netstat -at 列出所有 TCP 端口</li><li>netstat -au 列出所有 UDP 端口</li><li>netstat -lt 列出所有监听 TCP 端口的 socket</li><li>netstat -lu 列出所有监听 UDP 端口的 socket</li><li>netstat -lx 列出所有监听 UNIX 端口的 socket</li><li>netstat -ap | grep ssh 找出程序运行的端口</li><li>netstat -an | grep ‘:80’ 找出运行在指定端口的进程</li></ul><p><strong>1）netstat 默认显示连接的套接字数据</strong></p><center><img src="/images/linux/netstat.jpg" alt=""></center><p>整体上来看，输出结果包括两个部分：</p><ul><li>Active Internet connections ：有源 TCP 连接，其中 Recv-Q 和 Send-Q 指的是接收队列和发送队列，这些数字一般都是 0，如果不是，说明请求包和回包正在队列中堆积。</li><li>Active UNIX domain sockets：有源 UNIX 域套接口，其中 proto 显示连接使用的协议，RefCnt 表示连接到本套接口上的进程号，Types 是套接口的类型，State 是套接口当前的状态，Path 是连接到套接口的进程使用的路径名。</li></ul><p><strong>2）netstat -i 显示网络接口信息</strong></p><center><img src="/images/linux/netstati.png" alt=""></center><p>接口信息包括网络接口名称（Iface）、MTU，以及一系列接收（RX-）和传输（TX-）的指标。其中 OK 表示传输成功的包，ERR 是错误包，DRP 是丢包，OVR 是超限包。</p><p>这些参数有助于我们对网络收包情况进行分析，从而判断瓶颈所在。</p><p><strong>3）netstat -s 显示所有网络协议栈的信息</strong></p><center><img src="/images/linux/netstats.jpg" alt=""></center><p>可以看到，这条命令能够显示每个协议详细的信息，这有助于我们针对协议栈进行更细粒度的分析。</p><p><strong>4）netstat -r 显示路由表信息</strong></p><center><img src="/images/linux/netstat.jpg" alt=""></center><p>这条命令能够看到主机路由表的一个情况。当然查路由我们也可以用 ip route 和 route 命令，这个命令显示的信息会更详细一些。</p><h4 id="ifstat"><a href="#ifstat" class="headerlink" title="ifstat"></a><strong>ifstat</strong></h4><hr><p>ifstat 主要用来监测主机网口的网络流量，常用的选项包括：</p><ul><li>-a：监测主机所有网口</li><li>-i：指定要监测的网口</li><li>-t：在每行输出信息前加上时间戳</li><li>-b：以 Kbit/s 显示流量数据，而不是默认的 KB/s</li><li>delay：采样间隔（单位是 s），即每隔 delay 的时间输出一次统计信息</li><li>count：采样次数，即共输出 count 次统计信息</li></ul><p>比如，通过以下命令统计主机所有网口某一段时间内的流量数据：</p><center><img src="/images/linux/ifstat.png" alt=""></center><p>可以看出，分别统计了三个网口的流量数据，前面输出的时间戳，有助于我们统计一段时间内各网口总的输入、输出流量。</p><h4 id="netcat"><a href="#netcat" class="headerlink" title="netcat"></a><strong>netcat</strong></h4><hr><p>netcat，简称 nc，命令简单，但功能强大，在排查网络故障时非常有用，因此它也在众多网络工具中有着“瑞士军刀”的美誉。</p><p>它主要被用来构建网络连接。可以以客户端和服务端的方式运行，当以服务端方式运行时，它负责监听某个端口并接受客户端的连接，因此可以用它来调试客户端程序；当以客户端方式运行时，它负责向服务端发起连接并收发数据，因此也可以用它来调试服务端程序，此时它有点像 Telnet 程序。</p><p>常用的选项包括以下几种：</p><ul><li>-l：以服务端的方式运行，监听指定的端口。默认是以客户端的方式运行。</li><li>-k：重复接受并处理某个端口上的所有连接，必须与 -l 一起使用。</li><li>-n：使用 IP 地址表示主机，而不是主机名，使用数字表示端口号，而不是服务名称。</li><li>-p：当以客户端运行时，指定端口号。</li><li>-s：设置本地主机发出的数据包的 IP 地址。</li><li>-C：将 CR 和 LF 两个字符作为结束符。</li><li>-U：使用 UNIX 本地域套接字通信。</li><li>-u：使用 UDP 协议通信，默认使用的是 TCP 协议。</li><li>-w：如果 nc 客户端在指定的时间内未检测到任何输入，则退出。</li><li>-X：当 nc 客户端与代理服务器通信时，该选项指定它们之间的通信协议，目前支持的代理协议包括 “4”（SOCKS v.4），“5”（SOCKS v.5）和 “connect” （HTTPs Proxy），默认使用 SOCKS v.5。</li><li>-x：指定目标代理服务器的 IP 地址和端口号。</li></ul><p>下面举一个简单的例子，使用 nc 命令发送消息：<br>首先，启动服务端，用 nc -l 0.0.0.0 12345 监听端口 12345 上的所有连接。</p><center><img src="/images/linux/netcat.png" alt=""></center><p>然后，启动客户端，用 nc -p 1234 127.0.0.1 12345 使用 1234 端口连接服务器 127.0.0.1::12345。</p><center><img src="/images/linux/netcatc.png" alt=""></center><p>接着就可以在两端互发数据了。这里只是抛砖引玉，更多例子大家可以多实践。</p><h4 id="tcpdump"><a href="#tcpdump" class="headerlink" title="tcpdump"></a><strong>tcpdump</strong></h4><hr><p>最后是 tcpdump，强大的网络抓包工具。虽然有 wireshark 这样更易使用的图形化抓包工具，但 tcpdump 仍然是网络排错的必备利器。</p><p>tcpdump 选项很多，我就不一一列举了，大家可以看文章末尾的引用来进一步了解。这里列举几种 tcpdump 常用的用法。</p><p><strong>1）捕获某主机的数据包</strong></p><p>比如想要捕获主机 200.200.200.100 上所有收到和发出的所有数据包，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump host 200.200.200.100</span><br></pre></td></tr></table></figure></p><p><strong>2）捕获多个主机的数据包</strong></p><p>比如要捕获主机 200.200.200.1 和主机 200.200.200.2 或 200.200.200.3 的通信，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump host 200.200.200.1 and \(200.200.200.2 or \)</span><br></pre></td></tr></table></figure></p><p>同样要捕获主机 200.200.200.1 除了和主机 200.200.200.2 之外所有主机通信的 IP 包。使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump ip host 200.200.200.1 and ! 200.200.200.2</span><br></pre></td></tr></table></figure></p><p><strong>3）捕获某主机接收或发出的某种协议类型的包</strong><br>比如要捕获主机 200.200.200.1 接收或发出的 Telnet 包，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump tcp port 23 host 200.200.200.1</span><br></pre></td></tr></table></figure></p><p><strong>4）捕获某端口相关的数据包</strong></p><p>比如捕获在端口 6666 上通过的包，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump port 6666</span><br></pre></td></tr></table></figure></p><p><strong>5）捕获某网口的数据包</strong><br>比如捕获在网口 eth0 上通过的包，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0</span><br></pre></td></tr></table></figure></p><p>下面还是举个例子，抓取 TCP 三次握手的包：<br>首先，用 nc 启动一个服务端，监听端口 12345 上客户端的连接：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -v -l 0.0.0.0 12345</span><br></pre></td></tr></table></figure></p><p>接着，启动 tcpdump 监听端口 12345 上通过的包：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i any &apos;port 12345&apos; -XX -nn -vv -S</span><br></pre></td></tr></table></figure></p><p>然后，再用 nc 启动客户端，连接服务端：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -v 127.0.0.1 12345</span><br></pre></td></tr></table></figure></p><p>最后，我们看到 tcpdump 抓到包如下：</p><center><img src="/images/linux/tcpdump.jpg" alt=""></center><p>怎么分析是 TCP 的三次握手，就当做小作业留给大家吧，其实看图就已经很明显了。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><hr><p>本文总结了几种初级的网络工具，一般的网络性能分析，通过组合以上几种工具，基本都能应付，但对于复杂的问题，以上工具可能就无能为力了。更多高阶的工具将在下文送上，敬请期待。</p><p>Reference：</p><ol><li>ip 和 ipconfig：<br><a href="https://blog.csdn.net/freeking101/article/details/68939059" target="_blank" rel="noopener">https://blog.csdn.net/freeking101/article/details/68939059</a></li><li>性能之巅：Linux网络性能分析工具<br><a href="http://www.infoq.com/cn/articles/linux-networking-performance-analytics" target="_blank" rel="noopener">http://www.infoq.com/cn/articles/linux-networking-performance-analytics</a></li><li>抓包工具tcpdump用法说明<br><a href="https://www.cnblogs.com/f-ck-need-u/p/7064286.html" target="_blank" rel="noopener">https://www.cnblogs.com/f-ck-need-u/p/7064286.html</a></li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是 Linux 性能分析系列的第四篇。&lt;/p&gt;
&lt;p&gt;比较宽泛地讲，网络方向的性能分析既包括主机测的网络配置查看、监
      
    
    </summary>
    
      <category term="Linux" scheme="https://chambai.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="性能分析" scheme="https://chambai.github.io/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    
      <category term="网络" scheme="https://chambai.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>一文掌握 Linux 性能分析之 IO 篇</title>
    <link href="https://chambai.github.io/2018/05/24/%E4%BA%91%E8%AE%A1%E7%AE%97/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8BIO%E7%AF%87/"/>
    <id>https://chambai.github.io/2018/05/24/云计算/一文掌握Linux性能分析之IO篇/</id>
    <published>2018-05-24T05:16:14.000Z</published>
    <updated>2018-10-06T02:52:24.100Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>这是 Linux 性能分析系列的第三篇。</p><p>IO 和 存储密切相关，存储可以概括为磁盘，内存，缓存，三者读写的性能差距非常大，磁盘读写是毫秒级的（一般 0.1-10ms），内存读写是微妙级的（一般 0.1-10us），cache 是纳秒级的（一般 1-10ns）。但这也是牺牲其他特性为代价的，速度快的，价格越贵，容量也越小。</p><p>IO 性能这块，我们更多关注的是读写磁盘的性能。首先，先了解下磁盘的基本信息。</p><h3 id="磁盘基本信息"><a href="#磁盘基本信息" class="headerlink" title="磁盘基本信息"></a>磁盘基本信息</h3><hr><h4 id="fdisk"><a href="#fdisk" class="headerlink" title="fdisk"></a><strong>fdisk</strong></h4><p>查看磁盘信息，包括磁盘容量，扇区大小，IO 大小等信息，常用 <code>fdisk -l</code>查看：</p><center><img src="/images/linux/fdisk.jpg" alt=""></center><p>可以看到 /dev/ 下有一个 40G 的硬盘，一共 8K 多万个扇区，每个扇区 512字节，IO 大小也是 512 字节。</p><h4 id="df"><a href="#df" class="headerlink" title="df"></a><strong>df</strong></h4><p>查看磁盘使用情况，通常看磁盘使用率：</p><center><img src="/images/linux/df.jpg" alt=""></center><h3 id="磁盘性能分析"><a href="#磁盘性能分析" class="headerlink" title="磁盘性能分析"></a>磁盘性能分析</h3><hr><p>主要分析磁盘的读写效率（IOPS：每秒读写的次数；吞吐量：每秒读写的数据量），IO 繁忙程度，及 IO 访问对 CPU 的消耗等性能指标。</p><h4 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a><strong>vmstat</strong></h4><p>第一个较为常用的还是这个万能的 vmstat：</p><center><img src="/images/linux/vmstatio.jpg" alt=""></center><p>对于 IO，我们常关注三个部分：</p><ul><li>b 值：表示因为 IO 阻塞排队的任务数</li><li>bi 和 bo 值：表示每秒读写磁盘的块数，bi（block in）是写磁盘，bo（block out）是读磁盘。</li><li>wa 值：表示因为 IO 等待（wait）而消耗的 CPU 比例。</li></ul><p>一般这几个值偏大，都意味着系统 IO 的消耗较大，对于读请求较大的服务器，b、bo、wa 的值偏大，而写请求较大的服务器，b、bi、wa 的值偏大。</p><h4 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a><strong>iostat</strong></h4><p>vmstat 虽然万能，但是它分析的东西有限，iostat 是专业分析 IO 性能的工具，可以方便查看 CPU、网卡、tty 设备、磁盘、CD-ROM 等等设备的信息，非常强大，总结下来，共有以下几种用法：</p><p><strong>1）iostat -c 查看部分 CPU 使用情况：</strong></p><center><img src="/images/linux/iostat.png" alt=""></center><p>这里显示的是多个 CPU 的平均值，每个字段的含义我就不多解释了，我一般会重点关注 %iowait 和 %idle，分别表示 CPU 等待 IO 完成时间的百分比和 CPU 空闲时间百分比。</p><p>如果 %iowait 较高，则表明磁盘存在 IO 瓶颈，如果 %idle 较高，则 CPU 比较空闲，如果两个值都比较高，则有可能 CPU 在等待分配内存，瓶颈在内存，此时应该加大内存，如果 %idle 较低，则此时瓶颈在 CPU，应该增加 CPU 资源。</p><p><strong>2）iostat -d 查看磁盘使用情况，主要是显示 IOPS 和吞吐量信息</strong>（-k : 以 KB 为单位显示，-m：以 M 为单位显示）：</p><center><img src="/images/linux/iostatd.png" alt=""></center><p>其中，几个参数分别解释如下：</p><ul><li>tps：设备每秒的传输次数（transfers per second），也就是读写次数。</li><li>kB_read/s 和 kB_wrtn/s：每秒读写磁盘的数据量。</li><li>kB_read 和 kB_wrtn：读取磁盘的数据总量。</li></ul><p><strong>3）iostat -x 查看磁盘详细信息：</strong></p><center><img src="/images/linux/iostatx.jpg" alt=""></center><p>其中，几个参数解释如下；</p><ul><li>rrqm/s 和 wrqm/s：分别每秒进行合并的读操作数和写操作数，这是什么意思呢，合并就是说把多次 IO 请求合并成少量的几次，这样可以减小 IO 开销，buffer 存在的意义就是为了解决这个问题的。</li><li>r/s 和 w/s：每秒磁盘读写的次数。这两个值相加就是 tps。</li><li>rkB/s 和 wkB/s：每秒磁盘读写的数据量，这两个值和上面的 kB_read/s、kB_wrnt/s 是一样的。</li><li>avgrq-sz：平均每次读写磁盘扇区的大小。</li><li>avgqu-sze：平均 IO 队列长度。队列长度越短越好。</li><li>await：平均每次磁盘读写的等待时间（ms）。</li><li>svctm：平均每次磁盘读写的服务时间（ms）。</li><li>%util：一秒钟有百分之多少的时间用于磁盘读写操作。</li></ul><p>以上这些参数太多了，我们并不需要每个都关注，可以重点关注两个：</p><p><strong>a. %util：衡量 IO 的繁忙程度</strong></p><p>这个值越大，说明产生的 IO 请求较多，IO 压力较大，我们可以结合 %idle 参数来看，如果 %idle &lt; 70% 就说明 IO 比较繁忙了。也可以结合 vmstat 的 b 参数（等待 IO 的进程数）和 wa 参数（IO 等待所占 CPU 时间百分比）来看，如果 wa &gt; 30% 也说明 IO 较为繁忙。</p><p><strong>b. await：衡量 IO 的响应速度</strong></p><p>通俗理解，await 就像我们去医院看病排队等待的时间，这个值和医生的服务速度（svctm）和你前面排队的人数（avgqu-size）有关。如果 svctm 和 await 接近，说明磁盘 IO 响应时间较快，排队较少，如果 await 远大于 svctm，说明此时队列太长，响应较慢，这时可以考虑换性能更好的磁盘或升级 CPU。</p><p><strong>4）iostat 1 2 默认显示 cpu 和 吞吐量信息，1 定时 1s 显示，2 显示 2 条信息</strong></p><center><img src="/images/linux/iostat12.jpg" alt=""></center><h3 id="进程-IO-性能分析"><a href="#进程-IO-性能分析" class="headerlink" title="进程 IO 性能分析"></a>进程 IO 性能分析</h3><hr><p>有了以上两个命令，基本上能对磁盘 IO 的信息有个全方位的了解了。但如果要确定具体哪个进程的 IO 开销较大，这就得借助另外的工具了。</p><h4 id="iotop"><a href="#iotop" class="headerlink" title="iotop"></a><strong>iotop</strong></h4><p>这个命令类似 top，可以显示每个进程的 IO 情况，有了这个命令，就可以定位具体哪个进程的 IO 开销比较大了。</p><center><img src="/images/linux/iostop.jpg" alt=""></center><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>OK，最后还是总结下，fdisk -l 和 df 查看磁盘基本信息，iostat -d 查看磁盘 IOPS 和吞吐量，iostat -x 结合 vmstat 查看磁盘的繁忙程度和处理效率。</p><p>下文我们将探讨网络方面的的性能分析问题。</p><p>Reference：<br>1.linux 性能分析：<br><a href="http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html" target="_blank" rel="noopener">http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html</a></p><ol start="2"><li>linux 性能分析工具总结：<br><a href="http://rdc.hundsun.com/portal/article/731.html" target="_blank" rel="noopener">http://rdc.hundsun.com/portal/article/731.html</a></li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是 Linux 性能分析系列的第三篇。&lt;/p&gt;
&lt;p&gt;IO 和 存储密切相关，存储可以概括为磁盘，内存，缓存，三者读
      
    
    </summary>
    
      <category term="Linux" scheme="https://chambai.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="性能分析" scheme="https://chambai.github.io/tags/%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    
      <category term="I/O" scheme="https://chambai.github.io/tags/I-O/"/>
    
  </entry>
  
</feed>
