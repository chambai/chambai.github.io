<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>长亭的网志空间</title>
  
  <subtitle>Linux|云计算|网络|编程|读书|思维|认知</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chambai.github.io/"/>
  <updated>2019-04-17T12:49:36.610Z</updated>
  <id>https://chambai.github.io/</id>
  
  <author>
    <name>bike</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>一文总结虚拟网络设备 eth, tap/tun, veth-pair</title>
    <link href="https://chambai.github.io/2019/03/08/tech/eth-taptun-vethpair%E6%80%BB%E7%BB%93/"/>
    <id>https://chambai.github.io/2019/03/08/tech/eth-taptun-vethpair总结/</id>
    <published>2019-03-08T05:16:14.000Z</published>
    <updated>2019-04-17T12:49:36.610Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><blockquote><p>本文翻译自：<a href="http://t.cn/EIdjMTc" target="_blank" rel="noopener">http://t.cn/EIdjMTc</a></p></blockquote><p>Linux 虚拟网络的背后都是由一个个的虚拟设备构成的。虚拟化技术没出现之前，计算机网络系统都只包含物理的网卡设备，通过网卡适配器，线缆介质，连接外部网络，构成庞大的 Internet。</p><p><img src="/images/net/virtual-device-physical-2.png" alt="virtual-device-physical-2.png"></p><a id="more"></a><p>然而，随着虚拟化技术的出现，网络也随之被虚拟化，相较于单一的物理网络，虚拟网络变得非常复杂，在一个主机系统里面，需要实现诸如交换、路由、隧道、隔离、聚合等多种网络功能。</p><p>而实现这些功能的基本元素就是虚拟的网络设备，比如 tap、tun 和 veth-pair。</p><h2 id="tap-tun"><a href="#tap-tun" class="headerlink" title="tap/tun"></a>tap/tun</h2><p>tap/tun 提供了一台主机内用户空间的数据传输机制。它虚拟了一套网络接口，这套接口和物理的接口无任何区别，可以配置 IP，可以路由流量，不同的是，它的流量只在主机内流通。</p><p>tap/tun 有些许的不同，tun 只操作三层的 IP 包，而 tap 操作二层的以太网帧。</p><p><img src="/images/net/virtual-device-tuntap-4.png" alt="virtual-device-tuntap-4.png"></p><h2 id="veth-pair"><a href="#veth-pair" class="headerlink" title="veth-pair"></a>veth-pair</h2><p>veth-pair 是成对出现的一种虚拟网络设备，一端连接着协议栈，一端连接着彼此，数据从一端出，从另一端进。</p><p>它的这个特性常常用来连接不同的虚拟网络组件，构建大规模的虚拟网络拓扑，比如连接 Linux Bridge、OVS、LXC 容器等。</p><p>一个很常见的案例就是它被用于 OpenStack Neutron，构建非常复杂的网络形态。</p><p><img src="/images/net/virtual-device-veth-1.png" alt="virtual-device-veth-1.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后，总结一下，我们提到几种网络设备，eth0、tap、tun、veth-pair，这些都构成了如今云网络必不可少的元素。</p><p><img src="/images/net/virtual-devices-all-4.png" alt="virtual-devices-all-4.png"></p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」(id: cloud_dev)&lt;/strong&gt; ，专注于干货分享，号内有 &lt;strong&gt;10T&lt;/strong&gt; 书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可免费领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;本文翻译自：&lt;a href=&quot;http://t.cn/EIdjMTc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://t.cn/EIdjMTc&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Linux 虚拟网络的背后都是由一个个的虚拟设备构成的。虚拟化技术没出现之前，计算机网络系统都只包含物理的网卡设备，通过网卡适配器，线缆介质，连接外部网络，构成庞大的 Internet。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/net/virtual-device-physical-2.png&quot; alt=&quot;virtual-device-physical-2.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="06 网络" scheme="https://chambai.github.io/categories/06-%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="tap" scheme="https://chambai.github.io/tags/tap/"/>
    
      <category term="tun" scheme="https://chambai.github.io/tags/tun/"/>
    
      <category term="veth-pair" scheme="https://chambai.github.io/tags/veth-pair/"/>
    
  </entry>
  
  <entry>
    <title>Linux云网络基础之虚拟网络设备 veth-pair 详解</title>
    <link href="https://chambai.github.io/2019/03/03/tech/veth-pair%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chambai.github.io/2019/03/03/tech/veth-pair详解/</id>
    <published>2019-03-03T05:16:14.000Z</published>
    <updated>2019-04-17T12:51:28.442Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>前面这篇文章介绍了 tap/tun 设备之后，大家应该对虚拟网络设备有了一定的了解，本文来看另外一种虚拟网络设备 veth-pair。</p><h2 id="01-veth-pair-是什么"><a href="#01-veth-pair-是什么" class="headerlink" title="01 veth-pair 是什么"></a>01 veth-pair 是什么</h2><p>顾名思义，veth-pair 就是一对的虚拟设备接口，和 tap/tun 设备不同的是，它都是成对出现的。一端连着协议栈，一端彼此相连着。如下图所示：</p><p><img src="/images/net/veth.jpeg" alt="veth"></p><a id="more"></a><p>正因为有这个特性，它常常充当着一个桥梁，连接着各种虚拟网络设备，典型的例子像“两个 namespace 之间的连接”，“Bridge、OVS 之间的连接”，“Docker 容器之间的连接” 等等，以此构建出非常复杂的虚拟网络结构，比如 OpenStack Neutron。</p><h2 id="02-veth-pair-的连通性"><a href="#02-veth-pair-的连通性" class="headerlink" title="02 veth-pair 的连通性"></a>02 veth-pair 的连通性</h2><p>我们给上图中的 veth0 和 veth1 分别配上 IP：10.1.1.2 和 10.1.1.3，然后从 veth0 ping 一下 veth1。理论上它们处于同网段，是能 ping 通的，但结果却是 ping 不通。</p><p>抓个包看看，<code>tcpdump -nnt -i veth0</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# tcpdump -nnt -i veth0</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on veth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">ARP, Request who-has 10.1.1.3 tell 10.1.1.2, length 28</span><br><span class="line">ARP, Request who-has 10.1.1.3 tell 10.1.1.2, length 28</span><br></pre></td></tr></table></figure><p>可以看到，由于 veth0 和 veth1 处于同一个网段，且是第一次连接，所以会事先发 ARP 包，但 veth1 并没有响应 ARP 包。</p><p>经查阅，这是由于我使用的 Ubuntu 系统内核中一些 ARP 相关的默认配置限制所导致的，需要修改一下配置项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/conf/veth1/accept_local</span><br><span class="line">echo 1 &gt; /proc/sys/net/ipv4/conf/veth0/accept_local</span><br><span class="line">echo 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter</span><br><span class="line">echo 0 &gt; /proc/sys/net/ipv4/conf/veth0/rp_filter</span><br><span class="line">echo 0 &gt; /proc/sys/net/ipv4/conf/veth1/rp_filter</span><br></pre></td></tr></table></figure><p>完了再 ping 就行了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# ping -I veth0 10.1.1.3 -c 2</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) from 10.1.1.2 veth0: 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.047 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.064 ms</span><br><span class="line"></span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 3008ms</span><br><span class="line">rtt min/avg/max/mdev = 0.047/0.072/0.113/0.025 ms</span><br></pre></td></tr></table></figure><p>我们对这个通信过程比较感兴趣，可以抓包看看。</p><p>对于 veth0 口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# tcpdump -nnt -i veth0</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on veth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">ARP, Request who-has 10.1.1.3 tell 10.1.1.2, length 28</span><br><span class="line">ARP, Reply 10.1.1.3 is-at 5a:07:76:8e:fb:cd, length 28</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 1, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 2, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 3, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2244, seq 1, length 64</span><br></pre></td></tr></table></figure><p>对于 veth1 口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# tcpdump -nnt -i veth1</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on veth1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">ARP, Request who-has 10.1.1.3 tell 10.1.1.2, length 28</span><br><span class="line">ARP, Reply 10.1.1.3 is-at 5a:07:76:8e:fb:cd, length 28</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 1, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 2, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 3, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2244, seq 1, length 64</span><br></pre></td></tr></table></figure><p>奇怪，我们并没有看到 ICMP 的 <code>echo reply</code> 包，那它是怎么 ping 通的？</p><p>其实这里 <code>echo reply</code> 走的是 localback 口，不信抓个包看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# tcpdump -nnt -i lo</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on lo, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 2244, seq 1, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 2244, seq 2, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 2244, seq 3, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 2244, seq 4, length 64</span><br></pre></td></tr></table></figure><p>为什么？</p><p>我们看下整个通信流程就明白了。</p><ol><li>首先 ping 程序构造 ICMP <code>echo request</code>，通过 socket 发给协议栈。</li><li>由于 ping 指定了走 veth0 口，如果是第一次，则需要发 ARP 请求，否则协议栈直接将数据包交给 veth0。</li><li>由于 veth0 连着 veth1，所以 ICMP request 直接发给 veth1。</li><li>veth1 收到请求后，交给另一端的协议栈。</li><li>协议栈看本地有 10.1.1.3 这个 IP，于是构造 ICMP reply 包，查看路由表，发现回给 10.1.1.0 网段的数据包应该走 localback 口，于是将 reply 包交给 lo 口（会优先查看路由表的 0 号表，<code>ip route show table 0</code> 查看）。</li><li>lo 收到协议栈的 reply 包后，啥都没干，转手又回给协议栈。</li><li>协议栈收到 reply 包之后，发现有 socket 在等待包，于是将包给 socket。</li><li>等待在用户态的 ping 程序发现 socket 返回，于是就收到 ICMP 的 reply 包。</li></ol><p>整个过程如下图所示：</p><p><img src="/images/virt/pingveth.jpeg" alt="pingveth"></p><h2 id="03-两个-namespace-之间的连通性"><a href="#03-两个-namespace-之间的连通性" class="headerlink" title="03 两个 namespace 之间的连通性"></a>03 两个 namespace 之间的连通性</h2><p>namespace 是 Linux 2.6.x 内核版本之后支持的特性，主要用于资源的隔离。有了 namespace，一个 Linux 系统就可以抽象出多个网络子系统，各子系统间都有自己的网络设备，协议栈等，彼此之间互不影响。</p><p>如果各个 namespace 之间需要通信，怎么办呢，答案就是用 veth-pair 来做桥梁。</p><p>根据连接的方式和规模，可以分为“直接相连”，“通过 Bridge 相连” 和 “通过 OVS 相连”。</p><h3 id="3-1-直接相连"><a href="#3-1-直接相连" class="headerlink" title="3.1 直接相连"></a>3.1 直接相连</h3><p>直接相连是最简单的方式，如下图，一对 veth-pair 直接将两个 namespace 连接在一起。</p><p><img src="/images/virt/linuxswitch-veth.png" alt="linuxswitch-veth"></p><p>给 veth-pair 配置 IP，测试连通性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 创建 namespace</span><br><span class="line">ip netns a ns1</span><br><span class="line">ip netns a ns2</span><br><span class="line"></span><br><span class="line"># 创建一对 veth-pair veth0 veth1</span><br><span class="line">ip l a veth0 type veth peer name veth1</span><br><span class="line"></span><br><span class="line"># 将 veth0 veth1 分别加入两个 ns</span><br><span class="line">ip l s veth0 netns ns1</span><br><span class="line">ip l s veth1 netns ns2</span><br><span class="line"></span><br><span class="line"># 给两个 veth0 veth1 配上 IP 并启用</span><br><span class="line">ip netns exec ns1 ip a a 10.1.1.2/24 dev veth0</span><br><span class="line">ip netns exec ns1 ip l s veth0 up</span><br><span class="line">ip netns exec ns2 ip a a 10.1.1.3/24 dev veth1</span><br><span class="line">ip netns exec ns2 ip l s veth1 up</span><br><span class="line"></span><br><span class="line"># 从 veth0 ping veth1</span><br><span class="line">[root@localhost ~]# ip netns exec ns1 ping 10.1.1.3</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.073 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.068 ms</span><br><span class="line"></span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">15 packets transmitted, 15 received, 0% packet loss, time 14000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.068/0.084/0.201/0.032 ms</span><br></pre></td></tr></table></figure><h3 id="3-2-通过-Bridge-相连"><a href="#3-2-通过-Bridge-相连" class="headerlink" title="3.2 通过 Bridge 相连"></a>3.2 通过 Bridge 相连</h3><p>Linux Bridge 相当于一台交换机，可以中转两个 namespace 的流量，我们看看 veth-pair 在其中扮演什么角色。</p><p>如下图，两对 veth-pair 分别将两个 namespace 连到 Bridge 上。</p><p><img src="/images/virt/linuxswitch-ovs-veth.png" alt="linuxswitch-ovs-veth"></p><p>同样给 veth-pair 配置 IP，测试其连通性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># 首先创建 bridge br0</span><br><span class="line">ip l a br0 type bridge</span><br><span class="line">ip l s br0 up </span><br><span class="line"></span><br><span class="line"># 然后创建两对 veth-pair</span><br><span class="line">ip l a veth0 type veth peer name br-veth0</span><br><span class="line">ip l a veth1 type veth peer name br-veth1</span><br><span class="line"></span><br><span class="line"># 分别将两对 veth-pair 加入两个 ns 和 br0</span><br><span class="line">ip l s veth0 netns ns1</span><br><span class="line">ip l s br-veth0 master br0</span><br><span class="line">ip l s br-veth0 up</span><br><span class="line"></span><br><span class="line">ip l s veth1 netns ns2</span><br><span class="line">ip l s br-veth1 master br0</span><br><span class="line">ip l s br-veth1 up</span><br><span class="line"></span><br><span class="line"># 给两个 ns 中的 veth 配置 IP 并启用</span><br><span class="line">ip netns exec ns1 ip a a 10.1.1.2/24 dev veth0</span><br><span class="line">ip netns exec ns1 ip l s veth0 up</span><br><span class="line"></span><br><span class="line">ip netns exec ns2 ip a a 10.1.1.3/24 dev veth1</span><br><span class="line">ip netns exec ns2 ip l s veth1 up</span><br><span class="line"></span><br><span class="line"># veth0 ping veth1</span><br><span class="line">[root@localhost ~]# ip netns exec ns1 ping 10.1.1.3</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.060 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.105 ms</span><br><span class="line"></span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.060/0.082/0.105/0.024 ms</span><br></pre></td></tr></table></figure><h3 id="3-3-通过-OVS-相连"><a href="#3-3-通过-OVS-相连" class="headerlink" title="3.3 通过 OVS 相连"></a>3.3 通过 OVS 相连</h3><p>OVS 是第三方开源的 Bridge，功能比 Linux Bridge 要更强大，对于同样的实验，我们用 OVS 来看看是什么效果。</p><p>如下图所示：</p><p><img src="/images/virt/linuxswitch-ovs.png" alt="linuxswitch-ovs"></p><p>同样测试两个 namespace 之间的连通性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 用 ovs 提供的命令创建一个 ovs bridge</span><br><span class="line">ovs-vsctl add-br ovs-br</span><br><span class="line"></span><br><span class="line"># 创建两对 veth-pair</span><br><span class="line">ip l a veth0 type veth peer name ovs-veth0</span><br><span class="line">ip l a veth1 type veth peer name ovs-veth1</span><br><span class="line"></span><br><span class="line"># 将 veth-pair 两端分别加入到 ns 和 ovs bridge 中</span><br><span class="line">ip l s veth0 netns ns1</span><br><span class="line">ovs-vsctl add-port ovs-br ovs-veth0</span><br><span class="line">ip l s ovs-veth0 up</span><br><span class="line"></span><br><span class="line">ip l s veth1 netns ns2</span><br><span class="line">ovs-vsctl add-port ovs-br ovs-veth1</span><br><span class="line">ip l s ovs-veth1 up</span><br><span class="line"></span><br><span class="line"># 给 ns 中的 veth 配置 IP 并启用</span><br><span class="line">ip netns exec ns1 ip a a 10.1.1.2/24 dev veth0</span><br><span class="line">ip netns exec ns1 ip l s veth0 up</span><br><span class="line"></span><br><span class="line">ip netns exec ns2 ip a a 10.1.1.3/24 dev veth1</span><br><span class="line">ip netns exec ns2 ip l s veth1 up</span><br><span class="line"></span><br><span class="line"># veth0 ping veth1</span><br><span class="line">[root@localhost ~]# ip netns exec ns1 ping 10.1.1.3</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.311 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.087 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.087/0.199/0.311/0.112 ms</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>veth-pair 在虚拟网络中充当着桥梁的角色，连接多种网络设备构成复杂的网络。</p><p>veth-pair 的三个经典实验，直接相连、通过 Bridge 相连和通过 OVS 相连。</p><p><strong>参考：</strong></p><p><a href="http://www.opencloudblog.com/?p=66" target="_blank" rel="noopener">http://www.opencloudblog.com/?p=66</a></p><p><a href="https://segmentfault.com/a/1190000009251098" target="_blank" rel="noopener">https://segmentfault.com/a/1190000009251098</a></p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」(id: cloud_dev)&lt;/strong&gt; ，专注于干货分享，号内有 &lt;strong&gt;10T&lt;/strong&gt; 书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可免费领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;前面这篇文章介绍了 tap/tun 设备之后，大家应该对虚拟网络设备有了一定的了解，本文来看另外一种虚拟网络设备 veth-pair。&lt;/p&gt;
&lt;h2 id=&quot;01-veth-pair-是什么&quot;&gt;&lt;a href=&quot;#01-veth-pair-是什么&quot; class=&quot;headerlink&quot; title=&quot;01 veth-pair 是什么&quot;&gt;&lt;/a&gt;01 veth-pair 是什么&lt;/h2&gt;&lt;p&gt;顾名思义，veth-pair 就是一对的虚拟设备接口，和 tap/tun 设备不同的是，它都是成对出现的。一端连着协议栈，一端彼此相连着。如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/net/veth.jpeg&quot; alt=&quot;veth&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="06 网络" scheme="https://chambai.github.io/categories/06-%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="网络" scheme="https://chambai.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="veth-pair" scheme="https://chambai.github.io/tags/veth-pair/"/>
    
  </entry>
  
  <entry>
    <title>如何用 tap/tun 设备编写一个 ICMP 程序</title>
    <link href="https://chambai.github.io/2019/03/01/tech/%E5%9F%BA%E4%BA%8Etaptun%E5%86%99%E4%B8%80%E4%B8%AAICMP%E7%A8%8B%E5%BA%8F/"/>
    <id>https://chambai.github.io/2019/03/01/tech/基于taptun写一个ICMP程序/</id>
    <published>2019-03-01T05:16:14.000Z</published>
    <updated>2019-04-17T12:50:18.218Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>前面两篇文章已经介绍过 tap/tun 的<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247484961&amp;idx=1&amp;sn=f26d7994f57abbf5de2007a2f451d9f5&amp;chksm=ea743299dd03bb8f6ac063c1cb00d5a592094c7778ab0a5baf37b7469fa3eb018101ef34551f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">原理</a>和<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247484976&amp;idx=1&amp;sn=a3c5112bea36c8a543660c7ac1497b36&amp;chksm=ea743288dd03bb9e15e7971894c80151e504ad5912d660204d6a3ea140cb3e6f55af245d50f2&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">配置工具</a>。这篇文章通过一个编程示例来深入了解 tap/tun 的程序结构。</p><a id="more"></a><h2 id="01-准备工作"><a href="#01-准备工作" class="headerlink" title="01 准备工作"></a>01 准备工作</h2><p>首先通过 <code>modinfo tun</code> 查看系统内核是否支持 tap/tun 设备驱动。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]# modinfo tun</span><br><span class="line">filename:       /lib/modules/3.10.0-862.14.4.el7.x86_64/kernel/drivers/net/tun.ko.xz</span><br><span class="line">alias:          devname:net/tun</span><br><span class="line">alias:          char-major-10-200</span><br><span class="line">license:        GPL</span><br><span class="line">author:         (C) 1999-2004 Max Krasnyansky &lt;maxk@qualcomm.com&gt;</span><br><span class="line">description:    Universal TUN/TAP device driver</span><br><span class="line">retpoline:      Y</span><br><span class="line">rhelversion:    7.5</span><br><span class="line">srcversion:     50878D5D5A0138445B25AA8</span><br><span class="line">depends:</span><br><span class="line">intree:         Y</span><br><span class="line">vermagic:       3.10.0-862.14.4.el7.x86_64 SMP mod_unload modversions</span><br><span class="line">signer:         CentOS Linux kernel signing key</span><br><span class="line">sig_key:        E4:A1:B6:8F:46:8A:CA:5C:22:84:50:53:18:FD:9D:AD:72:4B:13:03</span><br><span class="line">sig_hashalgo:   sha256</span><br></pre></td></tr></table></figure><p>在 linux 2.4 及之后的内核版本中，tun/tap 驱动是默认编译进内核中的。</p><p>如果你的系统不支持，请先选择手动编译内核或者升级内核。编译时开启下面的选项即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Device Drivers =&gt; Network device support =&gt; Universal TUN/TAP device driver support</span><br></pre></td></tr></table></figure><p>tap/tun 也支持编译成模块，如果编译成模块，需要手动加载它：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# modprobe tun</span><br><span class="line">[root@localhost ~]# lsmod | grep tun</span><br><span class="line">tun                    31665  0</span><br></pre></td></tr></table></figure><p>关于以上的详细步骤，网上有很多教程，这里就不再赘述了。</p><p><a href="https://blog.csdn.net/lishuhuakai/article/details/70305543" target="_blank" rel="noopener">https://blog.csdn.net/lishuhuakai/article/details/70305543</a></p><p>上面只是加载了 tap/tun 模块，要完成 tap/tun 的编码，还需要有设备文件，运行命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mknod /dev/net/tun c 10 200 # c表示为字符设备，10和200分别是主设备号和次设备号</span><br></pre></td></tr></table></figure><p>这样在 <code>/dev/net</code> 下就创建了一个名为 tun 的文件。</p><h2 id="02-编程示例"><a href="#02-编程示例" class="headerlink" title="02 编程示例"></a>02 编程示例</h2><h3 id="2-1-启动设备"><a href="#2-1-启动设备" class="headerlink" title="2.1 启动设备"></a>2.1 启动设备</h3><p>使用 tap/tun 设备，需要先进行一些初始化工作，如下代码所示：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tun_alloc</span><span class="params">(<span class="keyword">char</span> *dev, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    assert(dev != <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ifreq</span> <span class="title">ifr</span>;</span></span><br><span class="line">    <span class="keyword">int</span> fd, err;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">char</span> *clonedev = <span class="string">"/dev/net/tun"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((fd = open(clonedev, O_RDWR)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> fd;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(&amp;ifr, <span class="number">0</span>, <span class="keyword">sizeof</span>(ifr));</span><br><span class="line">    ifr.ifr_flags = flags;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (*dev != <span class="string">'\0'</span>) &#123;</span><br><span class="line">        <span class="built_in">strncpy</span>(ifr.ifr_name, dev, IFNAMSIZ);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> ((err = ioctl(fd, TUNSETIFF, (<span class="keyword">void</span> *) &amp;ifr)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span> err;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一旦设备开启成功，系统会给设备分配一个名称，对于tun设备，一般为tunX，X为从0开始的编号；</span></span><br><span class="line">    <span class="comment">// 对于tap设备，一般为tapX</span></span><br><span class="line">    <span class="built_in">strcpy</span>(dev, ifr.ifr_name);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fd;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先打开字符设备文件 <code>/dev/net/tun</code>，然后用 <code>ioctl</code> 注册设备的工作模式，是 tap 还是 tun。这个模式由结构体 <code>struct ifreq</code> 的属性 <code>ifr_flags</code> 来定义，它有以下表示：</p><ul><li>IFF_TUN: 表示创建一个 tun 设备。</li><li>IFF_TAP: 表示创建一个 tap 设备。</li><li>IFF_NO_PI: 表示不包含包头信息，默认的，每个数据包传到用户空间时，都会包含一个附加的包头来保存包信息，这个表示不加包头。</li><li>IFF_ONE_QUEUE：表示采用单一队列模式。</li></ul><p>还是有一个属性是 <code>ifr_name</code>，表示设备的名字，它可以由用户自己指定，也可以由系统自动分配，比如 <code>tapX</code>、<code>tunX</code>，X 从 0 开始编号。</p><p><code>ioctl</code> 完了之后，文件描述符 fd 就和设备建立起了关联，之后就可以根据 fd 进行 read 和 write 操作了。</p><h3 id="2-2-写一个-ICMP-的调用函数"><a href="#2-2-写一个-ICMP-的调用函数" class="headerlink" title="2.2 写一个 ICMP 的调用函数"></a>2.2 写一个 ICMP 的调用函数</h3><p>为了测试上面的程序，我们写一个简单的 ICMP echo 程序。我们会使用 tun 设备，然后给 <code>tunX</code> 接口发送一个 ping 包，程序简单响应这个包，完成 ICMP 的 request 和 reply 的功能。</p><p>如下代码所示：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tun_fd, nread;</span><br><span class="line">    <span class="keyword">char</span> buffer[<span class="number">4096</span>];</span><br><span class="line">    <span class="keyword">char</span> tun_name[IFNAMSIZ];</span><br><span class="line"></span><br><span class="line">    tun_name[<span class="number">0</span>] = <span class="string">'\0'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Flags: IFF_TUN   - TUN device (no Ethernet headers)</span></span><br><span class="line"><span class="comment">     *        IFF_TAP   - TAP device</span></span><br><span class="line"><span class="comment">     *        IFF_NO_PI - Do not provide packet information</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    tun_fd = tun_alloc(tun_name, IFF_TUN | IFF_NO_PI);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tun_fd &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        perror(<span class="string">"Allocating interface"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Open tun/tap device: %s for reading...\n"</span>, tun_name);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">char</span> ip[<span class="number">4</span>];</span><br><span class="line">        <span class="comment">// 收包</span></span><br><span class="line">        nread = read(tun_fd, buffer, <span class="keyword">sizeof</span>(buffer));</span><br><span class="line">        <span class="keyword">if</span> (nread &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            perror(<span class="string">"Reading from interface"</span>);</span><br><span class="line">            close(tun_fd);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Read %d bytes from tun/tap device\n"</span>, nread);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 简单对收到的包调换一下顺序</span></span><br><span class="line">        <span class="built_in">memcpy</span>(ip, &amp;buffer[<span class="number">12</span>], <span class="number">4</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;buffer[<span class="number">12</span>], &amp;buffer[<span class="number">16</span>], <span class="number">4</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;buffer[<span class="number">16</span>], ip, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        buffer[<span class="number">20</span>] = <span class="number">0</span>;</span><br><span class="line">        *((<span class="keyword">unsigned</span> <span class="keyword">short</span> *)&amp;buffer[<span class="number">22</span>]) += <span class="number">8</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 发包</span></span><br><span class="line">        nread = write(tun_fd, buffer, nread);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Write %d bytes to tun/tap device, that's %s\n"</span>, nread, buffer);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面测试一下。</p><h3 id="2-3-给-tap-tun-设备配置-IP-地址"><a href="#2-3-给-tap-tun-设备配置-IP-地址" class="headerlink" title="2.3 给 tap/tun 设备配置 IP 地址"></a>2.3 给 tap/tun 设备配置 IP 地址</h3><p>编译：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost coding]# gcc -o taptun taptun.c</span><br><span class="line">[root@localhost coding]# ./taptun</span><br><span class="line">Open tun/tap device: tun0 for reading...</span><br></pre></td></tr></table></figure><p>开另一个终端，查看生成了 <code>tun0</code> 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost coding]# ip a</span><br><span class="line">6: tun0: &lt;POINTOPOINT,MULTICAST,NOARP&gt; mtu 1500 qdisc noop state DOWN qlen 500</span><br><span class="line">    link/none</span><br></pre></td></tr></table></figure><p>给 <code>tun0</code> 接口配置 IP 并启用，比如 <code>10.1.1.2/24</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip a a 10.1.1.2/24 dev tun0</span><br><span class="line">[root@localhost ~]# ip l s tun0 up</span><br></pre></td></tr></table></figure><p>再开一个终端，用 <code>tcpdump</code> 抓 <code>tun0</code> 的包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# tcpdump -nnt -i tun0</span><br></pre></td></tr></table></figure><p>然后在第二个终端 <code>ping</code> 一下 <code>10.1.1.0/24</code> 网段的 IP，比如 <code>10.1.1.3</code>，看到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ping -c 4 10.1.1.3</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.133 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.188 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=3 ttl=64 time=0.092 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=4 ttl=64 time=0.110 ms</span><br><span class="line"></span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3290ms</span><br><span class="line">rtt min/avg/max/mdev = 0.092/0.130/0.188/0.038 ms</span><br></pre></td></tr></table></figure><p>由于 <code>tun0</code> 接口建好之后，会生成一条到本网段 <code>10.1.1.0/24</code> 的默认路由，根据默认路由，数据包会走 <code>tun0</code> 口，所以能 ping 通，可以用 <code>route -n</code> 查看。</p><p>再看 tcpdump 抓包终端，成功显示 ICMP 的 request 包和 reply 包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# tcpdump -nnt -i tun0</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on tun0, link-type RAW (Raw IP), capture size 262144 bytes</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 3250, seq 1, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 3250, seq 1, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 3250, seq 2, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 3250, seq 2, length 64</span><br></pre></td></tr></table></figure><p>再看程序 <code>taptun.c</code> 的输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost coding]# ./taptun</span><br><span class="line">Open tun/tap device: tun0 for reading...</span><br><span class="line">Read 48 bytes from tun/tap device</span><br><span class="line">Write 48 bytes to tun/tap device</span><br><span class="line">Read 48 bytes from tun/tap device</span><br><span class="line">Write 48 bytes to tun/tap device</span><br></pre></td></tr></table></figure><p>ok，以上便验证了程序的正确性。</p><h2 id="03-总结"><a href="#03-总结" class="headerlink" title="03 总结"></a>03 总结</h2><p>通过这个小例子，让我们知道了基于 tap/tun 编程的流程，对 tap/tun 又加深了一层理解。</p><p>使用 tap/tun 设备需要包含头文件 <code>#include &lt;linux/if_tun.h&gt;</code>，以下是完整代码。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/******************************************************************************</span></span><br><span class="line"><span class="comment"> *File Name: taptun.c</span></span><br><span class="line"><span class="comment"> *Author: 公众号: Linux云计算网络</span></span><br><span class="line"><span class="comment"> *Created Time: 2019年02月23日 星期六 21时28分24秒</span></span><br><span class="line"><span class="comment"> *****************************************************************************/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;net/if.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/ioctl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/if_tun.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tun_alloc</span><span class="params">(<span class="keyword">char</span> *dev, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    assert(dev != <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ifreq</span> <span class="title">ifr</span>;</span></span><br><span class="line">    <span class="keyword">int</span> fd, err;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">char</span> *clonedev = <span class="string">"/dev/net/tun"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((fd = open(clonedev, O_RDWR)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> fd;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(&amp;ifr, <span class="number">0</span>, <span class="keyword">sizeof</span>(ifr));</span><br><span class="line">    ifr.ifr_flags = flags;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (*dev != <span class="string">'\0'</span>) &#123;</span><br><span class="line">        <span class="built_in">strncpy</span>(ifr.ifr_name, dev, IFNAMSIZ);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> ((err = ioctl(fd, TUNSETIFF, (<span class="keyword">void</span> *) &amp;ifr)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span> err;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一旦设备开启成功，系统会给设备分配一个名称，对于tun设备，一般为tunX，X为从0开始的编号；</span></span><br><span class="line">    <span class="comment">// 对于tap设备，一般为tapX</span></span><br><span class="line">    <span class="built_in">strcpy</span>(dev, ifr.ifr_name);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fd;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tun_fd, nread;</span><br><span class="line">    <span class="keyword">char</span> buffer[<span class="number">4096</span>];</span><br><span class="line">    <span class="keyword">char</span> tun_name[IFNAMSIZ];</span><br><span class="line"></span><br><span class="line">    tun_name[<span class="number">0</span>] = <span class="string">'\0'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Flags: IFF_TUN   - TUN device (no Ethernet headers)</span></span><br><span class="line"><span class="comment">     *        IFF_TAP   - TAP device</span></span><br><span class="line"><span class="comment">     *        IFF_NO_PI - Do not provide packet information</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    tun_fd = tun_alloc(tun_name, IFF_TUN | IFF_NO_PI);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tun_fd &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        perror(<span class="string">"Allocating interface"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Open tun/tap device: %s for reading...\n"</span>, tun_name);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">char</span> ip[<span class="number">4</span>];</span><br><span class="line">        <span class="comment">// 收包</span></span><br><span class="line">        nread = read(tun_fd, buffer, <span class="keyword">sizeof</span>(buffer));</span><br><span class="line">        <span class="keyword">if</span> (nread &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            perror(<span class="string">"Reading from interface"</span>);</span><br><span class="line">            close(tun_fd);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Read %d bytes from tun/tap device\n"</span>, nread);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 简单对收到的包调换一下顺序</span></span><br><span class="line">        <span class="built_in">memcpy</span>(ip, &amp;buffer[<span class="number">12</span>], <span class="number">4</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;buffer[<span class="number">12</span>], &amp;buffer[<span class="number">16</span>], <span class="number">4</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;buffer[<span class="number">16</span>], ip, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        buffer[<span class="number">20</span>] = <span class="number">0</span>;</span><br><span class="line">        *((<span class="keyword">unsigned</span> <span class="keyword">short</span> *)&amp;buffer[<span class="number">22</span>]) += <span class="number">8</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 发包</span></span><br><span class="line">        nread = write(tun_fd, buffer, nread);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Write %d bytes to tun/tap device, that's %s\n"</span>, nread, buffer);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」(id: cloud_dev)&lt;/strong&gt; ，专注于干货分享，号内有 &lt;strong&gt;10T&lt;/strong&gt; 书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可免费领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;前面两篇文章已经介绍过 tap/tun 的&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;amp;mid=2247484961&amp;amp;idx=1&amp;amp;sn=f26d7994f57abbf5de2007a2f451d9f5&amp;amp;chksm=ea743299dd03bb8f6ac063c1cb00d5a592094c7778ab0a5baf37b7469fa3eb018101ef34551f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;原理&lt;/a&gt;和&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;amp;mid=2247484976&amp;amp;idx=1&amp;amp;sn=a3c5112bea36c8a543660c7ac1497b36&amp;amp;chksm=ea743288dd03bb9e15e7971894c80151e504ad5912d660204d6a3ea140cb3e6f55af245d50f2&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;配置工具&lt;/a&gt;。这篇文章通过一个编程示例来深入了解 tap/tun 的程序结构。&lt;/p&gt;
    
    </summary>
    
      <category term="06 网络" scheme="https://chambai.github.io/categories/06-%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="tap" scheme="https://chambai.github.io/tags/tap/"/>
    
      <category term="tun" scheme="https://chambai.github.io/tags/tun/"/>
    
      <category term="ICMP" scheme="https://chambai.github.io/tags/ICMP/"/>
    
  </entry>
  
  <entry>
    <title>Linux网络命令必知必会之创建 tap/tun 设备</title>
    <link href="https://chambai.github.io/2019/02/28/tech/%E5%88%9B%E5%BB%BAtaptun%E8%AE%BE%E5%A4%87/"/>
    <id>https://chambai.github.io/2019/02/28/tech/创建taptun设备/</id>
    <published>2019-02-28T05:16:14.000Z</published>
    <updated>2019-04-16T16:22:57.290Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>在<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247484961&amp;idx=1&amp;sn=f26d7994f57abbf5de2007a2f451d9f5&amp;chksm=ea743299dd03bb8f6ac063c1cb00d5a592094c7778ab0a5baf37b7469fa3eb018101ef34551f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">这篇文章</a>中，我们已经介绍了 tap/tun 的基本原理，本文将介绍如何使用工具 <code>tunctl</code>和 <code>ip tuntap</code> 来创建并使用 tap/tun 设备。</p><a id="more"></a><h2 id="tunctl"><a href="#tunctl" class="headerlink" title="tunctl"></a>tunctl</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>首先在 <code>centos</code> 的环境中安装 <code>tunctl</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vim /etc/yum.repos.d/nux-misc.repo</span><br><span class="line"></span><br><span class="line">[nux-misc]</span><br><span class="line">name=Nux Misc</span><br><span class="line">baseurl=http://li.nux.ro/download/nux/misc/el7/x86_64/</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.ro</span><br></pre></td></tr></table></figure><p><code>ubuntu</code> 是 <code>apt-get install uml-utilities</code>。</p><p><code>man tunctl</code> 查看 <code>tunctl</code> 手册，用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Synopsis</span><br><span class="line">tunctl [ OPTIONS ] [ -u owner ] [-g group] [ -t device-name ]</span><br></pre></td></tr></table></figure><ul><li>-u 参数指定用户名，表明这个接口只受该用户控制，这个接口发生的事不会影响到系统的接口。</li><li>-g 指定一组用户</li><li>-t 指定要创建的 tap/tun 设备名。</li></ul><p><code>[OPTIONS]</code> 部分：</p><ul><li>-b 简单打印创建的接口名字</li><li>-n 创建 tun 设备</li><li>-p 创建 tap 设备，默认创建该设备</li><li>-f tun-clone-device 指定 tun 设备对应的文件名，默认是 <code>/dev/net/tun</code>，有些系统是 <code>/dev/misc/net/tun</code>。</li><li>-d interfacename 删除指定接口</li></ul><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p><strong>常见用法：</strong></p><p>默认创建 tap 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tunctl</span><br></pre></td></tr></table></figure><p>以上等价于 <code>tunctl -p</code></p><p>为用户 <code>user</code> 创建一个 tap 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tunctl -u user</span><br></pre></td></tr></table></figure><p>创建 tun 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tunctl -n</span><br></pre></td></tr></table></figure><p>为接口配置 IP 并启用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ifconfig tap0 192.168.0.254 up</span><br></pre></td></tr></table></figure><p>为接口添加路由：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># route add -host 192.168.0.1 dev tap0</span><br></pre></td></tr></table></figure><p>删除接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tunctl -d tap0</span><br></pre></td></tr></table></figure><h2 id="ip-tuntap"><a href="#ip-tuntap" class="headerlink" title="ip tuntap"></a>ip tuntap</h2><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>命令行输入 <code>ip help</code> 查看 <code>ip</code> 命令是否支持 <code>tuntap</code> 工具，支持的话就会显示 <code>tuntap</code> 选项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip help</span><br><span class="line">Usage: ip [ OPTIONS ] OBJECT &#123; COMMAND | help &#125;</span><br><span class="line">       ip [ -force ] -batch filename</span><br><span class="line">where  OBJECT := &#123; link | addr | addrlabel | route | rule | neigh | ntable |</span><br><span class="line">                   tunnel | tuntap | maddr | mroute | mrule | monitor | xfrm |</span><br><span class="line">                   netns | l2tp | tcp_metrics | token &#125;</span><br></pre></td></tr></table></figure><p>不支持就请升级或下载最新的 <code>iproute2</code> 工具包，或者使用上面介绍的 <code>tunctl</code> 工具。</p><h3 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h3><p>输入 <code>ip tuntap help</code> 查看详细使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip tuntap help</span><br><span class="line">Usage: ip tuntap &#123; add | del &#125; [ dev PHYS_DEV ]</span><br><span class="line">          [ mode &#123; tun | tap &#125; ] [ user USER ] [ group GROUP ]</span><br><span class="line">          [ one_queue ] [ pi ] [ vnet_hdr ] [ multi_queue ]</span><br><span class="line"></span><br><span class="line">Where: USER  := &#123; STRING | NUMBER &#125;</span><br><span class="line">       GROUP := &#123; STRING | NUMBER &#125;</span><br></pre></td></tr></table></figure><p><strong>常见用法：</strong></p><p>创建 tap/tun 设备：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip tuntap add dev tap0 mod tap # 创建 tap </span><br><span class="line">ip tuntap add dev tun0 mod tun # 创建 tun</span><br></pre></td></tr></table></figure><p>删除 tap/tun 设备：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip tuntap del dev tap0 mod tap # 删除 tap </span><br><span class="line">ip tuntap del dev tun0 mod tun # 删除 tun</span><br></pre></td></tr></table></figure><p>PS: <code>user</code> 和 <code>group</code> 参数和 <code>tunctl</code> 的 -u、 -g 参数是一样的。</p><p>以上两个工具，我们更推荐使用 <code>ip tuntap</code>，一个是因为 <code>iproute2</code> 更全更新，已经逐步在替代老旧的一些工具，另一个是因为 <code>tunctl</code> 在某些 <code>Debian</code> 类的系统上支持不全。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>tunctl</code> 和 <code>ip tuntap</code> 的常见使用方式。</p><p>更推荐使用 <code>ip tuntap</code> 工具。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」(id: cloud_dev)&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;amp;mid=2247484961&amp;amp;idx=1&amp;amp;sn=f26d7994f57abbf5de2007a2f451d9f5&amp;amp;chksm=ea743299dd03bb8f6ac063c1cb00d5a592094c7778ab0a5baf37b7469fa3eb018101ef34551f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这篇文章&lt;/a&gt;中，我们已经介绍了 tap/tun 的基本原理，本文将介绍如何使用工具 &lt;code&gt;tunctl&lt;/code&gt;和 &lt;code&gt;ip tuntap&lt;/code&gt; 来创建并使用 tap/tun 设备。&lt;/p&gt;
    
    </summary>
    
      <category term="06 网络" scheme="https://chambai.github.io/categories/06-%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="网络" scheme="https://chambai.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="tap" scheme="https://chambai.github.io/tags/tap/"/>
    
      <category term="tun" scheme="https://chambai.github.io/tags/tun/"/>
    
  </entry>
  
  <entry>
    <title>Linux云网络基础之虚拟网络设备 tap/tun 详解</title>
    <link href="https://chambai.github.io/2019/02/26/tech/taptun%E8%AF%A6%E8%A7%A3/"/>
    <id>https://chambai.github.io/2019/02/26/tech/taptun详解/</id>
    <published>2019-02-26T05:16:14.000Z</published>
    <updated>2019-04-16T16:23:34.421Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>在云计算时代，虚拟机和容器已经成为标配。它们背后的网络管理都离不开一样东西，就是虚拟网络设备，或者叫虚拟网卡，tap/tun 就是在云计算时代非常重要的虚拟网络网卡。</p><h2 id="tap-tun-是什么"><a href="#tap-tun-是什么" class="headerlink" title="tap/tun 是什么"></a>tap/tun 是什么</h2><p>tap/tun 是 Linux 内核 2.4.x 版本之后实现的虚拟网络设备，不同于物理网卡靠硬件网路板卡实现，tap/tun 虚拟网卡完全由软件来实现，功能和硬件实现完全没有差别，它们都属于网络设备，都可以配置 IP，都归 Linux 网络设备管理模块统一管理。</p><a id="more"></a><p>作为网络设备，tap/tun 也需要配套相应的驱动程序才能工作。tap/tun 驱动程序包括两个部分，一个是字符设备驱动，一个是网卡驱动。这两部分驱动程序分工不太一样，字符驱动负责数据包在内核空间和用户空间的传送，网卡驱动负责数据包在 TCP/IP 网络协议栈上的传输和处理。</p><h2 id="用户空间与内核空间的数据传输"><a href="#用户空间与内核空间的数据传输" class="headerlink" title="用户空间与内核空间的数据传输"></a>用户空间与内核空间的数据传输</h2><p>在 Linux 中，用户空间和内核空间的数据传输有多种方式，字符设备就是其中的一种。tap/tun 通过驱动程序和一个与之关联的字符设备，来实现用户空间和内核空间的通信接口。</p><p>在 Linux 内核 2.6.x 之后的版本中，tap/tun 对应的字符设备文件分别为：</p><ul><li>tap：/dev/tap0</li><li>tun：/dev/net/tun</li></ul><p>设备文件即充当了用户空间和内核空间通信的接口。当应用程序打开设备文件时，驱动程序就会创建并注册相应的虚拟设备接口，一般以 <code>tunX</code> 或 <code>tapX</code> 命名。当应用程序关闭文件时，驱动也会自动删除 <code>tunX</code> 和 <code>tapX</code> 设备，还会删除已经建立起来的路由等信息。</p><p>tap/tun 设备文件就像一个管道，一端连接着用户空间，一端连接着内核空间。当用户程序向文件 <code>/dev/net/tun</code> 或 <code>/dev/tap0</code> 写数据时，内核就可以从对应的 <code>tunX</code> 或 <code>tapX</code> 接口读到数据，反之，内核可以通过相反的方式向用户程序发送数据。</p><p><img src="/images/virt/tapwriteread.png" alt="tapwriteread.png"></p><h2 id="tap-tun-和网络协议栈的数据传输"><a href="#tap-tun-和网络协议栈的数据传输" class="headerlink" title="tap/tun 和网络协议栈的数据传输"></a>tap/tun 和网络协议栈的数据传输</h2><p>tap/tun 通过实现相应的网卡驱动程序来和网络协议栈通信。一般的流程和物理网卡和协议栈的交互流程是一样的，不同的是物理网卡一端是连接物理网络，而 tap/tun 虚拟网卡一般连接到用户空间。</p><p>如下图的示意图，我们有两个应用程序 A、B，物理网卡 <code>eth0</code> 和虚拟网卡 <code>tun0</code> 分别配置 IP：<code>10.1.1.11</code> 和 <code>192.168.1.11</code>，程序 A 希望构造数据包发往 <code>192.168.1.0/24</code> 网段的主机 <code>192.168.1.1</code>。</p><p><img src="/images/virt/taptun.png" alt="taptun"></p><p>基于上图，我们看看数据包的流程：</p><ol><li>应用程序 A 构造数据包，目的 IP 是 <code>192.168.1.1</code>，通过 <code>socket A</code> 将这个数据包发给协议栈。</li><li>协议栈根据数据包的目的 IP 地址，匹配路由规则，发现要从 <code>tun0</code> 出去。</li><li><code>tun0</code> 发现自己的另一端被应用程序 B 打开了，于是将数据发给程序 B.</li><li>程序 B 收到数据后，做一些跟业务相关的操作，然后构造一个新的数据包，源 IP 是 <code>eth0</code> 的 IP，目的 IP 是 <code>10.1.1.0/24</code> 的网关 <code>10.1.1.1</code>，封装原来的数据的数据包，重新发给协议栈。</li><li>协议栈再根据本地路由，将这个数据包从 <code>eth0</code> 发出。</li></ol><p>后续步骤，当 <code>10.1.1.1</code> 收到数据包后，会进行解封装，读取里面的原始数据包，继而转发给本地的主机 <code>192.168.1.1</code>。当接收回包时，也遵循同样的流程。</p><p>在这个流程中，应用程序 B 的作用其实是利用 <code>tun0</code> 对数据包做了一层隧道封装。其实 <code>tun</code> 设备的最大用途就是用于隧道通信的。</p><h2 id="tap-tun-的区别"><a href="#tap-tun-的区别" class="headerlink" title="tap/tun 的区别"></a>tap/tun 的区别</h2><p>看到这里，你可能还不大明白 tap/tun 的区别。<br>tap 和 tun 虽然都是虚拟网络设备，但它们的工作层次还不太一样。</p><ul><li>tap 是一个二层设备（或者以太网设备），只能处理二层的以太网帧；</li><li>tun 是一个点对点的三层设备（或网络层设备），只能处理三层的 IP 数据包。</li></ul><h2 id="tap-tun-的应用"><a href="#tap-tun-的应用" class="headerlink" title="tap/tun 的应用"></a>tap/tun 的应用</h2><p>从上面的数据流程中可以看到，<code>tun</code> 设备充当了一层隧道，所以，tap/tun 最常见的应用也就是用于隧道通信，比如 VPN，包括 tunnel 和应用层的 IPsec 等，其中比较有名的两个开源项目是 <a href="http://openvpn.sourceforge.net" target="_blank" rel="noopener">openvpn</a> 和 <a href="http://vtun.sourceforge.net" target="_blank" rel="noopener">VTun</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>tun/tap 虚拟网卡，对应于物理网卡，如 eth0。</p><p>tun/tap 驱动包括字符设备驱动和网卡驱动。</p><p>tun/tap 常用于隧道通信。</p><p><strong>参考：</strong></p><p><a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/" target="_blank" rel="noopener">https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/</a></p><p><a href="https://segmentfault.com/a/1190000009249039" target="_blank" rel="noopener">https://segmentfault.com/a/1190000009249039</a></p><p><a href="https://mirrors.edge.kernel.org/pub/linux/kernel/people/marcelo/linux-2.4/Documentation/networking/tuntap.txt" target="_blank" rel="noopener">https://mirrors.edge.kernel.org/pub/linux/kernel/people/marcelo/linux-2.4/Documentation/networking/tuntap.txt</a></p><p><a href="https://zh.wikipedia.org/wiki/TUN%E4%B8%8ETAP" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/TUN%E4%B8%8ETAP</a></p><p><a href="http://blog.chinaunix.net/uid-317451-id-92474.html" target="_blank" rel="noopener">http://blog.chinaunix.net/uid-317451-id-92474.html</a></p><p><a href="https://blog.csdn.net/bytxl/article/details/26586109" target="_blank" rel="noopener">https://blog.csdn.net/bytxl/article/details/26586109</a></p><p><a href="https://blog.csdn.net/u013982161/article/details/51816162" target="_blank" rel="noopener">https://blog.csdn.net/u013982161/article/details/51816162</a></p><p><a href="https://www.cnblogs.com/yml435/p/5917628.html" target="_blank" rel="noopener">https://www.cnblogs.com/yml435/p/5917628.html</a></p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」(id: cloud_dev)&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在云计算时代，虚拟机和容器已经成为标配。它们背后的网络管理都离不开一样东西，就是虚拟网络设备，或者叫虚拟网卡，tap/tun 就是在云计算时代非常重要的虚拟网络网卡。&lt;/p&gt;
&lt;h2 id=&quot;tap-tun-是什么&quot;&gt;&lt;a href=&quot;#tap-tun-是什么&quot; class=&quot;headerlink&quot; title=&quot;tap/tun 是什么&quot;&gt;&lt;/a&gt;tap/tun 是什么&lt;/h2&gt;&lt;p&gt;tap/tun 是 Linux 内核 2.4.x 版本之后实现的虚拟网络设备，不同于物理网卡靠硬件网路板卡实现，tap/tun 虚拟网卡完全由软件来实现，功能和硬件实现完全没有差别，它们都属于网络设备，都可以配置 IP，都归 Linux 网络设备管理模块统一管理。&lt;/p&gt;
    
    </summary>
    
      <category term="06 网络" scheme="https://chambai.github.io/categories/06-%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="网络" scheme="https://chambai.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="tap" scheme="https://chambai.github.io/tags/tap/"/>
    
      <category term="tun" scheme="https://chambai.github.io/tags/tun/"/>
    
  </entry>
  
  <entry>
    <title>小趋势解构</title>
    <link href="https://chambai.github.io/2019/01/15/life/%E5%B0%8F%E8%B6%8B%E5%8A%BF%E8%A7%A3%E6%9E%84/"/>
    <id>https://chambai.github.io/2019/01/15/life/小趋势解构/</id>
    <published>2019-01-15T05:16:14.000Z</published>
    <updated>2019-04-16T16:00:18.682Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>这篇文章是年初听完罗振宇的跨年演讲写的，算是笔记吧，当时只是发在简书上，因为是非技术文，所以就没发在这里。今天看到有网友喜欢，想着文章应该多多少少有点用，干脆就搬过来给大家看看，如果掉粉多，我后面就多发点技术文吧。</p><p>今年罗振宇的跨年演讲还是有些干货的。在演讲开始，罗胖就开宗明义，说今晚的演讲只关注「大环境下小个体的命运」，这对于我们这种小人物来说也许是莫大的福音。</p><a id="more"></a><p>确实，整个过程中，都能感受到那种认知的碰撞升级。但说实话，就像一句话说的，听过太多的大道理，仍然过不好这一生，这里的问题我觉得是出在我们没有将这些新的认知转化成自己的思想体系。</p><p>趁着演讲刚过去的这点余温，我想尝试着解构一下「小趋势」，这也是全篇演讲最核心的一点。通过这次解构，真正将「小趋势」化为己用。</p><p>我打算采用「5W+1H 法」进行解构，也就是 What、Where、When、Who、Why 和 How 问题分析法。</p><h2 id="01-What"><a href="#01-What" class="headerlink" title="01 What"></a>01 What</h2><p>首先，我们得知道，「小趋势」是什么？</p><p>从字面上看，有两层含义，第一是“小”，第二这是个“趋势”。</p><p>“小”说明很难察觉，它不在我们熟悉的领域内，超出我们的认知范围。</p><p>“趋势”说明它的存在影响了周围的事物，它带动了周围事物往良性的方向发展。所以，罗胖才说「小趋势」是：</p><blockquote><p>影响趋势的趋势，带来改变的改变。</p></blockquote><p>知道了这一点，基本上没什么卵用。没有实际的例子，很难形成共鸣。</p><p>罗胖举了两个非常典型的例子：移动支付和猫砂。怎么理解这两个例子呢，紧扣小趋势的定义，其实不难理解。</p><p>移动支付对于互联网人来说，是比较显现的趋势，比如前几年滴滴打车和快的打车的疯狂烧钱，那是微信和支付宝在互相较量，抢占移动支付的入口，对于我们做这一行的人来说，能不觉得它是个大趋势吗？</p><p>我们虽然看清了，但是有很多人看不清啊，特别是传统行业的人。对于这些人来说，移动支付就是个小趋势。</p><p>移动支付还有人看得出它是个趋势，猫砂就完全没人能够察觉。我们来看看它是不是个趋势？</p><p>没有猫砂之前，猫和人类的关系很疏远，有了猫砂之后，解决了猫咪臭臭的异味，于是猫和人类成为了好朋友，紧接着围绕猫、猫粮、猫玩具等产业开始蓬勃发展，进而影响到了互联网行业，很多公司以猫作为原型设计品牌 logo，带来了巨大流量。在将来，借助一些大趋势，这又将会带来疯狂扩张，比如借助人工智能的机器猫。</p><p>说到这里，我相信你应该比较清楚小趋势的定义了吧。我再举几个例子，这几个例子即使是同行，你也许也很难看得清它是个趋势。</p><p>一个是视频，一直以来，视频都是刚需，这是现代人娱乐的神器，也是学习的利器。但就是一直不温不火，人们已经习以为常，像吃饭穿衣那么平常。近两年却突然大火，特别是以直播、短视频为首的视频形式。我想这是因为我们已经进入一个个性化+信息泛滥的时代，任何「短平快」的东西都会特别容易受到重视。由此看来，当下环境的变化是个大趋势，视频只是个小趋势。</p><p>另一个是微信公众号，刚推出时有多少人能看得清它是个趋势，今天回看，这已然成为了一个内容红海，带动了自由职业、内容创业等多种个性化产业的发展。未来随着人工智能发展，很多人都说最没有可能被淘汰的工种就是内容创作，所以说将来仍有很多发展空间，这仍然是一个小趋势。</p><h2 id="02-Where"><a href="#02-Where" class="headerlink" title="02 Where"></a>02 Where</h2><p>第二点想聊聊小趋势的适用场景，也就是它一般出现在什么地方。我觉得小趋势可以出现在任何地方。上到中央政府，下到街边小商小贩。中央政府基本上是趋势的仲裁者，任何想要发展的趋势都必须经过这一关。但反过来，中央政府也是趋势的受益者，很多来自地方的新鲜想法可以带动地方经济发展，但政策不允许，迫于舆情的压力，中央政府也会适当调整，适应当下的趋势。</p><p>小商小贩就更不用说了，看看现在街边一个要饭的人都懂得用二维码支付。很多农村的人纷纷用上网络，淘宝，微商，代购，干起的买卖丝毫不亚于城里。很多大学生利用所学纷纷回到农村进行科技养殖、科技种植、人工智能养猪，等等，这样的例子太多了。</p><p>所以，小趋势适用于任何场合，在一个场合是大趋势，但在另一个场合兴许就是小趋势。</p><h2 id="03-Who"><a href="#03-Who" class="headerlink" title="03 Who"></a>03 Who</h2><p>有适用场景，自然就有适用人群。小趋势并不适用于所有人，只适用于那些不甘平凡，想要进步的人。这很好理解，一个自甘堕落，或者是甘于平凡、得过且过的人，你跟他说这件事是将来的一个趋势，好好做，他肯定只会用看病人的眼神看你。</p><p>另外，已经在大趋势中游走的人，也没必要关注小趋势，你只要把当下的事做好，就比大多数人赢得了先机。当下科技圈最火的技术当属人工智能了吧，这个巨无霸将会指导未来几十年甚至上百年的发展。很多人很幸运已经踏入这个领域，也有很多人想踏入，但因为门槛太高，一直在边缘游走，绝大多数人则是一直在观望。后面两类人即使没有第一类人幸运，但也挺好的，起码意识到了趋势，只要用心耕耘好自己的一亩三分地，找准时机，蹭也能蹭到。别忘了小趋势蝴蝶效应的威力。</p><h2 id="04-When"><a href="#04-When" class="headerlink" title="04 When"></a>04 When</h2><p>这一点想聊聊小趋势出现的时间。像猫砂这种，我们基本上是没人能感知，因为对我们来说太平常不过了。那我们该怎么样才能把握小趋势出现的时机呢？别想了，这个世上我相信没几个人能把握准确。</p><p>但是它既然是个趋势，我们就有办法去把握，即便准确率不高，那也比什么都不做要强。</p><p>办法就是把握大趋势的时机。这个应该比较容易的，今天的传播媒介多得让人抓狂，只要一有什么热点新闻，巴不得让你第一时间知道。</p><p>举一个杨超越的例子，这是一个超级颠覆我认知的例子。起初我怎么也想不通，一个五音不全，肢体僵硬的人，却受到那么多人喜欢。直到转发锦鲤这个事件出来，我才明白，噢，原来大家都需要一种幸运的寄托，就像我们拜观世音菩萨一样的道理。</p><p>那这个事件说明什么呢，是不是能说明这是个让人焦虑的时代，快节奏的生活让人们都来不及处理所有不幸的遭遇，只能寄托于那些被公认能带来好运的事与人身上。</p><p>其实这也没什么不好，这是一个趋势带动一个趋势逐步形成的，快节奏就是一个趋势，看到趋势的人，就推出很多快节奏的产品，也有人反其道而行之，推出慢节奏的产品。</p><p>这个例子可能不够接地气，但我想说的是小趋势你也许看不到，但大趋势你肯定看得清，我们的目的不是要进入大趋势的阵地（当然你能进入更好），而是吸取新观点，引用到自己的阵地。有朝一日，大趋势成熟时，你也能分到一杯羹。</p><h2 id="05-Why"><a href="#05-Why" class="headerlink" title="05 Why"></a>05 Why</h2><p>这一点想聊聊为什么会有小趋势？罗胖在讲小趋势前，讲了一个大背景。简单说就是 2018 年科技圈风云变幻，扎堆上市的很多，遗憾出局的也不少，更有裁员、停招等等令人忧伤的事。</p><p>在这样的一个节骨眼上，很多人焦虑已经跟不上时代了，每天都担心面临被淘汰的风险。罗胖引出小趋势，我觉得就是告诉这些人大可不必焦虑，因为对于一个「做事」的人来说，</p><blockquote><p>永远都不会有末班车，只有下一班车。</p></blockquote><p>专注做好当下事，才能有精力去感知自己身边的小趋势，否则自乱阵脚，最后难以控制。</p><h2 id="06-How"><a href="#06-How" class="headerlink" title="06 How"></a>06 How</h2><p>最后一点聊聊作为一个普通小人物的我们，该如何去感知小趋势，让自己的人生之路走得更加顺畅一些。我觉得可以参考以下几点：</p><ol><li>关注大趋势，时常看看自己从事的领域能不能沾到大趋势的光。</li><li>以百岁人生为目标而过活，如果不这样，一点小小的人生变动都会让你觉得人生完了。</li><li>多元化发展，能够让我们感知到不熟悉领域的小趋势。</li><li>做好当下的事，剩下的事交给上天安排吧。</li></ol><p>以上，第一次以这种方式来解构一个知识点，完了发现，这次演讲没有白听，停留在耳边的大道理终于吸收进去了。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有大量书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」(id: cloud_dev)&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这篇文章是年初听完罗振宇的跨年演讲写的，算是笔记吧，当时只是发在简书上，因为是非技术文，所以就没发在这里。今天看到有网友喜欢，想着文章应该多多少少有点用，干脆就搬过来给大家看看，如果掉粉多，我后面就多发点技术文吧。&lt;/p&gt;
&lt;p&gt;今年罗振宇的跨年演讲还是有些干货的。在演讲开始，罗胖就开宗明义，说今晚的演讲只关注「大环境下小个体的命运」，这对于我们这种小人物来说也许是莫大的福音。&lt;/p&gt;
    
    </summary>
    
      <category term="杂谈" scheme="https://chambai.github.io/categories/%E6%9D%82%E8%B0%88/"/>
    
    
  </entry>
  
  <entry>
    <title>一文搞懂 network namespace</title>
    <link href="https://chambai.github.io/2019/01/10/tech/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82network_namespace/"/>
    <id>https://chambai.github.io/2019/01/10/tech/一文搞懂network_namespace/</id>
    <published>2019-01-10T05:16:14.000Z</published>
    <updated>2019-04-16T16:25:52.504Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>本文通过 IP 命令操作来简单介绍 network namespace 的基本概念和用法。</p><p>深入了解可以看看我之前写的两篇文章 <a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247483982&amp;idx=1&amp;sn=35e2aac1f4c164c8afa79aa91707c90d&amp;chksm=ea7436f6dd03bfe036ccc8293aaf25d0c042f21afac5277bf04a32af36643a2780c980e53d09&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Docker 基础技术之 Linux namespace 详解</a> 和 <a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247483993&amp;idx=1&amp;sn=906551e374c0d8d40db00cbde934e624&amp;chksm=ea7436e1dd03bff724d7ee03267115b2ecf54bd146fa3f826b69e918b5a2455d1562d085ab62&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Docker 基础技术之 Linux namespace 源码分析</a>。</p><p>和 network namespace 相关的操作的子命令是 <code>ip netns</code> 。</p><a id="more"></a><h2 id="1-ip-netns-add-xx-创建一个-namespace"><a href="#1-ip-netns-add-xx-创建一个-namespace" class="headerlink" title="1. ip netns add  xx 创建一个 namespace"></a>1. ip netns add  xx 创建一个 namespace</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ip netns add net1</span><br><span class="line"># ip netns ls</span><br><span class="line">net1</span><br></pre></td></tr></table></figure><h2 id="2-ip-netns-exec-xx-yy-在新-namespace-xx-中执行-yy-命令"><a href="#2-ip-netns-exec-xx-yy-在新-namespace-xx-中执行-yy-命令" class="headerlink" title="2. ip netns exec xx yy 在新 namespace xx 中执行 yy 命令"></a>2. ip netns exec xx yy 在新 namespace xx 中执行 yy 命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net1 ip addr </span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line"># ip netns exec net1 bash // 在 net1 中打开一个shell终端</span><br><span class="line"># ip addr // 在net1中的shell终端</span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line"># exit // 退出net1</span><br></pre></td></tr></table></figure><p>上面 bash 不好区分是当前是在哪个 shell，可以采用下面的方法解决：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net1 /bin/bash --rcfile &lt;(echo &quot;PS1=\&quot;namespace net1&gt; \&quot;&quot;)</span><br><span class="line">namespace net1&gt; ping www.baidu.com</span><br></pre></td></tr></table></figure><p>每个 namespace 在创建的时候会自动创建一个回环接口 <code>lo</code> ，默认不启用，可以通过 <code>ip link set lo up</code> 启用。</p><h2 id="3-network-namespace-之间的通信"><a href="#3-network-namespace-之间的通信" class="headerlink" title="3. network namespace 之间的通信"></a>3. network namespace 之间的通信</h2><p>新创建的 namespace 默认不能和主机网络，以及其他 namespace 通信。</p><p>可以使用 Linux 提供的 <code>veth pair</code> 来完成通信。下面显示两个 namespace 之间通信的网络拓扑：</p><p><img src="/images/virt/netns.png" alt=""></p><h3 id="3-1-ip-link-add-type-veth-创建-veth-pair"><a href="#3-1-ip-link-add-type-veth-创建-veth-pair" class="headerlink" title="3.1 ip link add type veth 创建 veth pair"></a>3.1 ip link add type veth 创建 veth pair</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># ip link add type veth</span><br><span class="line"># ip link</span><br><span class="line">3: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 1a:53:39:5a:26:12 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">4: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 46:df:46:1f:bf:d6 brd ff:ff:ff:ff:ff:ff</span><br></pre></td></tr></table></figure><p>使用命令 <code>ip link add xxx type veth peer name yyy</code> 指定 veth pair 的名字。</p><h3 id="3-2-ip-link-set-xx-netns-yy-将-veth-xx-加入到-namespace-yy-中"><a href="#3-2-ip-link-set-xx-netns-yy-将-veth-xx-加入到-namespace-yy-中" class="headerlink" title="3.2 ip link set xx netns yy 将 veth xx 加入到 namespace yy 中"></a>3.2 ip link set xx netns yy 将 veth xx 加入到 namespace yy 中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ip link set veth0 netns net0</span><br><span class="line"># ip link set veth1 netns net1</span><br><span class="line">#</span><br><span class="line"># ip netns exec net0 ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">10: veth0@if11: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ether 1a:53:39:5a:26:12 brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br></pre></td></tr></table></figure><h3 id="3-3-给-veth-pair-配上-ip-地址"><a href="#3-3-给-veth-pair-配上-ip-地址" class="headerlink" title="3.3 给 veth pair 配上 ip 地址"></a>3.3 给 veth pair 配上 ip 地址</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net0 ip link set veth0 up</span><br><span class="line"># ip netns exec net0 ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">10: veth0@if11: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000</span><br><span class="line">    link/ether 1a:53:39:5a:26:12 brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line"># ip netns exec net0 ip addr add 10.1.1.1/24 dev veth0</span><br><span class="line"># ip netns exec net0 ip route</span><br><span class="line">10.1.1.0/24 dev veth0  proto kernel  scope link  src 10.1.1.1 linkdown</span><br><span class="line">#</span><br><span class="line"># ip netns exec net1 ip link set veth1 up</span><br><span class="line"># ip netns exec net1 ip addr add 10.1.1.2/24 dev veth1</span><br></pre></td></tr></table></figure><p>可以看到，在配完 ip 之后，还自动生成了对应的路由表信息。</p><h3 id="3-4-ping-测试两个-namespace-的连通性"><a href="#3-4-ping-测试两个-namespace-的连通性" class="headerlink" title="3.4. ping 测试两个 namespace 的连通性"></a>3.4. ping 测试两个 namespace 的连通性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net0 ping 10.1.1.2</span><br><span class="line">PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.069 ms</span><br><span class="line">64 bytes from 10.1.1.2: icmp_seq=2 ttl=64 time=0.054 ms</span><br><span class="line">64 bytes from 10.1.1.2: icmp_seq=3 ttl=64 time=0.053 ms</span><br><span class="line">64 bytes from 10.1.1.2: icmp_seq=4 ttl=64 time=0.053 ms</span><br></pre></td></tr></table></figure><p>Done!</p><h2 id="4-多个不同-namespace-之间的通信"><a href="#4-多个不同-namespace-之间的通信" class="headerlink" title="4. 多个不同 namespace 之间的通信"></a>4. 多个不同 namespace 之间的通信</h2><p>2 个 namespace 之间通信可以借助 <code>veth pair</code> ，多个 namespace 之间的通信则可以使用 bridge 来转接，不然每两个 namespace 都去配 <code>veth pair</code> 将会是一件麻烦的事。下面就看看如何使用 bridge 来转接。</p><p>拓扑图如下：</p><p><img src="/images/virt/bridgens.png" alt=""></p><h3 id="4-1-使用-ip-link-和-brctl-创建-bridge"><a href="#4-1-使用-ip-link-和-brctl-创建-bridge" class="headerlink" title="4.1 使用 ip link 和 brctl 创建 bridge"></a>4.1 使用 ip link 和 brctl 创建 bridge</h3><p>通常 Linux 中和 bridge 有关的操作是使用命令 <code>brctl</code> (<code>yum install -y bridge-utils</code> ) 。但为了前后照应，这里都用 ip 相关的命令来操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 建立一个 bridge</span><br><span class="line"># ip link add br0 type bridge</span><br><span class="line"># ip link set dev br0 up</span><br><span class="line">9: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether 42:55:ed:eb:a0:07 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::4055:edff:feeb:a007/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><h3 id="4-2-创建-veth-pair"><a href="#4-2-创建-veth-pair" class="headerlink" title="4.2 创建 veth pair"></a>4.2 创建 veth pair</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//（1）创建 3 个 veth pair</span><br><span class="line"># ip link add type veth</span><br><span class="line"># ip link add type veth</span><br><span class="line"># ip link add type veth</span><br></pre></td></tr></table></figure><h3 id="4-3-将-veth-pair-的一头挂到-namespace-中，一头挂到-bridge-上，并设-IP-地址"><a href="#4-3-将-veth-pair-的一头挂到-namespace-中，一头挂到-bridge-上，并设-IP-地址" class="headerlink" title="4.3 将 veth pair 的一头挂到 namespace 中，一头挂到 bridge 上，并设 IP 地址"></a>4.3 将 veth pair 的一头挂到 namespace 中，一头挂到 bridge 上，并设 IP 地址</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// （1）配置第 1 个 net0</span><br><span class="line"># ip link set dev veth1 netns net0</span><br><span class="line"># ip netns exec net0 ip link set dev veth1 name eth0</span><br><span class="line"># ip netns exec net0 ip addr add 10.0.1.1/24 dev eth0</span><br><span class="line"># ip netns exec net0 ip link set dev eth0 up</span><br><span class="line">#</span><br><span class="line"># ip link set dev veth0 master br0</span><br><span class="line"># ip link set dev veth0 up</span><br><span class="line"></span><br><span class="line">// （2）配置第 2 个 net1</span><br><span class="line"># ip link set dev veth3 netns net1</span><br><span class="line"># ip netns exec net1 ip link set dev veth3 name eth0</span><br><span class="line"># ip netns exec net1 ip addr add 10.0.1.2/24 dev eth0</span><br><span class="line"># ip netns exec net1 ip link set dev eth0 up</span><br><span class="line">#</span><br><span class="line"># ip link set dev veth2 master br0</span><br><span class="line"># ip link set dev veth2 up</span><br><span class="line"></span><br><span class="line">// （3）配置第 3 个 net2</span><br><span class="line"># ip link set dev veth5 netns net2</span><br><span class="line"># ip netns exec net2 ip link set dev veth5 name eth0</span><br><span class="line"># ip netns exec net2 ip addr add 10.0.1.3/24 dev eth0</span><br><span class="line"># ip netns exec net2 ip link set dev eth0 up</span><br><span class="line"># </span><br><span class="line"># ip link set dev veth4 master br0</span><br><span class="line"># ip link set dev veth4 up</span><br></pre></td></tr></table></figure><p>这样之后，竟然通不了，经查阅 <a href="https://segmentfault.com/q/1010000010011053/a-1020000010025650" target="_blank" rel="noopener">参见</a> ，是因为</p><blockquote><p>原因是因为系统为bridge开启了iptables功能，导致所有经过br0的数据包都要受iptables里面规则的限制，而docker为了安全性，将iptables里面filter表的FORWARD链的默认策略设置成了drop，于是所有不符合docker规则的数据包都不会被forward，导致你这种情况ping不通。</p><p>解决办法有两个，二选一：</p><ol><li>关闭系统bridge的iptables功能，这样数据包转发就不受iptables影响了：echo 0 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</li><li>为br0添加一条iptables规则，让经过br0的包能被forward：iptables -A FORWARD -i br0 -j ACCEPT</li></ol><p>第一种方法不确定会不会影响docker，建议用第二种方法。</p></blockquote><p>我采用以下方法解决：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -A FORWARD -i br0 -j ACCEPT</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net0 ping -c 2 10.0.1.2</span><br><span class="line">PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.0.1.2: icmp_seq=1 ttl=64 time=0.071 ms</span><br><span class="line">64 bytes from 10.0.1.2: icmp_seq=2 ttl=64 time=0.072 ms</span><br><span class="line"></span><br><span class="line">--- 10.0.1.2 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.071/0.071/0.072/0.008 ms</span><br><span class="line"></span><br><span class="line"># ip netns exec net0 ping -c 2 10.0.1.3</span><br><span class="line">PING 10.0.1.3 (10.0.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.0.1.3: icmp_seq=1 ttl=64 time=0.071 ms</span><br><span class="line">64 bytes from 10.0.1.3: icmp_seq=2 ttl=64 time=0.087 ms</span><br><span class="line"></span><br><span class="line">--- 10.0.1.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.071/0.079/0.087/0.008 ms</span><br></pre></td></tr></table></figure><p>Done!</p><h2 id="5-Bridge-之间的同住机通信"><a href="#5-Bridge-之间的同住机通信" class="headerlink" title="5. Bridge 之间的同住机通信"></a>5. Bridge 之间的同住机通信</h2><p>以上所说的是一个 bridge 同网段的通信，现在看看不同 bridge 跨网段的通信，如下拓扑：</p><h2 id="6-Bridge-之间的跨住机通信"><a href="#6-Bridge-之间的跨住机通信" class="headerlink" title="6. Bridge 之间的跨住机通信"></a>6. Bridge 之间的跨住机通信</h2><p><a href="https://www.cnblogs.com/iiiiher/p/8057922.html" target="_blank" rel="noopener">https://www.cnblogs.com/iiiiher/p/8057922.html</a></p><p><strong>参考资料：</strong>  </p><p><a href="http://cizixs.com/2017/02/10/network-virtualization-network-namespace" target="_blank" rel="noopener">linux 网络虚拟化： network namespace 简介</a></p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」(id: cloud_dev)&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本文通过 IP 命令操作来简单介绍 network namespace 的基本概念和用法。&lt;/p&gt;
&lt;p&gt;深入了解可以看看我之前写的两篇文章 &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;amp;mid=2247483982&amp;amp;idx=1&amp;amp;sn=35e2aac1f4c164c8afa79aa91707c90d&amp;amp;chksm=ea7436f6dd03bfe036ccc8293aaf25d0c042f21afac5277bf04a32af36643a2780c980e53d09&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker 基础技术之 Linux namespace 详解&lt;/a&gt; 和 &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;amp;mid=2247483993&amp;amp;idx=1&amp;amp;sn=906551e374c0d8d40db00cbde934e624&amp;amp;chksm=ea7436e1dd03bff724d7ee03267115b2ecf54bd146fa3f826b69e918b5a2455d1562d085ab62&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker 基础技术之 Linux namespace 源码分析&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;和 network namespace 相关的操作的子命令是 &lt;code&gt;ip netns&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="06 网络" scheme="https://chambai.github.io/categories/06-%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="Namespace" scheme="https://chambai.github.io/tags/Namespace/"/>
    
      <category term="网络" scheme="https://chambai.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>一文搞懂 network namespace</title>
    <link href="https://chambai.github.io/2019/01/10/tech/%E6%A8%A1%E6%9D%BF/"/>
    <id>https://chambai.github.io/2019/01/10/tech/模板/</id>
    <published>2019-01-10T05:16:14.000Z</published>
    <updated>2019-04-17T12:30:42.690Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」(id: cloud_dev)&lt;/strong&gt; ，专注于干货分享，号内有 &lt;strong&gt;10T&lt;/strong&gt; 书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong
      
    
    </summary>
    
      <category term="06 网络" scheme="https://chambai.github.io/categories/06-%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="Namespace" scheme="https://chambai.github.io/tags/Namespace/"/>
    
      <category term="网络" scheme="https://chambai.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Linux 系统下实践 VLAN</title>
    <link href="https://chambai.github.io/2019/01/07/tech/Linux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E5%AE%9E%E8%B7%B5VLAN/"/>
    <id>https://chambai.github.io/2019/01/07/tech/Linux系统下实践VLAN/</id>
    <published>2019-01-07T05:16:14.000Z</published>
    <updated>2019-04-16T16:25:28.128Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><h2 id="01-准备环境"><a href="#01-准备环境" class="headerlink" title="01 准备环境"></a>01 准备环境</h2><p>环境：ubuntu 16.04 环境（物理 or 虚拟）</p><p>确认 CPU 是否支持虚拟化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># egrep -o &apos;(vmx|svm)&apos; /proc/cpuinfo</span><br><span class="line"># vmx</span><br></pre></td></tr></table></figure><p>如果不支持，开启 KVM 嵌套虚拟化之后再重启。</p><a id="more"></a><h3 id="1-1-安装-KVM-环境"><a href="#1-1-安装-KVM-环境" class="headerlink" title="1.1 安装 KVM 环境"></a>1.1 安装 KVM 环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y qemu-kvm qemu-system libvirt-bin virt-manager bridge-utils vlan</span><br></pre></td></tr></table></figure><h3 id="1-2-安装-Ubuntu-图形化界面"><a href="#1-2-安装-Ubuntu-图形化界面" class="headerlink" title="1.2 安装 Ubuntu 图形化界面"></a>1.2 安装 Ubuntu 图形化界面</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y xinit gdm kubuntu-desktop</span><br></pre></td></tr></table></figure><h2 id="02-创建-KVM-虚拟机"><a href="#02-创建-KVM-虚拟机" class="headerlink" title="02 创建 KVM 虚拟机"></a>02 创建 KVM 虚拟机</h2><p>使用 virt-manager 创建 KVM 虚拟机，方法比较简单，由于篇幅有限，大家可以查阅相关资料自行了解。</p><p>创建完之后用 <code>virsh list --all</code> 查看创建的 VM：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Id    Name                           State</span><br><span class="line">----------------------------------------------------</span><br><span class="line"> -     kvm1                           shut off</span><br><span class="line"> -     kvm2                           shut off</span><br><span class="line"> -     kvm3                           shut off</span><br></pre></td></tr></table></figure><p>我们的实验拓扑如下：</p><p><img src="/images/net/brvlan.png" alt=""></p><p>图中创建了 2 个 Linux Bridge：brvlan1 和 brvlan2，宿主机的物理网卡 eth0 抽象出两个虚拟设备 eth0.1 和 eth0.2，也就是两个 VLAN 设备，它们分别定义了两个 VLAN：VLAN1 和 VLAN2。挂接到两个 Bridge 上的网络设备自动加入到相应的 VLAN 中。VLAN1 接两个 VM，VLAN 接一个 VM。</p><p>实验的目的是要验证属于同一个 VLAN1 中 VM1 和 VM2 能 ping 通，而属于不同 VLAN 中的 VM ping 不通。</p><h2 id="03-实验开始"><a href="#03-实验开始" class="headerlink" title="03 实验开始"></a>03 实验开始</h2><h3 id="3-1-配置-VLAN"><a href="#3-1-配置-VLAN" class="headerlink" title="3.1 配置 VLAN"></a>3.1 配置 VLAN</h3><p>编辑 <code>/etc/network/interfaces</code>，加入两个 Bridge 和两个 VLAN 设备的配置，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># The primary network interface</span><br><span class="line">auto ens33</span><br><span class="line">iface ens33 inet dhcp</span><br><span class="line"></span><br><span class="line">auto ens33.1</span><br><span class="line">iface ens33.1 inet manual</span><br><span class="line">        vlan-raw-device ens33</span><br><span class="line"></span><br><span class="line">auto brvlan1</span><br><span class="line">iface brvlan1 inet manual</span><br><span class="line">        bridge_stp off</span><br><span class="line">        bridge_waitport 0</span><br><span class="line">        bridge_fd 0</span><br><span class="line">        bridge_ports ens33.1</span><br><span class="line"></span><br><span class="line">auto ens33.2</span><br><span class="line">iface ens33.2 inet manual</span><br><span class="line">        vlan-raw-device ens33</span><br><span class="line"></span><br><span class="line">auto brvlan2</span><br><span class="line">iface brvlan2 inet manual</span><br><span class="line">        bridge_stp off</span><br><span class="line">        bridge_waitport 0</span><br><span class="line">        bridge_fd 0</span><br><span class="line">        bridge_ports ens33.2</span><br></pre></td></tr></table></figure><p><strong>注意</strong>，这里务必和自己电脑的接口名称统一，比如我这里叫 ens33，就配 ens33.1 和 ens33.2 的 VLAN 设备，当然你也可以改成 eth0 的形式。</p><p>重启宿主机，<code>ifconfig</code> 查看网络接口：</p><p><img src="/images/net/vlanif.png" alt="vlanif.png"></p><p>用 <code>brctl show</code> 查看当前 Linux Bridge 的配置，ens33.1 和 ens33.2 分别挂载 brvlan1 和 brvlan2 上了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># brctl show</span><br><span class="line">bridge namebridge idSTP enabledinterfaces</span><br><span class="line">brvlan18000.000c298c57e8noens33.1</span><br><span class="line">brvlan28000.000c298c57e8noens33.2</span><br><span class="line">virbr08000.000000000000yes</span><br></pre></td></tr></table></figure><h3 id="3-2-配置-VM"><a href="#3-2-配置-VM" class="headerlink" title="3.2 配置 VM"></a>3.2 配置 VM</h3><p>我们先配置 VM1，启动 <code>virt-manager</code>，在图形界面中将 VM1 的虚拟网卡挂到 brvlan1 上：</p><p><img src="/images/net/vm1brvlan1.png" alt="vm1brvlan1.png"></p><p>同样的方式配置 VM2 和 VM3，VM2 也配到 brvlan1 上，VM3 配到 brvlan2 上。</p><h3 id="3-3-查看-VM-配置"><a href="#3-3-查看-VM-配置" class="headerlink" title="3.3 查看 VM 配置"></a>3.3 查看 VM 配置</h3><p>用 <code>virsh start xxx</code> 启动 3 个 VM：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># virsh start kvm1</span><br><span class="line"># virsh start kvm2</span><br><span class="line"># virsh start kvm3</span><br></pre></td></tr></table></figure><p>再通过 <code>brctl show</code> 查看 Bridge，这时发现 brvlan1 下接了 vnet0 和 vnet1，brvlan2 下接了 vnet2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># brctl show</span><br><span class="line">bridge namebridge idSTP enabledinterfaces</span><br><span class="line">brvlan18000.000c298c57e8noens33.1</span><br><span class="line">vnet0</span><br><span class="line">vnet1</span><br><span class="line">brvlan28000.000c298c57e8noens33.2</span><br><span class="line">vnet2</span><br><span class="line">virbr08000.000000000000yes</span><br></pre></td></tr></table></figure><p>通过 <code>virsh domiflist xxx</code> 确认这就是 VM 的虚拟网卡：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># virsh domiflist kvm1</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet0      bridge     brvlan1    rtl8139     52:54:00:b3:dd:3a</span><br><span class="line"></span><br><span class="line"># virsh domiflist kvm2</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet1      bridge     brvlan1    rtl8139     52:54:00:b7:4f:ef</span><br><span class="line"></span><br><span class="line"># virsh domiflist kvm3</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet2      bridge     brvlan2    rtl8139     52:54:00:d8:b8:2a</span><br></pre></td></tr></table></figure><h3 id="04-验证"><a href="#04-验证" class="headerlink" title="04 验证"></a>04 验证</h3><p>为了验证相同 VLAN 之间的连通性和不同 VLAN 之间的隔离性，我们为 3 个 VM 都配置同一网段的 IP。</p><p>使用 <code>virt-manager</code> 进入 VM console 控制面。</p><p>配置 VM1 的 IP：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 192.168.100.10 netmask 255.255.255.0</span><br></pre></td></tr></table></figure><p>配置 VM2 的 IP：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 192.168.100.20 netmask 255.255.255.0</span><br></pre></td></tr></table></figure><p>配置 VM3 的 IP：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 192.168.100.30 netmask 255.255.255.0</span><br></pre></td></tr></table></figure><p>使用 VM1 ping VM2 能 ping 通，VM2 ping VM3 不能 ping 通。</p><p><img src="/images/net/vlanping.png" alt="vlanping.png"></p><p>验证完毕。</p><p>大家如果有兴趣，可以抓个包看看，在发送 ping 包之前，需要知道对方的 MAC 地址，所以会先在网络中广播 ARP 包。ARP 是二层协议，VLAN 的作用就是隔离二层的广播域，ARP 包自然就不能在不同 VLAN 中流通，所以在相同 VLAN 中，通信双方能够拿到对方的 MAC 地址，也就能 ping 通，不同 VLAN 反之。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;01-准备环境&quot;&gt;&lt;a href=&quot;#01-准备环境&quot; class=&quot;headerlink&quot; title=&quot;01 准备环境&quot;&gt;&lt;/a&gt;01 准备环境&lt;/h2&gt;&lt;p&gt;环境：ubuntu 16.04 环境（物理 or 虚拟）&lt;/p&gt;
&lt;p&gt;确认 CPU 是否支持虚拟化：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# egrep -o &amp;apos;(vmx|svm)&amp;apos; /proc/cpuinfo&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# vmx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果不支持，开启 KVM 嵌套虚拟化之后再重启。&lt;/p&gt;
    
    </summary>
    
      <category term="06 网络" scheme="https://chambai.github.io/categories/06-%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://chambai.github.io/tags/Linux/"/>
    
      <category term="网络" scheme="https://chambai.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
      <category term="VLAN" scheme="https://chambai.github.io/tags/VLAN/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 012 Pod 的自动扩容与缩容</title>
    <link href="https://chambai.github.io/2018/09/30/tech/Kubernetes_%E7%AC%94%E8%AE%B0_012_Pod_%E7%9A%84%E8%87%AA%E5%8A%A8%E6%89%A9%E5%AE%B9%E4%B8%8E%E7%BC%A9%E5%AE%B9/"/>
    <id>https://chambai.github.io/2018/09/30/tech/Kubernetes_笔记_012_Pod_的自动扩容与缩容/</id>
    <published>2018-09-30T05:16:14.000Z</published>
    <updated>2019-04-11T16:01:06.222Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 12 篇。</p><p>上一篇我们了解了 Pod 的手动扩容和缩容，本篇来看看自动的方式。</p><a id="more"></a><p>K8S 作为一个集群式的管理软件，自动化、智能化是免不了的功能。Google 在 K8S v1.1 版本中就加入了这个 Pod 横向自动扩容的功能（Horizontal Pod Autoscaling，简称 HPA）。</p><p>HPA 与之前的 Deployment、Service 一样，也属于一种 K8S 资源对象。</p><p>HPA 的目标是希望通过追踪集群中所有 Pod 的负载变化情况，来自动化地调整 Pod 的副本数，以此来满足应用的需求和减少资源的浪费。</p><p>HAP 度量 Pod 负载变化情况的指标有两种：  </p><ul><li>CPU 利用率（CPUUtilizationPercentage）</li><li>自定义的度量指标，比如服务在每秒之内的请求数（TPS 或 QPS）</li></ul><p>如何统计和查询这些指标，要依托于一个组件——Heapster。Heapster 会监控一段时间内集群内所有 Pod 的 CPU 利用率的平均值或者其他自定义的值，在满足条件时（比如 CPU 使用率超过 80% 或 降低到 10%）会将这些信息反馈给 HPA 控制器，HPA 控制器就根据 RC 或者 Deployment 的定义调整 Pod 的数量。</p><p>HPA 实现的方式有两种：配置文件和命令行</p><ol><li>配置文件</li></ol><p>这种方式是通过定义 yaml 配置文件来创建 HPA，如下是基本定义：   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: php-apache</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:               # (1)</span><br><span class="line">    kind: Deployment   </span><br><span class="line">    name: php-apache</span><br><span class="line">  minReplicas: 1                # (2)</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  targetAverageUtilization: 50  # (3)</span><br></pre></td></tr></table></figure><p>文件 kind 类型是 <code>HorizontalPodAutoscaler</code>，其中有 3 个地方需要额外注意下：  </p><p>（1）<code>scaleTargetRef</code> 字段指定需要管理的 Deployment/RC 的名字，也就是提前需要存在一个 Deployment/RC 对象。</p><p>（2） <code>minReplicas</code> 和 <code>maxReplicas</code> 字段定义 Pod 可伸缩的数量范围。这个例子中扩容最高不能超过 10 个，缩容最低不能少于 1 个。</p><p>（3）<code>targetAverageUtilization</code> 指定 CPU 使用率，也就是自动扩容和缩容的触发条件，当 CPU 使用率超过 50% 时会触发自动动态扩容的行为，当回落到 50% 以下时，又会触发自动动态缩容的行为。</p><ol start="2"><li>命令行</li></ol><p>这种方式就是通过 <code>kubectl autoscale</code> 命令来实现创建 HPA 对象，实现自动扩容和缩容行为。比如和上面的例子等价的命令如下：   </p><p><code>kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10</code></p><p>通过参数来引入各个字段。</p><p>OK，本文就到这里，更多实践的例子大家可以参考 K8S 官网。下文我们将会探索 K8S 的容错机制。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 12 篇。&lt;/p&gt;
&lt;p&gt;上一篇我们了解了 Pod 的手动扩容和缩容，本篇来看看自动的方式。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 11 Pod 扩容与缩容 双十一前后的忙碌</title>
    <link href="https://chambai.github.io/2018/09/29/tech/Kubernetes_%E7%AC%94%E8%AE%B0_11_Pod_%E6%89%A9%E5%AE%B9%E4%B8%8E%E7%BC%A9%E5%AE%B9_%E5%8F%8C%E5%8D%81%E4%B8%80%E5%89%8D%E5%90%8E%E7%9A%84%E5%BF%99%E7%A2%8C/"/>
    <id>https://chambai.github.io/2018/09/29/tech/Kubernetes_笔记_11_Pod_扩容与缩容_双十一前后的忙碌/</id>
    <published>2018-09-29T05:16:14.000Z</published>
    <updated>2019-04-11T15:58:36.247Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 11 篇。</p><p>资源的伸缩在云计算环境中是至关重要的，云计算的动机就是企图提高资源的利用率，在用户请求高峰期的时候能够对资源进行横向扩容，反之，当用户请求回落低谷的时候，能够及时缩减资源，避免资源的浪费。</p><a id="more"></a><p>这就像双十一的时候，随着用户不断地涌入，阿里后台需要不断调配更多的资源来支撑用户大量的请求，当过了双十一当天，再慢慢缩减资源的使用。</p><p>Kubernetes 作为一个集群管理系统，提供了两种资源伸缩的方式：手动和自动。本文先来看手动方式。</p><p>Kubernetes 的资源伸缩本质上指的是 Pod 的扩容和缩容（scale up/down），也就是增加或减少 Pod 的副本数。</p><p>手动的方式是使用 <code>kubectl scale</code> 命令手动进行，或者基于 YAML 配置来实现。</p><p>首先，定义一个 <code>nginx-deployment.yaml</code> 配置文件：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web_server</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        images: nginx:1.12.1</span><br></pre></td></tr></table></figure><p>其中定义了 3 个副本，执行 <code>kubectl create -f nginx-deployment.yaml</code> 创建 Pod。</p><center><img src="/images/k8s/pod_scale.png" alt=""></center><p>如果现在遇到高峰请求，我们急需进行扩容，执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment nginx-deployment --replicas 5</span><br></pre></td></tr></table></figure></p><p>将 Pod 扩增到 5 个。</p><center><img src="/images/k8s/pod_scale_up.png" alt=""></center><p>其中，用 <code>--replicas</code> 来指示增缩的数量，对于缩容，将 <code>--replicas</code> 设置为比当前 Pod 副本数量更小的数字即可，比如缩容到 2 个如下：</p><center><img src="/images/k8s/pod_scale_down.png" alt=""></center><p>可以看到，Pod 销毁会经历一个 <code>Terminating</code> 的过程，最终 3 个副本被删除，只保留了 2 个副本。</p><p>以上是通过命令的形式来实现手动的扩容和缩容，我们也可以修改 YAML 配置文件中的 <code>replicas</code> 来实现，只要修改完之后执行 <code>kubectl apply</code> 即可。</p><p>OK，本文到此为止，下文我们再来 Pod 伸缩的另一种方式——自动扩容和缩容。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 11 篇。&lt;/p&gt;
&lt;p&gt;资源的伸缩在云计算环境中是至关重要的，云计算的动机就是企图提高资源的利用率，在用户请求高峰期的时候能够对资源进行横向扩容，反之，当用户请求回落低谷的时候，能够及时缩减资源，避免资源的浪费。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 10 Job 机器人加工厂</title>
    <link href="https://chambai.github.io/2018/09/26/tech/Kubernetes_%E7%AC%94%E8%AE%B0_10_Job_%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A0%E5%B7%A5%E5%8E%82/"/>
    <id>https://chambai.github.io/2018/09/26/tech/Kubernetes_笔记_10_Job_机器人加工厂/</id>
    <published>2018-09-26T05:16:14.000Z</published>
    <updated>2019-04-11T15:59:54.399Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 10 篇。</p><p>通常，我们在执行任务时，会启用多个服务，有些任务需要长时间运行，全天 24 小时不中断，所以一般会启用 Daemon 类的 服务；而有些任务则只需要短暂执行，任务执行完，服务就没有存在的必要了。</p><a id="more"></a><p>容器提供服务运行的环境，根据任务持续运行的时间，将容器分为两类：服务类容器和工作类容器。</p><p>服务类容器需要一直运行来提供持续性的服务，而工作类容器则是运行一次性的任务，任务完成后便会退出。</p><p>前面学习的 Deployment、ReplicaSet 和 DaemonSet 都用于管理服务类容器，而工作类容器则由本文要讲得 Job 来管理。</p><p>Job 多用于执行一次性的任务，批处理任务等，Job 就像是现代化机械加工厂的机器人，当有任务来的时候，便会启动，按照预先设定好的程序执行任务，直至任务执行完，便会进入休眠状态。</p><p>进一步，Job 根据任务的类型和执行的动作又分为以下几类：   </p><ul><li>单 Job 单任务：只启动一个 Job 来完成任务，同时 Job 只启用一个 Pod ，适用于简单的任务。</li><li>多 Job 多任务：启动多个 Job 来处理批量任务，每个任务对应一个 Job，Pod 的数量可以自定义。</li><li>单 Job 多任务：采用一个任务队列来存放任务，启动一个 Job 作为消费者来处理这些任务，Job 会启动多个 Pod，Pod 的数量可以自定义。</li><li>定时 Job：也叫 CronJob，启动一个 Job 来定时执行任务，类似 Linux 的 Crontab 程序。</li></ul><p>上述 Job 的分类需要注意两点：</p><p>1）Job 执行失败的重启策略；Job 执行的是一次性的任务，但也不保证一定能执行成功，如果执行失败，应该怎么处理？这个是由前面所讲的 Pod 重启策略来决定的。在 Job Controller 中，只允许定义两种策略：  </p><ul><li>Never：Pod 执行失败，不会重启该 Pod，但会根据 Job 定义的期望数重新创建 Pod。</li><li>OnFailure：Pod 执行失败，则会尝试重启该 Pod。</li></ul><p>两种策略尝试的次数由 <code>spec.backoffLimits</code> 来限制，默认是 6 次（K8S 1.8.0 新加的特性）。</p><p>2）批量任务的多次并行处理的限制；对于批量任务，通常是一个 Pod 对应一个任务，但有时为了加快任务的处理，会启动多个 Pod 来并行处理单个任务。可以通过下面两个参数来设置并行度：</p><ul><li><code>spec.completions</code>：总的启动 Pod 数，只有当所有 Pod 执行成功结束，任务才结束。</li><li><code>spec.parallelism</code>：每个任务对应的 Pod 的并行数，当有一个 Pod 执行成功结束，该任务就执行结束。</li></ul><p>下面通过几个例子来实践一下上面的几种 Job 类别。</p><h3 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h3><h4 id="单-Job-单-Pod-执行一次性任务"><a href="#单-Job-单-Pod-执行一次性任务" class="headerlink" title="单 Job 单 Pod 执行一次性任务"></a>单 Job 单 Pod 执行一次性任务</h4><p>首先，定义 Job 的 yaml 配置文件 myjob.yaml：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: myjob</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: myjob</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: hello</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;echo&quot;, &quot;hello, I&apos;m Linux云计算网络, Welcome&quot;]</span><br><span class="line">      restartPolicy: Never</span><br></pre></td></tr></table></figure><p>执行 <code>kubectl create -f myjob.yaml</code> 创建 job 对象：</p><center><img src="/images/k8s/job-yaml.png" alt=""></center><p>可以看到期望创建的 Job 数为 1，成功执行的 Job 数也为 1，这表明该 Job 已经执行完任务退出了。这个 Job 执行的任务就是创建一个 Pod，Pod 中创建一个 busybox 容器，并进入容器输出一段字符串：<strong>“hello, I’m Linux云计算网络, Welcome”</strong>。</p><p>查看一下 Pod 的状态：   </p><center><img src="/images/k8s/job-pod.png" alt=""></center><p>可以看到，该 Pod 的状态为 <code>Completed</code>，表示它已经执行完任务并成功退出了。那怎么看该任务的执行结果呢？可以执行 <code>kubectl logs myjob</code> 调出该 Pod 的历史执行信息进行查看：</p><center><img src="/images/k8s/job-logs.png" alt=""></center><p>看到历史输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hello, I&apos;m Linux云计算网络, Welcome</span><br></pre></td></tr></table></figure><p>以上是执行成功的情况，如果执行失败，会根据 <code>restartPolicy</code> 进行重启，重启的方式上面也说了。大家可以自己实践下。</p><h4 id="多-Job-多-Pod-执行批量任务"><a href="#多-Job-多-Pod-执行批量任务" class="headerlink" title="多 Job 多 Pod 执行批量任务"></a>多 Job 多 Pod 执行批量任务</h4><p>首先，定义 Job 的 yaml 模板文件 job.yaml.txt，然后再根据这个模板文件创建多个 Job yaml 文件。模板文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: work-item-$ITEM</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: job</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: c</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo work item $ITEM &amp;&amp; sleep 2&quot;]</span><br><span class="line">      restartPolicy: Never</span><br></pre></td></tr></table></figure><p>其中，<code>$ITEM</code> 作为各个 Job 项的标识。接着，使用以下脚本，根据 Job 模板创建三个 Job 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">for i in app book phone</span><br><span class="line">do</span><br><span class="line">  cat myjob_tmp.yaml | sed &quot;s/\$ITEM/$i/g&quot; &gt; ./jobs/job-$i.yaml</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>最后，创建三个 Job 对象，如下：</p><center><img src="/images/k8s/job-multi.png" alt=""></center><h4 id="单-Job-多-Pod-执行批量任务"><a href="#单-Job-多-Pod-执行批量任务" class="headerlink" title="单 Job 多 Pod 执行批量任务"></a>单 Job 多 Pod 执行批量任务</h4><p>这种方式是用一个队列来存放任务，然后启动一个 Job 来执行任务，Job 可以根据需求启动多个 Pod 来承载任务的执行。定义下面的配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: myjob</span><br><span class="line">spec:</span><br><span class="line">  completions: 6</span><br><span class="line">  parallelism: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: myjob</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: hello</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;echo&quot;, &quot;hello Linux云计算网络&quot;]</span><br><span class="line">      restartPolicy: OnFailure</span><br></pre></td></tr></table></figure><p>这里用到了上面说的两个参数：<code>completions</code> 和 <code>parallelism</code>，表示每次并行运行两个 Pod，直到总共 6 个 Pod 成功运行完成。如下：</p><center><img src="/images/k8s/job-para.png" alt=""></center><p>可以看到 DESIRED 和 SUCCESSFUL 最终均为 6，符合预期，实际上也有 6 个 Pod 成功运行并退出，呈 <code>Completed</code> 状态。</p><p>随便查看其中一个 Pod 的历史执行情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl logs myjob-5lfnp</span><br><span class="line">hello Linux云计算网络</span><br></pre></td></tr></table></figure><h4 id="定时任务-CronJob"><a href="#定时任务-CronJob" class="headerlink" title="定时任务 CronJob"></a>定时任务 CronJob</h4><p>定义一个 CronJob 配置文件，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  schedule: &quot;*/1 * * * *&quot;</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: hello</span><br><span class="line">            images: busybox</span><br><span class="line">            command: [&quot;echo&quot;, &quot;Hello Linux云计算网络&quot;]</span><br><span class="line">          restartPolicy: OnFailure</span><br></pre></td></tr></table></figure><p>kind 类型为 CronJob，<code>spec.schedule</code> 表示定时调度，指定什么时候运行 Job，格式与 Linux 的 Crontab 命令是一样的，这里 <code>*/1 * * * *</code> 的含义是每一分钟启动一次。</p><p>创建 CronJob 对象，通过 <code>kubectl get cronjob</code> 查看 CronJob 的状态：  </p><center><img src="/images/k8s/cronjob-get.png" alt=""></center><p>过一段时间再查看 Pod 的状态：</p><center><img src="/images/k8s/cronjob.png" alt=""></center><p>可以看到，此时产生了 3 个 Pod，3 个 Jobs，这是每隔一分钟就会启动一个 Job。执行 <code>kubectl logs</code> 查看其中一个的历史执行情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl logs hello-1536764760-lm5kt</span><br><span class="line">Hello Linux云计算网络</span><br></pre></td></tr></table></figure><p>到此，本文就结束了。我们从理论结合实践，梳理了 Job 的几种类型，下文我们开始看一种有状态的 Controller——StatefulSet。</p><p>同样，需要学习资料的后台回复“K8S” 和 “K8S2”，想加群学习回复“加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 10 篇。&lt;/p&gt;
&lt;p&gt;通常，我们在执行任务时，会启用多个服务，有些任务需要长时间运行，全天 24 小时不中断，所以一般会启用 Daemon 类的 服务；而有些任务则只需要短暂执行，任务执行完，服务就没有存在的必要了。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 09 DaemonSet 我是一只看门狗</title>
    <link href="https://chambai.github.io/2018/09/19/tech/Kubernetes_%E7%AC%94%E8%AE%B0_09_DaemonSet_%E6%88%91%E6%98%AF%E4%B8%80%E5%8F%AA%E7%9C%8B%E9%97%A8%E7%8B%97/"/>
    <id>https://chambai.github.io/2018/09/19/tech/Kubernetes_笔记_09_DaemonSet_我是一只看门狗/</id>
    <published>2018-09-19T05:16:14.000Z</published>
    <updated>2019-04-11T16:00:32.410Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 9 篇。</p><p>和上文中的 Deployment 一样，DaemonSet 也是一种副本管理机制，和 Deployment 可以在每个 Node 上运行好几个 Pod 副本不同的是，DaemonSet 始终保证每个 Node 最多只会运行一个副本，就像它的名称一样，作为一只看门狗（Daemon）守护在主人家里。</p><a id="more"></a><p>那么，哪些应用适合用 DaemonSet 的方式来部署呢？</p><p>主要有以下几类：</p><ul><li>监控类的，比如 Prometheus，collectd，New Relic agent，Ganglia gmond 等。</li><li>系统管理类的，比如 kube-proxy, kube-flannel 等。</li><li>日志收集类的，比如 fluentd，logstash 等。</li><li>数据存储类的，比如 glusterd, ceph 等。</li><li>……</li></ul><p>其中，系统管理类的应用主要是 K8S 自身的一些系统组件，我们可以通过 <code>kubectl get daemonset --namespace=kube-system</code> 查看到：  </p><center><img src="/images/k8s/daemon-sys.png" alt=""></center><p>DaemonSet <code>kube-proxy</code> 和 <code>kube-flannel-ds</code> 有 3 个副本，分别负责在每个节点上运行 kube-proxy 和 flannel 组件。</p><p>kube-proxy 前面的文章讲过，它有负载均衡的功能，主要将外部对 Service 的访问导向后端的 Pod 上。显然，一个 Node 运行一个负载均衡器足矣。</p><p>我们可以通过 <code>kubectl edit daemonset kube-proxy --namespace=kube-system</code> 来查看 kube-proxy 的 yaml 配置文件。</p><center><img src="/images/k8s/kube-proxy-dae.png" alt=""></center><p>可以看到它的 kind 是 DaemonSet。</p><p>接着再来看 kube-flannel-ds，这是一个网络插件组件，主要用于构建 K8S 的集群网络，这里大家不懂可以跳过，不影响本文的理解，后面在讲到 K8S 网络的时候会重点讲这个网络方案。</p><p>这里我们只需要知道，各个 Pod 间的网络连通就是 flannel 来实现的。</p><p>这是一个第三方的插件，我们可以直接下载它的 yaml 文件进行安装，执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><p>得到 kube-flannel.yml 文件：</p><center><img src="/images/k8s/kube-flannel-dae.png" alt=""></center><p>这里只列出了一部分内容，kind 类型是 DaemonSet。</p><p>其实 DaemonSet 配置文件的语法和结构和 Deployment 几乎完全一样，不同就在于将 kind 设为 DaemonSet。</p><p>OK，DaemonSet 的探讨就到这里，下文我们继续讨论另外一种 Controller：Job。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 9 篇。&lt;/p&gt;
&lt;p&gt;和上文中的 Deployment 一样，DaemonSet 也是一种副本管理机制，和 Deployment 可以在每个 Node 上运行好几个 Pod 副本不同的是，DaemonSet 始终保证每个 Node 最多只会运行一个副本，就像它的名称一样，作为一只看门狗（Daemon）守护在主人家里。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 08 Deployment 副本管理 重新招一个员工来填坑</title>
    <link href="https://chambai.github.io/2018/09/16/tech/Kubernetes_%E7%AC%94%E8%AE%B0_08_Deployment_%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86_%E9%87%8D%E6%96%B0%E6%8B%9B%E4%B8%80%E4%B8%AA%E5%91%98%E5%B7%A5%E6%9D%A5%E5%A1%AB%E5%9D%91/"/>
    <id>https://chambai.github.io/2018/09/16/tech/Kubernetes_笔记_08_Deployment_副本管理_重新招一个员工来填坑/</id>
    <published>2018-09-16T05:16:14.000Z</published>
    <updated>2019-04-11T16:01:03.442Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 8 篇。</p><p>Deployment 是 K8S v1.2 引入的概念，与之一起引入还有 ReplicaSet。这两个概念是等同的，准确说是 Deployment 内部调用 ReplicaSet 来实现。</p><a id="more"></a><p>之前这个概念是由 Replication Controller 来实现的，但由于和 K8S 代码中的模块重名，所以就改成 Deployment + ReplicaSet 的组合。</p><p>Deployment 实现了 Pod 的副本管理，使得应用的表现形态和用户期望的状态保持一致。比如用户期望应用部署为 3 副本，如果在运行过程中有一个副本挂了，那么 Deployment 会自动拉起一个副本。</p><p>Deployment 对于应用的编排、自动扩容和缩容、升级回滚等功能都是至关重要的。</p><p>下面我们通过一个例子来看看 Deployment 是如何工作的。</p><p>定义一个 <code>nginx.yaml</code> 文件（对 yaml 文件不熟悉的可以查阅这篇文章）：   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1   </span><br><span class="line">kind: Deployment </span><br><span class="line">metadata:  </span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2  </span><br><span class="line">  template:  </span><br><span class="line">    metadata:</span><br><span class="line">      labels:  </span><br><span class="line">        app: web-server</span><br><span class="line">    spec: </span><br><span class="line">      containers:  </span><br><span class="line">      - name: nginx  </span><br><span class="line">        images: nginx:1.12.1     </span><br><span class="line">        ports:  </span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure><p>这个文件定义了一个 nginx 容器应用，两个 Pod 副本。也就是每个 Pod 中会跑一个 nginx 应用。</p><p>执行<code>kubectl create -f nginx.yaml</code>创建 Deployment 对象，在执行 <code>kubectl get deploy</code> 查看创建的 Deployment。</p><center><img src="/images/k8s/deploy-yaml.png" alt=""></center><p>可以看到，其中两个参数 <code>desired</code>（期待副本数）和 <code>current</code>（当前副本数）都为 2，保持一致，我们再执行 <code>kubectl get pod -o wide</code> 查看当前 Pod 的情况：</p><center><img src="/images/k8s/deploy-pod.png" alt=""></center><p>可以看到，创建了两个 Pod 自动调度到了 Node1 和 Node2 上。这说明每个 Pod 副本是由 Deployment 统一创建并维护的。</p><p>为了一探究竟，我们继续深挖 Deployment。</p><p>执行 <code>kubectl describe deployment nginx-deployment</code> 查看该 Deployment 的详细信息。</p><center><img src="/images/k8s/deploy-replica.png" alt=""></center><p>图中圈住的地方告诉我们，这里创建了一个 ReplicaSet，也就是说 Deployment 内部是调用 ReplicaSet 来完成 Pod 副本的创建的。是否是这样，我们继续验证。</p><p>执行 <code>kubeclt get replicaset</code> 显示创建的 ReplicaSet 对象：</p><center><img src="/images/k8s/deploy-get-replica.png" alt=""></center><p>可以看到这里的 ReplicaSet 名称和上面 Deployment 信息里显示的是一样的，同样，执行 <code>kubectl describe replicaset xxx</code> 显示该 ReplicaSet 的详细信息。</p><center><img src="/images/k8s/deploy-replica-events.png" alt=""></center><p>图中，有两处地方值得注意。一处是 <code>Controlled By</code>，表明 ReplicaSet 是由谁创建并控制的，显然这里显示是 Deployment。第二处是 <code>Events</code>，Events 记录了 K8S 中每一种对象的日志信息，这里的信息有助于排错查问题。我们可以看到这里记录了两个 Pod 副本的创建，Pod 的名称和我们在上面执行 <code>kubectl get pod</code> 看到的结果是一样的。</p><p>继续执行 <code>kubectl describe pod xxx</code> 查看其中一个 Pod 的详细信息：</p><center><img src="/images/k8s/deploy-pod-des.png" alt=""></center><p>可以看到这个 Pod 是由 ReplicaSet 创建的。</p><p>到此，我们不难得出下面这幅图：</p><center><img src="/images/k8s/deploy-pod1.png" alt=""></center><p>用户通过 kubeclt 创建 Deployment，Deployment 又创建 ReplicaSet，最终由 ReplicaSet 创建 Pod。</p><p>从命名上我们也可以看出，子对象的名字 = 父对象的名字 + 随机字符串。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们从实践上剖析了 Deployment 创建 Pod，实际上经过 ReplicaSet 进行创建。Deployment 最主要是对 Pod 进行副本管理，这样可以进行很多自动化管理的复杂操作，后面我们逐步从实践上去剖析 Pod 的各种操作。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文首发于我的公众号 &lt;strong&gt;「Linux云计算网络」&lt;/strong&gt; ，专注于干货分享，号内有大量书籍和视频资源，后台回复&lt;strong&gt;「1024」&lt;/strong&gt;即可领取，欢迎大家关注，二维码文末可以扫。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 8 篇。&lt;/p&gt;
&lt;p&gt;Deployment 是 K8S v1.2 引入的概念，与之一起引入还有 ReplicaSet。这两个概念是等同的，准确说是 Deployment 内部调用 ReplicaSet 来实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>历史上那些具有跨时代意义的云计算创业公司</title>
    <link href="https://chambai.github.io/2018/09/13/tech/%E5%8E%86%E5%8F%B2%E4%B8%8A%E9%82%A3%E4%BA%9B%E5%85%B7%E6%9C%89%E8%B7%A8%E6%97%B6%E4%BB%A3%E6%84%8F%E4%B9%89%E7%9A%84%E4%BA%91%E8%AE%A1%E7%AE%97%E5%88%9B%E4%B8%9A%E5%85%AC%E5%8F%B8/"/>
    <id>https://chambai.github.io/2018/09/13/tech/历史上那些具有跨时代意义的云计算创业公司/</id>
    <published>2018-09-13T05:16:14.000Z</published>
    <updated>2019-04-11T14:38:24.063Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/cloud/8.jpg" alt=""></center><p>这里所说的不是像 Google、VMware、Cisco、Intel 等等这些巨头公司，而是在巨头的夹缝中冉冉升起的那些新星。由于知识面有限，列举不尽完全，大家可以留言说说你心目中还有哪些值得铭记的公司。</p><a id="more"></a><h3 id="Rackspace"><a href="#Rackspace" class="headerlink" title="Rackspace"></a>Rackspace</h3><hr><p>Rackspace 理应不算新星了，但说到“跨时代意义”，它是绝对的名副其实。原因就在于它和 NASA（美国国家航空航天局）合作开发了 OpenStack——一个让云计算产业进入井喷时代的项目。OpenStack 一经开源，短短两年时间就力压群雄成为开源社区仅次于 Linux 的全球第二大开源项目。</p><center><img src="/images/cloud/rackspace.jpg" alt=""></center><p>可以说是 OpenStack 拯救了 Rackspace。Rackspace 于 1998 成立，是最早期的一批云计算提供商，OpenStack 出来之前，一直不温不火，2010 年 OpenStack 出来之后，名声大震，一步跃居为全球仅次于 Amazon 和 VMware 的三大云计算中心之一。</p><p>如今 OpenStack 已经更新到 Rocky 版本，在众多竞争者和效仿者的围追堵截之下，依然很强势，这和它优秀的架构是分不开的。我很好奇它更新到 Z 版本之后会发生什么？</p><h3 id="dotCloud"><a href="#dotCloud" class="headerlink" title="dotCloud"></a>dotCloud</h3><hr><p>OpenStack 带动了 IaaS 的发展，在这个节骨眼上，人们开始意识到，只有 IaaS 已经无法满足用户变态的需求了。</p><p>时间也落在 2010 年，这个时候几个大胡子年轻人在旧金山成立了一家做 PaaS 的公司，起名 dotCloud，开始杀入 PaaS 领域。</p><center><img src="/images/cloud/dotcloud.png" alt=""></center><p>年轻一辈都意识到了商机，大佬们（Google、Microsoft、Amazon 等）能不意识到吗？于是大佬们也纷纷涉足 PaaS 领域，最终 dotCloud 寡不敌众，不得不缴械投降。</p><p>dotCloud 的工程师于心不甘啊，辛辛苦苦画下的大饼就这样被割分完了。但也没办法，谁让人家是大佬呢。</p><p>鉴于工程师们血液里都流淌着一股热爱分享的劲儿，他们决定将 dotCloud 的核心技术开源给世人。</p><p>谁能想到，无心插柳柳成荫。这门技术瞬间风靡全球，开启了又一个新的时代。</p><p>这门技术就是 Docker。</p><center><img src="/images/cloud/docker.jpg" alt=""></center><p>dotCloud 又火了，但为了纪念这个神圣的时刻，dotCloud 改名为 Docker Inc，全身心投入 Docker 的研发中。至于原来的 PaaS 业务，Docker 将其卖给了德国人的 cloudControl。但好景不长，cloudControl 于 2016 年就关闭了。</p><p>如今，Docker 技术依然可圈可点，虽然很多人在 Kubernetes 出来之后，叫衰 Docker，但我却不以为然。</p><h3 id="Nicira"><a href="#Nicira" class="headerlink" title="Nicira"></a>Nicira</h3><hr><p>2007 年，斯坦福大学的 Nick McKeown 教授和他的天才学生 Martin Casado 博士根据他们的研究成果创办了 Nicira。这是 SDN 网络的鼻祖公司，Nick 教授和 Martin 博士也因此被人们称为 SDN 之父。</p><p>Nicira 公司做了很多牛逼的事：发明了世界上第一个 SDN 控制器 NOX，第一个分布式交换机 Open vSwitch 及其配套协议 OpenFlow 协议，领导研发 OpenStack 网络驱动模块 Quantum/Neutron，开源了业界第一个与硬件无关，支持多种 X86 虚拟化平台的分布式网络虚拟化架构（DVNI），等等等等。</p><p>Nicira 当时那套 SDN 的解决方案，放在今天来说都是很超前的东西。很快，Nicira 就被 VMware 和 Cisco 这些巨头盯上了。巨头们看上的不仅是 Nicira 的技术，更是 Nicira 那帮工程师天才般的智慧和对技术敏锐的洞察力。</p><p>巨头们开始了疯狂的收购战，最终 VMware 以 12.6 亿美元拔得头筹。Martin 博士也带领他的一般众将归入了 VMware 的麾下。</p><center><img src="/images/cloud/nicira.jpg" alt=""></center><p>至此，Nicira 的舞台也算退出了。Open vSwitch 如今仍然占据 SDN 数据面的头把交椅。</p><h3 id="Palo-Alto-Networks"><a href="#Palo-Alto-Networks" class="headerlink" title="Palo Alto Networks"></a>Palo Alto Networks</h3><hr><p>2005 年，以色列的一个天才少年 Nir Zuk 在一间简陋的办公室里，一手创办了 PAN，这就是当下极富盛名的下一代防火墙的初创者（很多公司都宣称自己的防火墙是下一代防火墙，众说纷坛，其实争论这个没多大意义，人家 PAN 都还没说话呢）。</p><center><img src="/images/cloud/palo.jpg" alt=""></center><p>回看 Nir 的一生，也是颇具传奇，经历像极了乔布斯，但没乔布斯那么惨是被老东家赶出的，Nir 是因为老东家小气不肯给他资源做项目而无奈出走。谁能想到，PAN 能在短时间之内就超越了老东家。</p><blockquote><p>小公司要在大公司的夹缝中生长，需要天时地利人和，以及运气，就像 Docker，上帝为你关上一扇门，就会为你打开一扇窗。</p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/cloud/8.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;这里所说的不是像 Google、VMware、Cisco、Intel 等等这些巨头公司，而是在巨头的夹缝中冉冉升起的那些新星。由于知识面有限，列举不尽完全，大家可以留言说说你心目中还有哪些值得铭记的公司。&lt;/p&gt;
    
    </summary>
    
      <category term="01 云计算" scheme="https://chambai.github.io/categories/01-%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="OpenStack" scheme="https://chambai.github.io/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 07 豌豆荚之旅（二）</title>
    <link href="https://chambai.github.io/2018/09/11/tech/Kubernetes_%E7%AC%94%E8%AE%B0_07_%E8%B1%8C%E8%B1%86%E8%8D%9A%E4%B9%8B%E6%97%85%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>https://chambai.github.io/2018/09/11/tech/Kubernetes_笔记_07_豌豆荚之旅（二）/</id>
    <published>2018-09-11T05:16:14.000Z</published>
    <updated>2019-04-11T14:38:24.060Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/9.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学习 K8S，这是系列第 7 篇。</p><p>上篇我们简单学习了 Pod 的基础知识，本篇开始讲述一些 Pod 的高阶知识（本文只做理论的简单阐述，后面会针对每个点进行实践）。</p><a id="more"></a><h3 id="Pod-的生命周期管理"><a href="#Pod-的生命周期管理" class="headerlink" title="Pod 的生命周期管理"></a>Pod 的生命周期管理</h3><p>豌豆荚自诞生之日起，便注定要经历生老病死的一生。Pod 是由容器组成的，Pod 生命周期实际上是由容器的生命周期决定的。</p><p>在整个生命周期过程中，Pod 会被定义为各种状态，如下：</p><center><img src="/images/k8s/pod_lifetime.png" alt=""></center><p>这些状态包括正常状态和异常状态，当出现异常状态时，K8S 的监控机制会检测到这种异常，并执行相应的异常处理。</p><h3 id="Pod-的监控机制"><a href="#Pod-的监控机制" class="headerlink" title="Pod 的监控机制"></a>Pod 的监控机制</h3><p>Pod 的监控主要是监控 Pod 内容器的健康状况，并进行相关的异常处理和容错管理。</p><p>当监控到某个容器异常退出或健康检查失败时，Pod 会执行重启策略，使得 Pod 内的容器健康运转。</p><p>如下记录了 Pod 的重启策略和健康检查机制：</p><center><img src="/images/k8s/pod_jk.png" alt=""></center><h3 id="Pod-的调度管理"><a href="#Pod-的调度管理" class="headerlink" title="Pod 的调度管理"></a>Pod 的调度管理</h3><p>K8S Master 上的 Scheduler 服务负责实现 Pod 的调度管理，Pod  是静态的，只有真正被调度到具体的节点上才能发挥它的作用。K8S 根据不同的应用场景，定义了多种不同的调度策略。这些策略可以是根据算法自动完成的，也可以是人为指定的。具体可以看下面这张导图：</p><center><img src="/images/k8s/pod_scheduler.png" alt=""></center><p>笼统来看，有时候为了权衡应用场景和集群资源的需求，需要对 Pod 进行扩容和缩容，这同样属于 Pod 调度管理的范畴。</p><h3 id="Pod-的存储管理"><a href="#Pod-的存储管理" class="headerlink" title="Pod 的存储管理"></a>Pod 的存储管理</h3><p>Pod 和容器的数据存储使用 Volume，K8S Volume 和 Docker 的 Volume 是一样的原理，都是文件系统上的一个目录，只不过在 K8S 中实现了更多的 backend driver。包括 emptyDir、hostPath、GCE Persistent Disk、NFS、Ceph 等。</p><center><img src="/images/k8s/pod_volume.png" alt=""></center><p>Volume 提供了对各种 driver 的抽象，容器在使用 Volume 读写数据的时候不需要关心底层具体存储方式的实现，对它来说，所有类型的 Volume 都是一个目录。</p><p>当 Volume 被挂载到 Pod 中时，这个 Pod 中的容器都会共享这个 Volume，当其中的容器销毁时，Volume 中的数据也不会丢失，当 Pod 销毁时，根据不同的 driver 实现，数据也可以保存下来。</p><p>Volume 提高了 Pod 内数据的持久化管理，延长了 Pod 和容器的生命周期。</p><h3 id="Pod-的网络管理"><a href="#Pod-的网络管理" class="headerlink" title="Pod 的网络管理"></a>Pod 的网络管理</h3><p>在 K8S 中，定义了多种资源对象，很多对象本身就是一个通信的实体，比如容器、Pod、Service、Node。</p><p>K8S 维护这多种对象之间的通信关系，比如：Pod 内容器之间的通信、Pod 与容器之间的通信、Pod 之间的通信、Pod 与 Service 之间的通信，以及外部的访问。</p><p>这些通信机制的建立离不开 K8S 建立的完善的网络模型。K8S 使用了 CNI（容器网络规范）来标准化、归一化网络模型。</p><p>第三方的厂商或开发者可以根据自身网络需求，遵从 CNI 的规范，实现各种网络方案，并以插件的形式提供给 K8S 使用。目前比较知名的网络方案有：flannel、calico、weave、canal 等。</p><center><img src="/images/k8s/pod_net.png" alt=""></center><p>这些网络方案各有千秋、虽然实现方式各有区别，但殊途同归，最终都是满足 K8S 中各种实体间的通信需求。</p><p>OK，本文就到这里，我们通过两篇文章大致梳理了豌豆荚从出生到死亡要面临的多种人生的关卡。跨过去了，就成熟了，希望我们都能跨过自己人生的关卡。</p><p>下文我们开始进入实践的部分。</p><p>为了给大家更多的福利，这个系列的每一篇文章我都会送一些电子书，可能有重的，也有一些新书，之前送了《K8S 指南》和《容器与容器云》，这次送一本新书《》，大家有需要的后台回复“K8S2”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/9.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学习 K8S，这是系列第 7 篇。&lt;/p&gt;
&lt;p&gt;上篇我们简单学习了 Pod 的基础知识，本篇开始讲述一些 Pod 的高阶知识（本文只做理论的简单阐述，后面会针对每个点进行实践）。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 06 豌豆荚之旅（一）</title>
    <link href="https://chambai.github.io/2018/09/09/tech/Kubernetes_%E7%AC%94%E8%AE%B0_06_%E8%B1%8C%E8%B1%86%E8%8D%9A%E4%B9%8B%E6%97%85%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://chambai.github.io/2018/09/09/tech/Kubernetes_笔记_06_豌豆荚之旅（一）/</id>
    <published>2018-09-09T05:16:14.000Z</published>
    <updated>2019-04-11T14:38:24.073Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/8.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学习 K8S，这是系列第 6 篇。</p><p>Pod 中文译为豌豆荚，很形象，豌豆荚里面包裹的多颗小豌豆就是容器，小豌豆和亲密无间的老伙计壳荚子自出生之日起就得面对各种各样的人生大事：  </p><a id="more"></a><ul><li>容器、Pod、Node 之间的关系</li><li>Pod 的生命周期管理</li><li>Pod 的调度管理</li><li>Pod 的监控</li><li>Pod 的升级与回滚</li><li>Pod 的扩容与缩容</li><li>Pod 的存储管理</li><li>Pod 的网络管理</li><li>……</li></ul><h3 id="为什么需要-Pod？"><a href="#为什么需要-Pod？" class="headerlink" title="为什么需要 Pod？"></a>为什么需要 Pod？</h3><p>我们假设没有 Pod，应用部署的最小单元就是容器，会有什么问题？首先，应用调度粒度太细，不便于管理。想象一下淘宝网站运行着海量应用，每个应用又拆分成多个服务，每个服务部署在一个容器里，一个集群管理系统要管理庞大的容器集群，既要顾忌不同应用之间的隔离性，又要考虑相同应用之间的关联性，这在管理上将会是灾难性的难题。</p><p>其次，资源利用率低。有很多应用之间存在某种强关联关系，它们需要彼此能共享对方的资源，双方的交互需要快捷有效，如果把它们部署到单独的容器中，资源利用和通信将成为最主要的系统瓶颈。</p><p>Pod 的提出改变了这种局面，它将强关联的应用整合在一起，作为一个整体对外提供服务，既简化了管理的难度，又提高了资源的利用率。</p><p>那哪些应用是强关联，适合放到一个 Pod 中呢？举个例子，比如下面这个 Pod 包含两个容器，一个 File Puller，一个是 Web Server。</p><center><img src="/images/k8s/pod.png" alt=""></center><p>File Puller 会定期从外部的 Content Manager 中拉取最新的文件，将其存放在 Volume 中。然后 Web Server 从 Volume 中读取文件，来响应 Consumer 的请求。</p><p>这两个容器通过 Volume 来共享实时的数据，协作处理一个 Consumer 的请求，把它们放到同一个 Pod 中就是合适的。</p><p>如果有应用和任何应用之间都不存在联系，那么它们就单独部署在一个 Pod 中，称为<code>one-container-per-pod</code>。即便只有一个容器，K8S 管理的也是 Pod 而不是直接管理容器。</p><p>综上，Pod 在设计的时候，主要动机有以下两点：</p><ol><li>方便管理</li></ol><p>Pod 提供了比容器更高一层的抽象，K8S以 Pod 为最小单元进行应用的部署、调度、扩展、共享资源和管理周期。</p><ol start="2"><li>资源共享和通信</li></ol><p>Pod 内的所有容器共享同一个网络空间，它们之间可以通过 <code>localhost</code> 相互通信。同样，所有容器共享 Volume，一个容器挂载一个 Volume，其余容器都可以访问这个 Volume。</p><h3 id="容器、Pod、Node-之间的关系"><a href="#容器、Pod、Node-之间的关系" class="headerlink" title="容器、Pod、Node 之间的关系"></a>容器、Pod、Node 之间的关系</h3><p>容器是 Pod 的一个属性，定义了应用的类型及共享的资源。每个容器会分配一个 Port，Pod 内的容器通过 localhost:Port 的形式来通信。</p><p>一个 Pod 包含一个或多个容器，每个 Pod 会分配一个唯一的 IP 地址，Pod 内的多个容器共享这个 IP 地址，每个容器的 Port 加上 Pod IP 共同组成一个 <code>Endpoint</code>，共同对外提供服务。</p><p>在部署应用的时候，Pod 会被 Master 作为一个整体调度到一个 Node 上。如果开启多副本管理，则多个 Pod 会根据调度策略调度到不同的 Node 上。如果 Node 宕机，则该 Node 上的所有 Pod 会被自动调度到其他 Node 上。</p><p>下面是容器、Pod、Node 三者之间的关系图：  </p><center><img src="/images/k8s/pod_docker_node.png" alt=""></center><h3 id="Pod-根容器"><a href="#Pod-根容器" class="headerlink" title="Pod 根容器"></a>Pod 根容器</h3><p>Pod 中有一个特殊的容器，叫 Pod 的根容器——Pause 容器，这是一个很小的容器，镜像大小大概为 200KB。</p><center><img src="/images/k8s/pod_pause.png" alt=""></center><p>Pause 容器存在的意义是: <strong>维护 Pod 的状态信息</strong>。</p><p>由于 Pod 是作为一个整体进行调度，我们难以对这个整体的信息进行简单的判断和有效地进行行动。</p><p>想象一下，假如 Pod 内一个容器死亡了，是算整体死亡呢还是 N/M 死亡率，如果 Pod 内所有容器都死亡了，那是不是该 Pod 也就死亡了，如果加入新的容器或原有容器故障恢复呢，如何让新成员快速融入环境？</p><p>理论上，虽然 Pod 是由一组容器组成的，但 Pod 和容器是彼此独立的，也就是容器的故障不应该影响 Pod 的存在，Pod 有相应的手段来保证容器的健康状况。</p><p>引入与业务无关的，并且不易死亡的 Pause 容器就可以很好的解决这个问题，Pause 容器的状态就代表了 Pod 的状态，只要 Pause 不死，那么不管应用容器发生什么变化，Pod 的状态信息都不会改变。</p><p>这样，Pod 内的多个应用容器共享 Pause 容器的 IP 和 Volume，当加入新的容器或者原有的容器因故障重启后就可以根据 Pause 保存的状态快速学习到当前 Pod 的状态。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文简单学习了 Pod 的初级知识，包括 Pod 的设计动机，容器、Pod 和 Node 之间的关系，以及 Pod 的守护者——Pause 容器。</p><p>容器的 Port + Pod IP = Endpoint，构成一个 Pod 的通信实体，Pod 中的容器共享网络和存储，这些共享信息是由 Pause 容器来维护的。</p><p>下文继续豌豆荚之旅的第二个部分，学习 Pod 的管理哲学。</p><p>为了给大家更多的福利，这个系列的每一篇文章我都会送一些电子书，可能有重的，也有一些新书，之前送了《K8S 指南》和《容器与容器云》，这次送一本由 K8S 中文社区主编的《K8S 中文手册》，大家有需要的后台回复“K8S2”</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/8.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学习 K8S，这是系列第 6 篇。&lt;/p&gt;
&lt;p&gt;Pod 中文译为豌豆荚，很形象，豌豆荚里面包裹的多颗小豌豆就是容器，小豌豆和亲密无间的老伙计壳荚子自出生之日起就得面对各种各样的人生大事：  &lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 05 使用产品前请先阅读说明书</title>
    <link href="https://chambai.github.io/2018/09/07/tech/Kubernetes_%E7%AC%94%E8%AE%B0_05_%E4%BD%BF%E7%94%A8%E4%BA%A7%E5%93%81%E5%89%8D%E8%AF%B7%E5%85%88%E9%98%85%E8%AF%BB%E8%AF%B4%E6%98%8E%E4%B9%A6/"/>
    <id>https://chambai.github.io/2018/09/07/tech/Kubernetes_笔记_05_使用产品前请先阅读说明书/</id>
    <published>2018-09-07T05:16:14.000Z</published>
    <updated>2019-04-11T14:38:24.078Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/7.png" alt=""></center><hr><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 5 篇。</p><p>生活中，随处可见，几乎每一款产品都会附带一份说明书，说明书可以记录产品的使用方法，也可以记录产品的配方。有了说明书，我们完全可以窥探一款产品的全貌。</p><a id="more"></a><p>在 K8S 中，<code>yaml</code> 配置文件就是 K8S 资源对象的说明书，定义了对象包含的元素及采取的动作，每种对象都可以通过 yaml 配置文件来创建。</p><center><img src="/images/k8s/yaml.jpg" alt=""></center><h3 id="yaml-是什么"><a href="#yaml-是什么" class="headerlink" title="yaml 是什么"></a>yaml 是什么</h3><p>yaml 是一种用来写配置文件的语言，没错，它是一门语言。如果你用过 json，那么对它就不会陌生，yaml 又被称为是 json 的超集，使用起来比 json 更方便。</p><p>结构上它有两种可选的类型：Lists 和 Maps。</p><p>List 用 -（破折号） 来定义每一项，Map 则是一个 key:value 的键值对来表示。如下是一个 json 文件到 yaml 文件的转换：  </p><p>json:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">    &quot;kind&quot;: &quot;Pod&quot;,</span><br><span class="line">    &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;xx&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;containers&quot;: [&#123;</span><br><span class="line">            &quot;name&quot;: &quot;front-end&quot;,</span><br><span class="line">            &quot;images&quot;: &quot;nginx&quot;,</span><br><span class="line">            &quot;ports&quot;: [&#123;</span><br><span class="line">                &quot;containerPort&quot;: &quot;80&quot;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;, &#123;</span><br><span class="line">            &quot;name&quot;: &quot;flaskapp-demo&quot;,</span><br><span class="line">            &quot;images&quot;: &quot;jcdemo/flaskapp&quot;,</span><br><span class="line">            &quot;ports&quot;: [&#123;</span><br><span class="line">                &quot;containerPort&quot;: &quot;5000&quot;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>yaml:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">    name: xx</span><br><span class="line">spec:</span><br><span class="line">    containers:</span><br><span class="line">        - name: front-end</span><br><span class="line">        images: nginx</span><br><span class="line">        ports:</span><br><span class="line">            - containerPort: 80</span><br><span class="line">    - name: flaskapp-demo</span><br><span class="line">        images: jcdemo/flaskapp</span><br><span class="line">        ports: 8080</span><br></pre></td></tr></table></figure></p><p>这个文件简单地定义了一个 Pod 对象，包含两个容器，我们可以很清晰地看到两者是如何转换的。</p><h3 id="K8S-创建资源的两种方式"><a href="#K8S-创建资源的两种方式" class="headerlink" title="K8S 创建资源的两种方式"></a>K8S 创建资源的两种方式</h3><p>在 K8S 中，有两种创建资源的方式：kubectl 命令和 yaml 配置文件。</p><p>两种方式各有各的好处。命令行的方式最为简单，一条命令就万事大吉，但缺点也很明显，你并不知道这条命令背后到底做了哪些事，配置文件就提供了一种让你知其然更知其所以然的方式。总的来说，它有以下好处：   </p><ul><li>完整性：配置文件描述了一个资源的完整状态，可以很清楚地知道一个资源的创建背后究竟做了哪些事；</li><li>灵活性：配置文件可以创建比命令行更复杂的结构；</li><li>可维护性：配置文件提供了创建资源对象的模板，能够重复使用；</li><li>可扩展性：适合跨环境、规模化的部署。</li><li>……</li></ul><p>当然，复杂的东西对用户就难以做到友好，我们需要熟悉它的配置文件的语法，有一定难度。下面举几个例子，让你对 yaml 配置文件有一个基本的认识。</p><h3 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h3><p>下面，我们分别来看看 <code>deployment</code>、<code>pod</code>、<code>service</code> 这三种资源的说明书都长啥样。</p><p>由于 K8S 对每种资源的定义非常庞杂，限于篇幅，我们只看一些必选的参数，目的是通过这几个例子，读懂 yaml 配置文件。</p><h4 id="deployment"><a href="#deployment" class="headerlink" title="deployment"></a>deployment</h4><p>定义 deployment 配置文件，命名为：nginx-deployment.yaml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1   # 1.9.0 之前的版本使用 apps/v1beta2，可通过命令 kubectl api-versions 查看</span><br><span class="line">kind: Deployment     #指定创建资源的角色/类型</span><br><span class="line">metadata:      #资源的元数据/属性</span><br><span class="line">    name: nginx-deployment    #资源的名字，在同一个namespace中必须唯一</span><br><span class="line">spec:</span><br><span class="line">    replicas: 2      #副本数量2</span><br><span class="line">    selector:      #定义标签选择器</span><br><span class="line">        matchLabels:</span><br><span class="line">            app: web-server</span><br><span class="line">    template:      #这里Pod的定义</span><br><span class="line">        metadata:</span><br><span class="line">            labels:      #Pod的label</span><br><span class="line">                app: web-server</span><br><span class="line">    spec:         # 指定该资源的内容  </span><br><span class="line">        containers:  </span><br><span class="line">        - name: nginx      #容器的名字  </span><br><span class="line">          images: nginx:1.12.1  #容器的镜像地址    </span><br><span class="line">          ports:  </span><br><span class="line">          - containerPort: 80  #容器对外的端口</span><br></pre></td></tr></table></figure></p><p>执行<code>kubectl create -f nginx.yaml</code>创建 deployment 资源：</p><center><img src="/images/k8s/deploy-yaml.png" alt=""></center><h4 id="pod"><a href="#pod" class="headerlink" title="pod"></a>pod</h4><p>定义 pod 配置文件，命名为 redis-pod.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod  </span><br><span class="line">metadata:  </span><br><span class="line">    name: pod-redis</span><br><span class="line">    labels:</span><br><span class="line">        name: redis</span><br><span class="line">spec: </span><br><span class="line">    containers:</span><br><span class="line">    - name: pod-redis</span><br><span class="line">        images: docker.io/redis  </span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80    #容器对外的端口</span><br></pre></td></tr></table></figure><p>执行<code>kubectl create -f pod-redis.yaml</code>创建 pod 资源：  </p><center><img src="/images/k8s/pod-yaml.png" alt=""></center><p>可以看到，成功创建一个 Pod，<code>ContainerCreating</code>表示 Pod 中的容器正在执行镜像的下载和安装过程，过一会儿，就显示<code>Running</code>了，表明 Pod 应用部署完成。</p><h4 id="service"><a href="#service" class="headerlink" title="service"></a>service</h4><p>定义 service 配置文件，命名为 httpd-svc.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1  </span><br><span class="line">kind: Service  # 指明资源类型是 service</span><br><span class="line">metadata:  </span><br><span class="line">    name: httpd-svc # service 的名字是 httpd-svc</span><br><span class="line">    labels:  </span><br><span class="line">        name: httpd-svc </span><br><span class="line">spec:  </span><br><span class="line">    ports:  # 将 service 8080 端口映射到 pod 的 80 端口，使用 TCP 协议</span><br><span class="line">    - port: 8080</span><br><span class="line">        targetPort: 80  </span><br><span class="line">        protocol: TCP  </span><br><span class="line">    selector:  </span><br><span class="line">        run: httpd # 指明哪些 label 的 pod 作为 service 的后端</span><br></pre></td></tr></table></figure><p>执行<code>kubectl create -f httpd-svc.yaml</code>创建 service 资源：  </p><center><img src="/images/k8s/svc-yaml.png" alt=""></center><p>可以看到，service httpd-svc 分配到一个 Cluster-IP 10.96.0.1，我们可以通过该 IP 访问 service 所维护的后端 Pod。</p><p>另外，还有一个 service kubernetes，这个是 Kubernetes API Server 的 service，Cluster 内部的各组件就是通过这个 service 来访问 API Server。 </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>yaml 是 K8S 资源对象的说明书，每个对象拥有哪些属性都可以在 yaml 中找到详尽的说明，初学者建议多写 yaml 文件，少用命令行。</p><p>以上三个例子只是对 yaml 文件做个简单说明，更详细的信息还是参考官网。</p><p>OK，本文就到此为止，下文我们开始进入豌豆荚之旅。觉得不错，别忘了转发分享给你的朋友哦。</p><blockquote><p>参考：<br><a href="https://www.kubernetes.org.cn/1414.html" target="_blank" rel="noopener">https://www.kubernetes.org.cn/1414.html</a></p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/7.png&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;hr&gt;
&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 5 篇。&lt;/p&gt;
&lt;p&gt;生活中，随处可见，几乎每一款产品都会附带一份说明书，说明书可以记录产品的使用方法，也可以记录产品的配方。有了说明书，我们完全可以窥探一款产品的全貌。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 04 架构是个好东西</title>
    <link href="https://chambai.github.io/2018/09/05/tech/Kubernetes_%E7%AC%94%E8%AE%B0_04_%E6%9E%B6%E6%9E%84%E6%98%AF%E4%B8%AA%E5%A5%BD%E4%B8%9C%E8%A5%BF/"/>
    <id>https://chambai.github.io/2018/09/05/tech/Kubernetes_笔记_04_架构是个好东西/</id>
    <published>2018-09-05T05:16:14.000Z</published>
    <updated>2019-04-11T14:38:24.077Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/6.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 4 篇。</p><p>任何技术的诞生，都会经历从架构设计到开发测试的过程，好的技术，往往也会有一套好的架构。架构是个好东西，它能帮助我们站在高处看清楚事物的整体结构，避免过早地进入细节而迷失方向。</p><a id="more"></a><p>上篇文章扫清了 K8S 的一些基本概念，今天这篇文章我们就来看看 K8S 的架构。</p><p>先上图：</p><center><img src="/images/k8s/architecture.png" alt=""></center><p>图中包括两种类型的节点：Master 和 Node，每个节点上运行着多种 K8S 服务。</p><h3 id="Master-节点"><a href="#Master-节点" class="headerlink" title="Master 节点"></a>Master 节点</h3><p>Master 是 K8S 集群的大脑，运行着如下 Daemon 服务：kube-apiserver、kube-controller-manager、kube-scheduler、etcd 等。</p><h4 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h4><p>如果把 K8S 环境看作是一个公司，那 API Server 就是这个公司的基础平台部，是公司最为核心的技术能力输出出口。它对外提供 HTTP/HTTPS REST API，统称 K8S API，可以供各类客户端工具（CLI 或 WebUI）、K8S 其他组件，以及第三方的平台接入。对内提供了 K8S 各类资源（如 Pod、Deployment、Service等）的增删改查和监控等操作，是集群内各个功能模块之间数据交互和通信的中心枢纽。</p><h4 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h4><p>Controller Manager 更像是公司的人力资源部，负责统筹公司的人员分布。它管理着 K8S 各类资源的生命周期，保证资源处于预期状态，如果现有状态和预期状态不符，它会自动化执行修正。</p><p>Controller Manager 由多种 Controller 组成，包括 Replication Controller、Node Controller、ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Service Controller、Token Controller 及 Endpoint Controller 等。每种 Controller 都负责一种具体的资源管控流程，而 Controller Manager 正是这些 Controller 的核心管理者。</p><h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>Scheduler 则像是公司各个部门的项目经理之类的角色，负责将具体的人力放到他们擅长的位置上，知人善用。具体来说，Scheduler 负责将待调度的 Pod 对象按照特定的调度策略绑定到集群中某个合适的节点上，调度策略会综合考虑集群的拓扑结构、节点的负载情况、以及应用对高可用、性能、数据亲和性的需求。</p><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>etcd 是一个高可用的分布式数据库，负责保存 K8S 的配置信息和各种资源的状态信息。当数据发生变化时，etcd 会及时告知集群中的其他组件。</p><h4 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h4><p>kubectl 是 K8S 的 CLI 工具，这是使用 K8S API 建立的一套命令行工具，使用它，可以非常方便地管理 K8S 集群。</p><h3 id="Node-节点"><a href="#Node-节点" class="headerlink" title="Node 节点"></a>Node 节点</h3><p>Node 是 K8S 集群的具体执行者，也运行着多种服务，如：kubelet、kube-proxy、container runtime、Pod 网络等。Node 可以看作是 Master 的代理，负责处理 Master 下发到本节点的任务，管理 Pod 和 Pod 中的容器，定期向 Master 汇报自身资源的使用情况。</p><h4 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h4><p>kubelet 更像是部门中各个小组的 Leader，对外从 API Server 拿资源，对内负责小组内各种资源的管理，比如从 Master 拿到 Pod 的具体配置信息（images、Volume 等）之后，kubelet 根据这些信息创建和运行容器。</p><h4 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h4><p>kube-proxy 则像穿插在公司各个部门之间的接口人，对内和内部人员沟通，对外协调部门之间的各种事宜，展示部门风采。kube-proxy 作用于 Service，通过前面学习，我们知道 Service 是对一组 Pod 的抽象，统一对外提供某种服务，当外部访问 Service 时，实际上是需要访问 Service 所管辖的 Pod 中的容器，kube-proxy 在这里就是负责将访问 Service 的数据流转发到后端的容器，如果有多个副本，kube-proxy 会实现负载均衡。</p><h4 id="cAdvisor"><a href="#cAdvisor" class="headerlink" title="cAdvisor"></a>cAdvisor</h4><p>cAdvisor 对 Node 上的资源进行实时监控和性能数据采集，包括 CPU 使用情况、内存使用情况、网络吞吐量及文件系统使用情况等。cAdvisor 集成在 kubelet 中，当 kubelet 启动时会自动启动 cAdvisor，一个cAdvisor 仅对一台 Node 机器进行监控。</p><h4 id="container-runtime"><a href="#container-runtime" class="headerlink" title="container runtime"></a>container runtime</h4><p>container runtime 是真正运行容器的地方，为容器提供运行环境，主流的三种 container runtime 是 lxc、runc 和 rkt，K8S 都支持它们，但常用的事 runc，原因是 runc 是 Docker 默认的 runtime。在 K8S 的容器应用中，Docker 是主流。</p><p>OK，K8S 架构介绍就到此为止。</p><p>最后，还是继续送书，容器网络专家倪朋飞写的《K8S 指南》电子书，如有需要后台回复“K8S”（之前回复过就不用回复了）。如需加群学习回复“加群”。</p><p>下文我们开始对 K8S 的说明书一探究竟。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/6.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 4 篇。&lt;/p&gt;
&lt;p&gt;任何技术的诞生，都会经历从架构设计到开发测试的过程，好的技术，往往也会有一套好的架构。架构是个好东西，它能帮助我们站在高处看清楚事物的整体结构，避免过早地进入细节而迷失方向。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 笔记 03 扫清概念</title>
    <link href="https://chambai.github.io/2018/09/03/tech/Kubernetes_%E7%AC%94%E8%AE%B0_03_%E6%89%AB%E6%B8%85%E6%A6%82%E5%BF%B5/"/>
    <id>https://chambai.github.io/2018/09/03/tech/Kubernetes_笔记_03_扫清概念/</id>
    <published>2018-09-03T05:16:14.000Z</published>
    <updated>2019-04-11T14:38:24.058Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/5.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第三篇。</p><p>每一种技术，为了描述清楚它的设计理念，都会自定义一堆概念或术语。在进入一门技术的研究之前，我们有必要扫清它的基本概念。</p><a id="more"></a><h3 id="资源对象"><a href="#资源对象" class="headerlink" title="资源对象"></a>资源对象</h3><p>K8S 的操作实体，在 K8S 中，有很多的操作对象，比如容器、Pod、Deployment、Service、Node 等，我们统统称它们为资源对象。</p><h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p>K8S 集群，是计算、存储和网络资源的集合，K8S 基于这些资源来承载容器化的应用。</p><h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><p>K8S 集群的大脑，负责整个集群的管控、资源调度。可以部署在普通物理机或虚拟机上，为了实现高可用，可以部署多个 Master。</p><h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><p>K8S 集群的执行者，受 Master 指挥，负责运行和监控容器应用、管理容器的生命周期，并向 Master 定期汇报容器的状态。同样，Node 也可以部署在物理机或虚拟机之上，也可以部署多个。</p><h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><p>在 K8S 集群中，Pod 是资源调度的最小单位，一个 Pod 可以包含一个或多个容器应用，这些容器应用彼此之间存在某种强关联。Pod 内的所有容器应用共享计算、存储、网络资源。</p><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><p>Controller 是 K8S 中负责管理 Pod 的资源对象，它定义 Pod 的部署属性，比如有几个副本，副本异常怎么处理等，如果把 Pod 副本看做是一个公司职员，那么 Controller 就像是 HR，会不断根据人员的变动来招人满足公司的发展需求。</p><p>为了满足多种业务场景，K8S 提供了多种 Controller，包括 Deployment、ReplicaSet、DaemonSet、StatefulSet、Job 等。</p><h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><p>Deployment 是最常用的 Controller，定义了用户的期望场景，实现了 Pod 的多副本管理，如果运行过程中有一个副本挂了（员工离职），那么 K8S 会根据 Deployment 的定义重新拉起一个副本继续工作（招一个新员工），始终保证 Pod 按照用户期望的状态运行。</p><h4 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h4><p>ReplicaSet 和 Deployment 实现了同样的功能，确切的说是 Deployment 通过 ReplicaSet 来实现 Pod 的多副本管理。我们通常不需要直接使用 ReplicaSet。</p><h4 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h4><p>DaemonSet 用于每个 Node 最多只运行一个副本的场景，通常用于运行 Daemon。</p><h4 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h4><p>Job 用于运行结束就删除的应用，而其他 Controller 则是会长期保持运行。</p><h4 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h4><p>以上 Controller 都是无状态的，也就是说副本的状态信息会改变，比如当某个 Pod 副本异常重启时，其名称会改变。StatefulSet 提供有状态的服务，能够保证 Pod 的每个副本在其生命周期中名称保持不变。这是通过持久化的存储卷来实现的。</p><h3 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h3><p>Label 定义了其他资源对象所属的标签，类似于你在公司被分到 A 小组、B 小组。有了标签，就可以针对性地对每个小组进行管理。比如把某个小组搬到哪个办公区（把某个 Pod 部署到哪个 Node 上）。给指定的资源对象定义一个或多个不同的标签能够实现多维度的资源分组管理，方便进行资源分配、调度、配置、部署等管理工作。</p><h3 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h3><p>Label 选择器，K8S 通过 Selector 来过滤筛选指定的资源对象，类似于 SQL 语句中的 where 查询条件，Label 实现了简单又通用的对象查询机制。</p><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>在 K8S 中，Service 是对 Pod 对象的抽象，通常，Pod 会以多副本的形式部署，每个 Pod 都有自己的 IP，都可以对外提供服务，但 Pod 是脆弱的，也就是说，它随时都有可能被频繁地销毁和重启，IP 也会随之改变，这样，服务的访问就会出现问题。</p><p>Service 就是提出来解决这个问题的，它定义了一个虚拟 IP（也叫集群 IP），这个 IP 在 Service 的整个生命周期内都不会改变。当有访问到达时，Service 会将请求导向 Pod，如果存在多个 Pod，Service 还能实现负载均衡。</p><h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><p>K8S 的存储卷，定义了一个 Pod 中多个容器可访问的共享目录。和 Docker 的 Volume 不太一样的是，K8S 的 Volume 是以 Pod 为单位的，也就是 Volume 的生命周期和 Pod 相关，和 Pod 内的容器不相关，即使容器终止或重启，Volume 中的数据也不会丢失，只有当 Pod 被删除时，数据才会丢失。</p><h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><p>当有多个用户或租户使用同一个 K8S 集群时，如何区分它们创建的资源呢？答案就是 Namespace。</p><p>Namespace 将一个物理的集群从逻辑上划分成多个虚拟的集群，每个集群就是一个 Namespace，不同 Namespace 里的资源是完全隔离的。每个用户在自己创建的 Namespace 里操作，都不会影响到其他用户。</p><h3 id="Annotation"><a href="#Annotation" class="headerlink" title="Annotation"></a>Annotation</h3><p>Annotation 与 Label 类似，但和 Label 不同 的事，Annotation 不用于过滤筛选，它只是用户定义的某一种资源的附加信息，目的是方便外部查找该资源。有点类似于我们常说的别名，没有它完全可以，但有了它可以很方便查找。</p><p>最后，还是继续送书，容器网络专家倪朋飞写的《K8S 指南》电子书，如有需要后台回复“K8S”（之前回复过就不用回复了）。如需加群学习回复“加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;center&gt;&lt;img src=&quot;/images/k8s/5.jpg&quot; alt=&quot;&quot;&gt;&lt;/center&gt;

&lt;p&gt;Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第三篇。&lt;/p&gt;
&lt;p&gt;每一种技术，为了描述清楚它的设计理念，都会自定义一堆概念或术语。在进入一门技术的研究之前，我们有必要扫清它的基本概念。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/categories/Kubernetes/"/>
    
    
      <category term="云计算" scheme="https://chambai.github.io/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kubernetes" scheme="https://chambai.github.io/tags/Kubernetes/"/>
    
  </entry>
  
</feed>
