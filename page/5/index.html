<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>小白的网志空间</title>
  
  
  <meta name="description" content="专注于云计算/网络/CC++/Python/Go等技术栈，分享读书、写作、思维、认知等话题">
  

  <link rel="alternate" href="/atom.xml" title="小白的网志空间">

  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  
  
  <meta name="theme-color" content="#f24e32">
  
  <meta name="msapplication-TileColor" content="#f24e32">
  
  <meta name="msapplication-config" content="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/browserconfig.xml">
  
  
  <!-- link -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.6.3/css/all.min.css">

  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-fonts@master/Ubuntu/Ubuntu-Regular.ttf">
  
  <link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicon.ico" type="image/x-icon">
  
  <link rel="icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/favicon-32x32.png" type="image/x-icon" sizes="32x32">
  
  <link rel="apple-touch-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/apple-touch-icon.png" type="image/png" sizes="180x180">
  
  <link rel="mask-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/safari-pinned-tab.svg" color="#f24e32">
  
  <link rel="manifest" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/site.webmanifest">
  
  

  
  <link rel="stylesheet" href="/style.css">
  

  



  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
</head>

<body>
    <div id="loading-bar-wrapper">
  <div id="loading-bar" class="pure"></div>
</div>

    <script>setLoadingBarProgress(20)</script>
    <header class="l_header pure">
	<div class="wrapper">
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href="/">
        
          小白的网志空间
        
      </a>
			<div class="menu">
				<ul class="h-list">
          
  					
  						<li>
								<a id="home" class="nav flat-box" href="/">
									<i class="fas fa-home fa-fw"></i>&nbsp;主页
								</a>
							</li>
      			
  						<li>
								<a id="archives" class="nav flat-box" href="/archives/">
									<i class="fas fa-archive fa-fw"></i>&nbsp;归档
								</a>
							</li>
      			
  						<li>
								<a id="friends" class="nav flat-box" href="/friends/">
									<i class="fas fa-users fa-fw"></i>&nbsp;导航
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索">
						<span class="icon"><i class="fas fa-search fa-fw"></i></span>
					</form>
				</div>
			
			<ul class="switcher h-list">
				
					<li class="s-search"><a class="fas fa-search fa-fw" href="javascript:void(0)"></a></li>
				
				<li class="s-menu"><a class="fas fa-bars fa-fw" href="javascript:void(0)"></a></li>
			</ul>
		</div>

		<div class="nav-sub container container--flex">
			<a class="logo flat-box"></a>
			<ul class="switcher h-list">
				<li class="s-comment"><a class="flat-btn fas fa-comments fa-fw" href="javascript:void(0)"></a></li>
				<li class="s-toc"><a class="flat-btn fas fa-list fa-fw" href="javascript:void(0)"></a></li>
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu">
      <ul>
          
              
                  <li>
										<a id="home" class="nav flat-box" href="/">
											<i class="fas fa-home fa-fw"></i>&nbsp;主页
										</a>
                  </li>
              
                  <li>
										<a id="archives" class="nav flat-box" href="/archives/">
											<i class="fas fa-archive fa-fw"></i>&nbsp;归档
										</a>
                  </li>
              
                  <li>
										<a id="friends" class="nav flat-box" href="/friends/">
											<i class="fas fa-users fa-fw"></i>&nbsp;导航
										</a>
                  </li>
              
       
      </ul>
		</nav>
    </header>
	</aside>

    <script>setLoadingBarProgress(40);</script>
    <div class="l_body">
    <div class='container clearfix'>
        <div class='l_main'>
            

<section class="post-list">
    
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/01/12/云计算/Linux_网络体系结构全景分析/">
              
                  Linux 网络体系结构全景分析
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-01-12
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Linux/">Linux</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>Linux 是一个非常庞大的系统结构，模块众多，各个模块分工合作，共同运作着各自的任务，为 Linux 这个大家庭贡献着自己的力量。</p>
<h3 id="Linux-网络子系统"><a href="#Linux-网络子系统" class="headerlink" title="Linux 网络子系统"></a>Linux 网络子系统</h3><hr>
<p>其中，网络子系统是一个较为复杂的模块，如下图是 Linux 网络子系统的体系结构，自顶向下，可以分应用层、插口层（或协议无关层）、协议层和接口层（或设备驱动层），这个层级关系也分别对应着我们所熟悉的 OSI 七层模型（物理层、数据链路层、网络层、传输层、会话层、表示层和应用层）。</p>
<center><img src="/images/linux/linux_net_level.jpg" alt=""></center>

<p>Linux 所有用户态的应用程序要访问链接内核都要依赖系统调用，这是内核提供给用户态的调用接口，这通常是由标准的 C 库函数来实现的。对于网络应用程序，内核提供了一套通用的 socket 系统调用的接口。</p>
<p>插口层，也叫协议无关层，它屏蔽了协议相关的操作，不论是什么协议（UDP，TCP），都提供一组通用的函数接口，也就是 socket 的实现。</p>
<p>协议层，用于实现各种具体的网络协议族，包括 TCP/IP，OSI 和 Unix 域的实现，每个协议族都包含自己的内部结构，例如，对于 TCP/IP 协议族，IP 是最底层，TCP 和 UDP 层在 IP 层的上面。</p>
<p>接口层，包括了通用设备接口，和网络设备通信的设备驱动程序，其中，包串口使用的 SLIP 驱动程序以及以太网使用的以太网驱动程序，以及环回口使用的都是这一层的设备。它对上提供了协议与设备驱动通信的通用接口，对下也提供了一组通用函数供底层网络设备驱动程序使用。具体的细节后面再开文章详细讲述。</p>
<p>以上是从分层的体系结构来看，下面从数据传输的角度来说说，一个数据包是如何依赖这些层次关系被发送/接收的。</p>
<p>首先，数据的发送过程，应用层组织好待发送的数据包，执行系统调用进入内核协议栈，按照层次关系分别进行包头的封装，如到达传输层，封装 TCP/UDP 的包头，进入网络层封装 IP 包头，进入接口层封装 MAC 帧，最后借助驱动将数据包从网卡发送出去。</p>
<p>然后，数据的接收过程正好与发包过程相反，是一个拆包的过程。接口层借助网卡 DMA 硬件中断感知数据包的到来，拆解包头，识别数据，借助软中断机制告知上层进行相应的收包处理，最终用户态执行系统调用接收数据包。</p>
<center><img src="/images/linux/linux_net_flow.jpg" alt=""></center>

<p>继续细化这个过程，其内部在实现上都是通过具体的数据结构来完成每一层的过渡的，譬如说，应用层和内核之间通过 struct sock 结构实现协议无关的接口调用进行交互，传输层提供 struct proto 的结构来支持多种协议，网络层通过 struct sk_buff 来传递数据，接口层定义 struct net_device 来完成和驱动程序的交互。</p>
<center><img src="/images/linux/linux_net_struct.jpg" alt=""></center>

<p>Linux 网络体系结构还是比较博大精深的，尤其是这套自顶向下的分层设计方式对很多技术都有借鉴意义。更具体的函数调用和数据收发过程，后面的文章再进行讲述，敬请期待。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr>
<p>Linux 网络子系统的分层模型，数据收发过程，以及核心数据结构。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/Linux/"><i class="fas fa-hashtag fa-fw"></i>Linux</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/01/05/云计算/Linux_下几种零拷贝的方法/">
              
                  Linux 下几种零拷贝的方法
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-01-05
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Linux/">Linux</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>本文讲解 Linux 的零拷贝技术，我在「云计算技能图谱」中说过，云计算是一门很庞大的技术学科，融合了很多技术，Linux 算是比较基础的技术，所以，学好 Linux 对于云计算的学习会有比较大的帮助。</p>
<p>本文借鉴并总结了几种比较常见的 Linux 下的零拷贝技术，相关的引用链接见文后，大家如果觉得本文总结得太抽象，可以转到链接看详细解释。</p>
<h3 id="为什么需要零拷贝"><a href="#为什么需要零拷贝" class="headerlink" title="为什么需要零拷贝"></a>为什么需要零拷贝</h3><hr>
<p>传统的 Linux 系统的标准 I/O 接口（read、write）是基于数据拷贝的，也就是数据都是 copy_to_user 或者 copy_from_user，这样做的好处是，通过中间缓存的机制，减少磁盘 I/O 的操作，但是坏处也很明显，大量数据的拷贝，用户态和内核态的频繁切换，会消耗大量的 CPU 资源，严重影响数据传输的性能，有数据表明，在Linux内核协议栈中，这个拷贝的耗时甚至占到了数据包整个处理流程的57.1%。</p>
<h3 id="什么是零拷贝"><a href="#什么是零拷贝" class="headerlink" title="什么是零拷贝"></a>什么是零拷贝</h3><hr>
<p>零拷贝就是这个问题的一个解决方案，通过尽量避免拷贝操作来缓解 CPU 的压力。Linux 下常见的零拷贝技术可以分为两大类：一是针对特定场景，去掉不必要的拷贝；二是去优化整个拷贝的过程。由此看来，零拷贝并没有真正做到“0”拷贝，它更多是一种思想，很多的零拷贝技术都是基于这个思想去做的优化。</p>
<center><img src="/imagessss/linux/copy_type.jpg" alt=""></center>

<h3 id="零拷贝的几种方法"><a href="#零拷贝的几种方法" class="headerlink" title="零拷贝的几种方法"></a>零拷贝的几种方法</h3><hr>
<h4 id="原始数据拷贝操作"><a href="#原始数据拷贝操作" class="headerlink" title="原始数据拷贝操作"></a>原始数据拷贝操作</h4><hr>
<p>在介绍之前，先看看 Linux 原始的数据拷贝操作是怎样的。如下图，假如一个应用需要从某个磁盘文件中读取内容通过网络发出去，像这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while((n = read(diskfd, buf, BUF_SIZE)) &gt; 0)</span><br><span class="line"></span><br><span class="line">write(sockfd, buf , n);</span><br></pre></td></tr></table></figure>
<p>那么整个过程就需要经历：1）read 将数据从磁盘文件通过 DMA 等方式拷贝到内核开辟的缓冲区；2）数据从内核缓冲区复制到用户态缓冲区；3）write 将数据从用户态缓冲区复制到内核协议栈开辟的 socket 缓冲区；4）数据从 socket 缓冲区通过 DMA 拷贝到网卡上发出去。</p>
<center><img src="/imagessss/linux/copy_tran.png" alt=""></center>

<p>可见，整个过程发生了至少四次数据拷贝，其中两次是 DMA 与硬件通讯来完成，CPU 不直接参与，去掉这两次，仍然有两次 CPU 数据拷贝操作。</p>
<h4 id="方法一：用户态直接-I-O"><a href="#方法一：用户态直接-I-O" class="headerlink" title="方法一：用户态直接 I/O"></a>方法一：用户态直接 I/O</h4><hr>
<p>这种方法可以使应用程序或者运行在用户态下的库函数直接访问硬件设备，数据直接跨过内核进行传输，内核在整个数据传输过程除了会进行必要的虚拟存储配置工作之外，不参与其他任何工作，这种方式能够直接绕过内核，极大提高了性能。</p>
<center><img src="/imagessss/linux/copy_dirtio.jpg" alt=""></center>

<p><strong>缺陷：</strong></p>
<p>1）这种方法只能适用于那些不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。</p>
<p>2）这种方法直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成资源的浪费，解决这个问题需要和异步 I/O 结合使用。</p>
<h4 id="方法二：mmap"><a href="#方法二：mmap" class="headerlink" title="方法二：mmap"></a>方法二：mmap</h4><hr>
<p>这种方法，使用 mmap 来代替 read，可以减少一次拷贝操作，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">buf = mmap(diskfd, len);</span><br><span class="line"></span><br><span class="line">write(sockfd, buf, len);</span><br></pre></td></tr></table></figure>
<p>应用程序调用 mmap ，磁盘文件中的数据通过 DMA 拷贝到内核缓冲区，接着操作系统会将这个缓冲区与应用程序共享，这样就不用往用户空间拷贝。应用程序调用write ，操作系统直接将数据从内核缓冲区拷贝到 socket 缓冲区，最后再通过 DMA 拷贝到网卡发出去。</p>
<center><img src="/imagessss/linux/copy_mmap.png" alt=""></center>

<p><strong>缺陷：</strong> </p>
<p>1）mmap 隐藏着一个陷阱，当 mmap 一个文件时，如果这个文件被另一个进程所截获，那么 write 系统调用会因为访问非法地址被 SIGBUS 信号终止，SIGBUS 默认会杀死进程并产生一个 coredump，如果服务器被这样终止了，那损失就可能不小了。</p>
<p>解决这个问题通常使用文件的租借锁：首先为文件申请一个租借锁，当其他进程想要截断这个文件时，内核会发送一个实时的 RT_SIGNAL_LEASE 信号，告诉当前进程有进程在试图破坏文件，这样 write 在被 SIGBUS 杀死之前，会被中断，返回已经写入的字节数，并设置 errno 为 success。</p>
<p>通常的做法是在 mmap 之前加锁，操作完之后解锁：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) == -1) &#123;</span><br><span class="line"></span><br><span class="line">perror(&quot;kernel lease set signal&quot;);</span><br><span class="line"></span><br><span class="line">return -1;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* l_type can be F_RDLCK F_WRLCK  加锁*/</span><br><span class="line"></span><br><span class="line">/* l_type can be  F_UNLCK 解锁*/</span><br><span class="line"></span><br><span class="line">if(fcntl(diskfd, F_SETLEASE, l_type))&#123;</span><br><span class="line"></span><br><span class="line">perror(&quot;kernel lease set type&quot;);</span><br><span class="line"></span><br><span class="line">return -1;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="方法三：sendfile"><a href="#方法三：sendfile" class="headerlink" title="方法三：sendfile"></a>方法三：sendfile</h4><hr>
<p>从Linux 2.1版内核开始，Linux引入了sendfile，也能减少一次拷贝。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;sys/sendfile.h&gt;</span><br><span class="line"></span><br><span class="line">ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);</span><br></pre></td></tr></table></figure>
<p>sendfile 是只发生在内核态的数据传输接口，没有用户态的参与，自然避免了用户态数据拷贝。它指定在 in_fd 和 out_fd 之间传输数据，其中，它规定 in_fd 指向的文件必须是可以 mmap 的，out_fd 必须指向一个套接字，也就是规定数据只能从文件传输到套接字，反之则不行。sendfile 不存在像 mmap 时文件被截获的情况，它自带异常处理机制。</p>
<center><img src="/imagessss/linux/copy_sendfile.jpg" alt=""></center>

<p><strong>缺陷：</strong></p>
<p>1）只能适用于那些不需要用户态处理的应用程序。</p>
<h4 id="方法四：DMA-辅助的-sendfile"><a href="#方法四：DMA-辅助的-sendfile" class="headerlink" title="方法四：DMA 辅助的 sendfile"></a>方法四：DMA 辅助的 sendfile</h4><hr>
<p>常规 sendfile 还有一次内核态的拷贝操作，能不能也把这次拷贝给去掉呢？</p>
<p>答案就是这种 DMA 辅助的 sendfile。</p>
<p>这种方法借助硬件的帮助，在数据从内核缓冲区到 socket 缓冲区这一步操作上，并不是拷贝数据，而是拷贝缓冲区描述符，待完成后，DMA 引擎直接将数据从内核缓冲区拷贝到协议引擎中去，避免了最后一次拷贝。</p>
<center><img src="/imagessss/linux/copy_dma_sendfile.jpg" alt=""></center>

<p><strong>缺陷：</strong></p>
<p>1）除了3.4 中的缺陷，还需要硬件以及驱动程序支持。</p>
<p>2）只适用于将数据从文件拷贝到套接字上。</p>
<h4 id="方法五：splice"><a href="#方法五：splice" class="headerlink" title="方法五：splice"></a>方法五：splice</h4><hr>
<p>splice 去掉 sendfile 的使用范围限制，可以用于任意两个文件描述符中传输数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#define _GNU_SOURCE         /* See feature_test_macros(7) */</span><br><span class="line"></span><br><span class="line">#include &lt;fcntl.h&gt;</span><br><span class="line"></span><br><span class="line">ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);</span><br></pre></td></tr></table></figure>
<p>但是 splice 也有局限，它使用了 Linux 的管道缓冲机制，所以，它的两个文件描述符参数中至少有一个必须是管道设备。</p>
<p>splice 提供了一种流控制的机制，通过预先定义的水印（watermark）来阻塞写请求，有实验表明，利用这种方法将数据从一个磁盘传输到另外一个磁盘会增加 30%-70% 的吞吐量，CPU负责也会减少一半。</p>
<p><strong>缺陷：</strong> </p>
<p>1）同样只适用于不需要用户态处理的程序</p>
<p>2）传输描述符至少有一个是管道设备。</p>
<h4 id="方法六：写时复制"><a href="#方法六：写时复制" class="headerlink" title="方法六：写时复制"></a>方法六：写时复制</h4><hr>
<p>在某些情况下，内核缓冲区可能被多个进程所共享，如果某个进程想要这个共享区进行 write 操作，由于 write 不提供任何的锁操作，那么就会对共享区中的数据造成破坏，写时复制就是 Linux 引入来保护数据的。</p>
<p>写时复制，就是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么就需要将其拷贝到自己的进程地址空间中，这样做并不影响其他进程对这块数据的操作，每个进程要修改的时候才会进行拷贝，所以叫写时拷贝。这种方法在某种程度上能够降低系统开销，如果某个进程永远不会对所访问的数据进行更改，那么也就永远不需要拷贝。</p>
<p><strong>缺陷：</strong></p>
<p>需要 MMU 的支持，MMU 需要知道进程地址空间中哪些页面是只读的，当需要往这些页面写数据时，发出一个异常给操作系统内核，内核会分配新的存储空间来供写入的需求。</p>
<h4 id="方法七：缓冲区共享"><a href="#方法七：缓冲区共享" class="headerlink" title="方法七：缓冲区共享"></a>方法七：缓冲区共享</h4><hr>
<p>这种方法完全改写 I/O 操作，因为传统 I/O 接口都是基于数据拷贝的，要避免拷贝，就去掉原先的那套接口，重新改写，所以这种方法是比较全面的零拷贝技术，目前比较成熟的一个方案是最先在 Solaris 上实现的 fbuf （Fast Buffer，快速缓冲区）。</p>
<p>Fbuf 的思想是每个进程都维护着一个缓冲区池，这个缓冲区池能被同时映射到程序地址空间和内核地址空间，内核和用户共享这个缓冲区池，这样就避免了拷贝。</p>
<center><img src="/imagessss/linux/copy_fbuf.jpg" alt=""></center>

<p><strong>缺陷：</strong></p>
<p>1）管理共享缓冲区池需要应用程序、网络软件、以及设备驱动程序之间的紧密合作</p>
<p>2）改写 API ，尚处于试验阶段。</p>
<h4 id="高性能网络-I-O-框架——netmap"><a href="#高性能网络-I-O-框架——netmap" class="headerlink" title="高性能网络 I/O 框架——netmap"></a>高性能网络 I/O 框架——netmap</h4><hr>
<p>Netmap 基于共享内存的思想，是一个高性能收发原始数据包的框架，由Luigi Rizzo 等人开发完成，其包含了内核模块以及用户态库函数。其目标是，不修改现有操作系统软件以及不需要特殊硬件支持，实现用户态和网卡之间数据包的高性能传递。</p>
<center><img src="/imagessss/linux/netmap.jpg" alt=""></center>

<p>在 Netmap 框架下，内核拥有数据包池，发送环\接收环上的数据包不需要动态申请，有数据到达网卡时，当有数据到达后，直接从数据包池中取出一个数据包，然后将数据放入此数据包中，再将数据包的描述符放入接收环中。内核中的数据包池，通过 mmap 技术映射到用户空间。用户态程序最终通过 netmap_if 获取接收发送环 netmap_ring，进行数据包的获取发送。</p>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><hr>
<p>1、零拷贝本质上体现了一种优化的思想</p>
<p>2、直接 I/O，mmap，sendfile，DMA sendfile，splice，缓冲区共享，写时复制……</p>
<h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><hr>
<p>（1）Linux 中的零拷贝技术，第 1 部分<br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html</a><br>（2）Linux 中的零拷贝技术，第 2 部分<br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/</a><br>（3）Netmap 原理<br><a href="http://www.tuicool.com/articles/MnIRbuU" target="_blank" rel="noopener">http://www.tuicool.com/articles/MnIRbuU</a></p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/imagessss/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/Linux/"><i class="fas fa-hashtag fa-fw"></i>Linux</a>
                
                    <a href="/tags/零拷贝/"><i class="fas fa-hashtag fa-fw"></i>零拷贝</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2017/12/30/云计算/雾计算简史/">
              
                  雾计算简史
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2017-12-30
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/02-雾计算/">02 雾计算</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>在我看来，雾计算和云计算的本质是一样的——都是充分利用「闲置的资源」进行任务的处理。不同在于云计算利用的是网络核心的资源，而雾计算则利用网络边缘的资源。</p>
<p>1961 年，人工智能之父麦卡锡在一次会议上提出了「效用计算」这个概念，第一次体现了这种共享资源的思想。当时计算设备的价格非常昂贵，远非普通企业和机构所能承受，所以就想到将分散的闲置资源整合起来，共享给多个用户使用。类似的概念还有「网络计算」、「分布式计算」、「弹性计算」等，这些概念都是经由学术界提出，并逐步成为支撑工程应用发展的基础理论。</p>
<center><img src="/images/cloud/mkx.jpg" alt=""></center>

<p>工业界虽说比学术界要落后，但工业界做着让所有人都兴奋的事——让概念标准化，真正让技术实现落地。上面这些概念让人晕乎，虽有差异，但本质一致，所以工业界就统一用「云计算」一词来囊括。</p>
<p>2006 年 8 月 9 日，Google 首席执行官埃里克·施密特在搜索引擎大会(SES San Jose 2006)首次提出云计算(Cloud Computing)的概念。从此云计算的发展进入了井喷时代。</p>
<center><img src="/images/cloud/smt.jpg" alt=""></center>

<p>2007 年初，Amazon 推出弹性计算云 EC2 服务；2007 年 11 月，IBM 发布业界首个云计算商业解决方案「蓝云」计划；2008 年 4 月，Google APP Engine 发布；2008 年 Gartner 发布报告，认为云计算代表了今后计算的方向；2009 年 1 月，阿里巴巴在南京建立首个“电子商务云计算中心”；2010 年 1 月，微软正式发布 Microsoft Azure 云平台服务；2010 年 7 月，美国国家航空航天局和包括 Rackspace、AMD、Intel、戴尔等支持厂商共同宣布开放「OpenStack」项目源代码……</p>
<p>时至今日，云计算已经进入稳定发展时期，面临的是新的问题。</p>
<p>随着移动设备、嵌入式设备和传感设备等智能设备的不断创新和普及，进入了「万物互联网时代（IoT）」，全球的移动数据呈现出疯狂式的增长。</p>
<p>据 Cisco 2016 年做的一个关于移动数据预测报告显示，全球的移动数据将会在 2016 年和 2021 年之间提高 18  倍，2021 年截止将会超过 49 EB。面对大量的数据和新型的应用程序对服务质量的严苛需求，云计算的问题也凸显出来。</p>
<center><img src="/images/cloud/datagrow.jpg" alt=""></center>

<p>首先，云计算中心位于远程的 Internet，对于那些对延迟敏感的应用程序（如视频流、在线游戏等），将会带来较长的传播时延（WAN），这对于用户体验来说是无法忍受的。其次，对移动场景支持不足，特别是对于高速移动的车载网络环境，司机对于路况、交通流等的感知都必须是快速且实时的。再次，无法满足地理位置分布相关的感知环境的实时要求，如大规模的传感网络，要求传感节点定时向其他节点更新自身的信息。再有，大量的设备接入云端，网络带宽就显得捉襟见肘。最后，云计算的安全性和隐私性不容乐观，在用户和云计算中心之间需要经过多跳的网络传输，越深的网络传输，数据的完整性和机密性就越难保证。</p>
<p>以上问题的解决方案，自然是由学术界首先提出的。2009 年卡内基梅隆大学的沙特亚教授等人在其发表的论文[1]中提出「微云（Cloudlet）」的概念。论文指出微云是一种「和云有着同样的技术标准，但邻近用户」的新型计算模式。</p>
<center><img src="/images/cloud/cloudlet.jpg" alt=""></center>

<p>这里面蕴含着几层信息。第一，能够提供和云计算一样的服务，但所提供的资源有限，不如云计算能够提供无限的资源；第二，邻近用户，意味着用户请求的响应时延大大减少；第三，雾计算基础设施以分布式的方式部署在网络的边缘，满足高速移动场景和地理位置分布的场景需求，同时，减缓了网络核心的带宽负载；第四，安全性和隐私性较云计算得到较大保障。</p>
<p>这是「雾计算」最初的雏形。之后，学术界又提出很多类似的解决方案，如「Fog Computing」、「Edge Computing」、「Follow me Cloud」、「Small-Cell Cloud」、「Virtual Cloud」、「FemtoCloud」等。</p>
<p>这些概念其实本质都是一样的，都是在讲一件事，就是将「计算去中心化」——将云计算资源和服务从网络的核心转移到网络的边缘，以此来适应今天多种 IoT 应用的需求。和云计算的提出如出一辙，只不过引导这次工业变革的对象不再是 Google，而是 Cisco。</p>
<center><img src="/images/cloud/fog_cisco.jpg" alt=""></center>

<p>Cisco 在 Cisco Live 2014 会议上首度提出这个概念。Cisco 强调雾计算是依托于现今无处不在的 IoT 应用产生的一种新型计算模式。</p>
<p>相比于云计算，雾计算是一种更加新进和广泛的计算模式，更具扩展性和可持续性。但是雾计算也不能完全取代云计算，必须依托于云计算才能更好地发挥其作用，因此它们的关系是相辅相成，相互联系的。</p>
<p>在会上，Cisco 同时发布了供开发者使用的开发套件 IOx。IOx 是 Cisco 对于雾计算模式的实现。它为开发者提供了一整套的开发框架（包括开发、分发、部署、监控和管理等多种组件）和计算平台，开发者能够将开发好的应用部署到网络的边界上（路由器、交换机等）进行处理。如下是 IOx 的架构图，更多信息请访问 IOx 开发者文档。</p>
<center><img src="/images/cloud/fog_iox.jpg" alt=""></center>

<p>雾计算是应现今「人工智能」、「物联网」一波红海而生的技术革新。业界多家企业和组织机构都开始在布局雾计算的生态体系，除了 Cisco、华为这些通讯产商，还有很多云计算、物联网的企业也加入进来。</p>
<p>在中国，最早布局雾计算架构的是成立于2005 年的全球领先的物联网云服务商「智云」。他们在 2016 年初即发布了主打 IoT 雾计算的机智云 4.0，整合了雾计算、物联网大数据和机器学习应用能力，形成了一体化的解决方案。</p>
<p>此外，2015 年 11 月 19 日，Cisco 联合 ARM、Dell、Intel、Microsoft 和普林斯顿大学成立了「开放雾联盟（OpenFog）」，旨在制定雾计算相关的技术标准和推动行业的技术变革。</p>
<p>由此可见，雾计算将会成为继云计算之后的又一波技术浪潮。</p>
<p>PS：最近听一些朋友在谈「雾计算」，正好我研究生期间研究过一段时间这玩意，于是有一种不吐不快之感，遂拿出来跟大家分享了。如果你觉得对你有一点帮助，点赞，转发，不胜感激，或者有什么想跟我探讨的，欢迎留言。</p>
<p><strong>Reference:</strong> </p>
<hr>
<p>[1] The Case for VM-Based Cloudlets in Mobile Computing<br>[2] Fog Computing: A Taxonomy, Survey and future direction<br>[3] Edge-centric Computing: Vision and Challenges<br>[4] Edge-Computing: Challenges to support edge-as-a-service<br>[5] A virtual Cloud computing provider for mobile devices<br>[6] Femto Clouds: Leveraging mobile devices to provide Cloud service at the edge<br>[7] Follow me Cloud: interworking federated Clouds and distributed mobile networks</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/雾计算/"><i class="fas fa-hashtag fa-fw"></i>雾计算</a>
                
                    <a href="/tags/边缘计算/"><i class="fas fa-hashtag fa-fw"></i>边缘计算</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2017/12/23/云计算/OVS_总体架构、源码结构及数据流程全面解析/">
              
                  OVS 总体架构、源码结构及数据流程全面解析
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2017-12-23
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/OVS/">OVS</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>在前文「从 Bridge 到 OVS」中，我们已经对 OVS 进行了一番探索。本文决定从 OVS 的整体架构到各个组件都进行一个详细的介绍。</p>
<h3 id="OVS-架构"><a href="#OVS-架构" class="headerlink" title="OVS 架构"></a>OVS 架构</h3><hr>
<p>OVS 是产品级的虚拟交换机，大量应用在生产环境中，支撑整个数据中心虚拟网络的运转。OVS 基于 SDN 的思想，将整个核心架构分为控制面和数据面，数据面负责数据的交换工作，控制面实现交换策略，指导数据面工作。</p>
<center><img src="/images/virt/ovs_arch.jpg" alt=""></center>

<p>从整体上看，OVS 可以划分为三大块，管理面、数据面和控制面。</p>
<p>数据面就是以用户态的 ovs-vswitchd 和内核态的 datapath 为主的转发模块，以及与之相关联的数据库模块 ovsdb-server，控制面主要是由 ovs-ofctl 模块负责，基于 OpenFlow 协议与数据面进行交互。而管理面则是由 OVS 提供的各种工具来负责，这些工具的提供也是为了方便用户对底层各个模块的控制管理，提高用户体验。下面就对这些工具进行一个逐一的阐述。</p>
<p><strong>ovs-ofctl：</strong> 这个是控制面的模块，但本质上它也是一个管理工具，主要是基于 OpenFlow 协议对 OpenFlow 交换机进行监控和管理，通过它可以显示一个 OpenFlow 交换机的当前状态，包括功能、配置和表中的项。使用时，有很多参数，我们可以通过 ovs-ofctl –help 查看。</p>
<p>常用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ovs-ofctl show switch-name ：输出交换机信息，包括其流量表和端口信息。</span><br><span class="line"></span><br><span class="line">ovs-ofctl dump-ports switch-name：输出交换机的端口统计信息，包括收发包、丢包、错误包等数量。</span><br><span class="line"></span><br><span class="line">ovs-ofctl add-flow switch-name：为交换机配置流策略。</span><br></pre></td></tr></table></figure>
<p><strong>ovs-dpctl：</strong> 用来配置交换机的内核模块 datapath，它可以创建，修改和删除 datapath，一般，单个机器上的 datapath 有 256 条（0-255）。一条 datapath 对应一个虚拟网络设备。该工具还可以统计每条 datapath 上的设备通过的流量，打印流的信息等，更过参数通过 ovs-dpctl –help 查看。</p>
<p>常用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ovs-dpctl show ：显示所有 datapath 的基本信息。</span><br><span class="line"></span><br><span class="line">ovs-dpctl dump-dps ：显示所有 datapath 的名字。</span><br><span class="line"></span><br><span class="line">ovs-dpctl dump-flows DP ：显示一条 datapath DP 上的流信息。</span><br></pre></td></tr></table></figure>
<p><strong>ovs-appctl：</strong> 查询和控制运行中的 OVS 守护进程，包括 ovs-switchd，datapath，OpenFlow 控制器等，兼具 ovs-ofctl、ovs-dpctl 的功能，是一个非常强大的命令。ovs-vswitchd 等进程启动之后就以一个守护进程的形式运行，为了能够很好的让用户控制这些进程，就有了这个命令。详细可以 ovs-appctl –help 查看。</p>
<p><strong>ovs-vsctl：</strong> 查询和更新 ovs-vswitchd 的配置，这也是一个很强大的命令，网桥、端口、协议等相关的命令都由它来完成。此外，还负责和 ovsdb-server 相关的数据库操作。</p>
<p>常用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ovs-vsctl show ：显示主机上已有的网桥及端口信息。</span><br><span class="line"></span><br><span class="line">ovs-vsctl add-br br0：添加网桥 br0。</span><br></pre></td></tr></table></figure>
<p><strong>ovsdb-client：</strong> 访问 ovsdb-server 的客户端程序，通过 ovsdb-server 执行一些数据库操作。</p>
<p>常用命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ovsdb-client dump：用来查看ovsdb内容。</span><br><span class="line"></span><br><span class="line">ovsdb-client transact ：用来执行一条类 sql。</span><br></pre></td></tr></table></figure></p>
<p><strong>ovsdb-tool：</strong> 和 ovsdb-client 要借助 ovsdb-server 才能进行相关数据库操作不同，ovsdb-tool 可以直接操作数据库。</p>
<h3 id="OVS-源码结构"><a href="#OVS-源码结构" class="headerlink" title="OVS 源码结构"></a>OVS 源码结构</h3><hr>
<p>OVS 源码结构中，主要包含以下几个主要的模块，数据交换逻辑在 vswitchd 和 datapath 中实现，vswitchd 是最核心的模块，OpenFlow 的相关逻辑都在 vswitchd 中实现，datapath 则不是必须的模块。ovsdb 用于存储 vswitch 本身的配置信息，如端口、拓扑、规则等。控制面部分采用的是 OVS 自家实现的 OVN，和其他控制器相比，OVN 对 OVS 和 OpenStack 有更好的兼容性和性能。</p>
<center><img src="/images/virt/ovs_module.jpg" alt=""></center>

<p>从图中可以看出 OVS 的分层结构，最上层 vswitchd 主要与 ovsdb 通信，做配置下发和更新等，中间层是 ofproto ，用于和 OpenFlow 控制器通信，并基于下层的 ofproto provider 提供的接口，完成具体的设备操作和流表操作等工作。</p>
<p>dpif 层实现对流表的操作。</p>
<p>netdev 层实现了对网络设备（如 Ethernet）的抽象，基于 netdev provider 接口实现多种不同平台的设备，如 Linux 内核的 system, tap, internal 等，dpdk 系的 vhost, vhost-user 等，以及隧道相关的 gre, vxlan 等。</p>
<h3 id="数据转发流程"><a href="#数据转发流程" class="headerlink" title="数据转发流程"></a>数据转发流程</h3><hr>
<p>通过一个例子来看看 OVS 中数据包是如何进行转发的。</p>
<center><img src="/images/virt/ovs_dataflow.jpg" alt=""></center>

<p>1）ovs 的 datapath 接收到从 ovs 连接的某个网络端口发来的数据包，从数据包中提取源/目的 IP、源/目的 MAC、端口等信息。</p>
<p>2）ovs 在内核态查看流表结构（通过 hash），如果命中，则快速转发。</p>
<p>3）如果没有命中，内核态不知道如何处置这个数据包，所以，通过 netlink upcall 机制从内核态通知用户态，发送给 ovs-vswitchd 组件处理。</p>
<p>4）ovs-vswitchd 查询用户态精确流表和模糊流表，如果还不命中，在 SDN 控制器接入的情况下，经过 OpenFlow 协议，通告给控制器，由控制器处理。</p>
<p>5）如果模糊命中， ovs-vswitchd 会同时刷新用户态精确流表和内核态精确流表，如果精确命中，则只更新内核态流表。</p>
<p>6）刷新后，重新把该数据包注入给内核态 datapath 模块处理。</p>
<p>7）datapath 重新发起选路，查询内核流表，匹配；报文转发，结束。<br>总结</p>
<p>OVS 为了方便用户操作，提供了很多管理工具，我们平常在使用过程中只需记住每个工具的作用，具体的命令可以使用 -h 或 –help 查看。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/虚拟化/"><i class="fas fa-hashtag fa-fw"></i>虚拟化</a>
                
                    <a href="/tags/OVS/"><i class="fas fa-hashtag fa-fw"></i>OVS</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
                    <a href="/tags/OpenFlow/"><i class="fas fa-hashtag fa-fw"></i>OpenFlow</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2017/12/17/云计算/从_Bridge_到_OVS，探索虚拟交换机/">
              
                  从 Bridge 到 OVS，探索虚拟交换机
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2017-12-17
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/OVS/">OVS</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>和物理网络一样，虚拟网络要通信，必须借助一些交换设备来转发数据。因此，对于网络虚拟化来说，交换设备的虚拟化是很关键的一环。</p>
<p>上文「网络虚拟化」已经大致介绍了 Linux 内核为了满足网络虚拟化的要求，实现了一套虚拟交换设备——Bridge。本文重点介绍下 Bridge 的加强版——Open vSwitch（OVS），并从 Bridge 过渡到 OVS 的缘由讲起，让大家有个全面的认识。</p>
<p>借助 Linux Bridge 功能，同主机或跨主机的虚拟机之间能够轻松实现通信，也能够让虚拟机访问到外网，这就是我们所熟知的桥接模式，一般在装 VMware 虚拟机或者 VirtualBox 虚拟机的时候，都会提示我们要选择哪种模式，常用的两种模式是桥接和 NAT。</p>
<p>NAT 也很好理解，可以简单理解为当虚拟机启用了 NAT 模式之后，宿主机便通过 DHCP 为其生成可以访问外网的 IP，当 VM 访问外网的时候，就可以用该 IP 访问，其实就是宿主机为其做了地址转换。更详细的内容请自行搜索了解。</p>
<p>物理交换机有个重要的功能，就是虚拟局域网（VLAN），是对局域网（LAN）的软件化升级。一般，两台计算机通过一台交换机连接在一起就构成了一个 LAN。</p>
<p>一个 LAN 表示一个广播域，这意味着这个 LAN 中的任何节点发的数据包，其他节点都能收到，这有两个问题，一个是容易形成广播风暴，造成网络拥塞，另一个是广播包无法隔离，比如节点 B 不想接收节点 A 的包，但节点 A 强行要发，这就有点说不过去了。</p>
<p>解决这个问题的方案就是 VLAN，VLAN 能够对广播包进行有效隔离，它的做法是从软件上将交换机的端口虚拟出多个子端口，用 tag 来标记，相当于将交换机的端口划分多个 LAN，同一个 LAN 中的节点发出的数据包打上本 LAN 的 tag，这样，其他 LAN 中的节点就无法收到包，达到隔离的目的。</p>
<p>Bridge 本身是支持 VLAN 功能的，如下图所示，通过配置，Bridge 可以将一个物理网卡设备 eth0 划分成两个子设备 eth0.10，eth0.20，分别挂到 Bridge 虚拟出的两个 VLAN 上，VLAN id 分别为 VLAN 10 和 VLAN 20。同样，两个 VM 的虚拟网卡设备 vnet0 和 vnet 1 也分别挂到相应的 VLAN 上。这样配好的最终效果就是 VM1 不能和 VM2 通信了，达到了隔离。</p>
<center><img src="/images/virt/net_vlan.png" alt=""></center>

<p>Linux Bridge + VLAN 便可以构成一个和物理交换机具备相同功能的虚拟交换机了。对于网络虚拟化来说，Bridge 已经能够很好地充当交换设备的角色了。</p>
<p>但是为什么还有很多厂商都在做自己的虚拟交换机，比如比较流行的有 VMware virtual switch、Cisco Nexus 1000V，以及 Open vSwitch。究其原因，主要有以下几点（我们重点关注 OVS）：</p>
<p><strong>1）</strong> 方便网络管理与监控。OVS 的引入，可以方便管理员对整套云环境中的网络状态和数据流量进行监控，比如可以分析网络中流淌的数据包是来自哪个 VM、哪个 OS 及哪个用户，这些都可以借助 OVS 提供的工具来达到。</p>
<p><strong>2）</strong> 加速数据包的寻路与转发。相比 Bridge 单纯的基于 MAC 地址学习的转发规则，OVS 引入流缓存的机制，可以加快数据包的转发效率。</p>
<p><strong>3）</strong> 基于 SDN 控制面与数据面分离的思想。上面两点其实都跟这一点有关，OVS 控制面负责流表的学习与下发，具体的转发动作则有数据面来完成。可扩展性强。</p>
<p><strong>4）</strong> 隧道协议支持。Bridge 只支持 VxLAN，OVS 支持 gre/vxlan/IPsec 等。</p>
<p><strong>5）</strong> 适用于 Xen、KVM、VirtualBox、VMware 等多种 Hypervisors。</p>
<p>……</p>
<p>除此之外，OVS 还有很多高级特性，详情可以查阅官网自行了解。</p>
<p>下面简单看下 OVS 的整体架构，如下图所示，OVS 在 Linux 用户态和内核态都实现了相应的模块，用户态主要组件有数据库服务 ovsdb-server 和守护进程 ovs-vswitchd。内核态中实现了 datapath 模块。</p>
<center><img src="/images/virt/ovs_path.jpg" alt=""></center>

<p>其中， ovs-vswitchd 和 datapath 共同构成了 OVS 的数据面，控制面由 controller 模块来完成，controller 一般表示的是 OpenFlow 控制器，在 OVS 中，它可以借由第三方来完成，只要支持 OpenFlow 协议即可。</p>
<p>这里额外提一点，很多的一些产品级的虚拟交换机都是自身集成了控制器，比如 Cisco 1000V 的 Virtual Supervisor Manager(VSM)，VMware 的分布式交换机中的 vCenter，而 OVS 是把这个事交由第三方去做，这么做的意义还是比较大的，可以让自己的产品很好地融入到各种解决方案中。</p>
<h4 id="OpenFlow"><a href="#OpenFlow" class="headerlink" title="OpenFlow"></a><strong>OpenFlow</strong></h4><hr>
<p>OpenFlow 是控制面和数据面通信的一套协议，我们常常把支持 OpenFlow 协议的交换机称为 OpenFlow 交换机，控制器称为 OpenFlow 控制器，业界比较知名的 OpenFlow 控制器有 OpenDaylight、ONOS 等。</p>
<p>OpenFlow 是一个独立的完整的流表协议，不依赖于 OVS，OVS 只是支持 OpenFlow 协议，有了支持，就可以使用 OpenFlow 控制器来管理 OVS 中的流表。OpenFlow 不仅仅支持虚拟交换机，某些硬件交换机也支持 OpenFlow 协议。</p>
<h4 id="ovs-vswitchd"><a href="#ovs-vswitchd" class="headerlink" title="ovs-vswitchd"></a><strong>ovs-vswitchd</strong></h4><hr>
<p>ovs-vswitchd 是 OVS 的核心组件，它和内核模块 datapath 共同构成了 OVS 的数据面。它使用 OpenFlow 协议与 OpenFlow 控制器通信，使用 OVSDB 协议与 ovsdb-server 通信，使用 netlink 和 datapath 内核模块通信。</p>
<h4 id="ovsdb-server"><a href="#ovsdb-server" class="headerlink" title="ovsdb-server"></a><strong>ovsdb-server</strong></h4><hr>
<p>ovsdb-server 是 OVS 轻量级的数据库服务，用于整个 OVS 的配置信息，包括接口、交换内容、VLAN 等，ovs-vswitchd 根据这些配置信息工作。</p>
<h4 id="OpenFlow-控制器"><a href="#OpenFlow-控制器" class="headerlink" title="OpenFlow 控制器"></a><strong>OpenFlow 控制器</strong></h4><hr>
<p>OpenFlow 控制器可以通过 OpenFlow 协议连接到任何支持 OpenFlow 的交换机，比如 OVS 。控制器通过向交换机下发流表规则来控制数据流向。</p>
<h4 id="Kernel-Datapath"><a href="#Kernel-Datapath" class="headerlink" title="Kernel Datapath"></a><strong>Kernel Datapath</strong></h4><hr>
<p>datapath 内核模块和 ovs-vswitchd 是相互协作工作的，datapath 负责具体的收发包，而 ovs-vswitchd 通过 controller 下发的流表规则指导 datapath 如何转发包。</p>
<p>举个例子，datapath 从主机物理网卡 NIC 或者 VM 的 虚拟网卡 vNIC 收到包，如果是第一次收到包，datapath 不知道怎么处理这个包，于是将其丢给  ovs-vswitchd ， ovs-vswitchd 决定该如何处理这个包之后又丢给 datapath，datapath 根据 ovs-vswitchd 的指示执行相应的动作，是丢弃还是从哪个口传出去。同时，ovs-vswitchd 会让 datapath 缓存好这个包的动作，下次再来就可以直接执行动作。</p>
<p>如果不是第一次收到包，就是按照之前缓存好的动作执行，这样极大地提高了数据处理的速度。</p>
<p>本文先对 OVS 有个初步印象，下文再详细介绍 OVS 的其他组件。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/虚拟化/"><i class="fas fa-hashtag fa-fw"></i>虚拟化</a>
                
                    <a href="/tags/OVS/"><i class="fas fa-hashtag fa-fw"></i>OVS</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
                    <a href="/tags/Bridge/"><i class="fas fa-hashtag fa-fw"></i>Bridge</a>
                
                    <a href="/tags/OpenFlow/"><i class="fas fa-hashtag fa-fw"></i>OpenFlow</a>
                
                    <a href="/tags/VLAN/"><i class="fas fa-hashtag fa-fw"></i>VLAN</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2017/12/14/云计算/一文搞懂网络虚拟化/">
              
                  一文搞懂网络虚拟化
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2017-12-14
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/03-虚拟化/">03 虚拟化</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>网络虚拟化相对计算、存储虚拟化来说是比较抽象的，以我们在学校书本上学的那点网络知识来理解网络虚拟化可能是不够的。</p>
<p>在我们的印象中，网络就是由各种网络设备（如交换机、路由器）相连组成的一个网状结构，世界上的任何两个人都可以通过网络建立起连接。</p>
<p>带着这样一种思路去理解网络虚拟化可能会感觉云里雾里——这样一个庞大的网络如何实现虚拟化？</p>
<p>其实，网络虚拟化更多关注的是数据中心网络、主机网络这样比较「细粒度」的网络，所谓细粒度，是相对来说的，是深入到某一台物理主机之上的网络结构来谈的。</p>
<p>如果把传统的网络看作「宏观网络」的话，那网络虚拟化关注的就是「微观网络」。网络虚拟化的目的，是要节省物理主机的网卡设备资源。从资源这个角度去理解，可能会比较好理解一点。</p>
<h3 id="传统网络架构"><a href="#传统网络架构" class="headerlink" title="传统网络架构"></a>传统网络架构</h3><hr>
<p>在传统网络环境中，一台物理主机包含一个或多个网卡（NIC），要实现与其他物理主机之间的通信，需要通过自身的 NIC 连接到外部的网络设施，如交换机上，如下图所示。</p>
<center><img src="/images/virt/net_tran.jpg" alt=""><br>传统网络（图片来源于网络，侵权必删）<br></center>

<p>这种架构下，为了对应用进行隔离，往往是将一个应用部署在一台物理设备上，这样会存在两个问题，1）是某些应用大部分情况可能处于空闲状态，2）是当应用增多的时候，只能通过增加物理设备来解决扩展性问题。不管怎么样，这种架构都会对物理资源造成极大的浪费。</p>
<h3 id="虚拟化网络架构"><a href="#虚拟化网络架构" class="headerlink" title="虚拟化网络架构"></a>虚拟化网络架构</h3><hr>
<p>为了解决这个问题，可以借助虚拟化技术对一台物理资源进行抽象，将一张物理网卡虚拟成多张虚拟网卡（vNIC），通过虚拟机来隔离不同的应用。</p>
<p>这样对于上面的问题 1），可以利用虚拟化层 Hypervisor 的调度技术，将资源从空闲的应用上调度到繁忙的应用上，达到资源的合理利用；针对问题 2），可以根据物理设备的资源使用情况进行横向扩容，除非设备资源已经用尽，否则没有必要新增设备。这种架构如下所示。</p>
<center><img src="/images/virt/net_virt.jpg" alt=""><br>虚拟化网络（图片来源于网络，侵权必删）<br></center>


<p>其中虚拟机与虚拟机之间的通信，由虚拟交换机完成，虚拟网卡和虚拟交换机之间的链路也是虚拟的链路，整个主机内部构成了一个虚拟的网络，如果虚拟机之间涉及到三层的网络包转发，则又由另外一个角色——虚拟路由器来完成。</p>
<p>一般，这一整套虚拟网络的模块都可以独立出去，由第三方来完成，如其中比较出名的一个解决方案就是 Open vSwitch（OVS）。</p>
<p>OVS 的优势在于它基于 SDN 的设计原则，方便虚拟机集群的控制与管理，另外就是它分布式的特性，可以「透明」地实现跨主机之间的虚拟机通信，如下是跨主机启用 OVS 通信的图示。</p>
<center><img src="/images/virt/net_dis.jpg" alt=""><br>分布式虚拟交换机（图片来源于网络，侵权必删）<br></center>


<p>总结下来，网络虚拟化主要解决的是虚拟机构成的网络通信问题，完成的是各种网络设备的虚拟化，如网卡、交换设备、路由设备等。</p>
<h3 id="Linux-下网络设备虚拟化的几种形式"><a href="#Linux-下网络设备虚拟化的几种形式" class="headerlink" title="Linux 下网络设备虚拟化的几种形式"></a>Linux 下网络设备虚拟化的几种形式</h3><hr>
<p>为了完成虚拟机在同主机和跨主机之间的通信，需要借助某种“桥梁”来完成用户态到内核态（Guest 到 Host）的数据传输，这种桥梁的角色就是由虚拟的网络设备来完成，上面介绍了一个第三方的开源方案——OVS，它其实是一个融合了各种虚拟网络设备的集大成者，是一个产品级的解决方案。</p>
<p>但 Linux 本身由于虚拟化技术的演进，也集成了一些虚拟网络设备的解决方案，主要有以下几种：</p>
<h4 id="（1）TAP-TUN-VETH"><a href="#（1）TAP-TUN-VETH" class="headerlink" title="（1）TAP/TUN/VETH"></a>（1）TAP/TUN/VETH</h4><hr>
<p>TAP/TUN 是 Linux 内核实现的一对虚拟网络设备，TAP 工作在二层，TUN 工作在三层。Linux 内核通过 TAP/TUN 设备向绑定该设备的用户空间程序发送数据，反之，用户空间程序也可以像操作物理网络设备那样，向 TAP/TUN 设备发送数据。</p>
<p>基于 TAP 驱动，即可实现虚拟机 vNIC 的功能，虚拟机的每个 vNIC 都与一个 TAP 设备相连，vNIC 之于 TAP 就如同 NIC 之于 eth。</p>
<p>当一个 TAP 设备被创建时，在 Linux 设备文件目录下会生成一个对应的字符设备文件，用户程序可以像打开一个普通文件一样对这个文件进行读写。</p>
<p>比如，当对这个 TAP 文件执行 write 操作时，相当于 TAP 设备收到了数据，并请求内核接受它，内核收到数据后将根据网络配置进行后续处理，处理过程类似于普通物理网卡从外界收到数据。当用户程序执行 read 请求时，相当于向内核查询 TAP 设备是否有数据要发送，有的话则发送，从而完成 TAP 设备的数据发送。</p>
<p>TUN 则属于网络中三层的概念，数据收发过程和 TAP 是类似的，只不过它要指定一段 IPv4 地址或 IPv6 地址，并描述其相关的配置信息，其数据处理过程也是类似于普通物理网卡收到三层 IP 报文数据。</p>
<p>VETH 设备总是成对出现，一端连着内核协议栈，另一端连着另一个设备，一个设备收到内核发送的数据后，会发送到另一个设备上去，这种设备通常用于容器中两个 namespace 之间的通信。</p>
<h4 id="（2）Bridge"><a href="#（2）Bridge" class="headerlink" title="（2）Bridge"></a>（2）Bridge</h4><hr>
<p>Bridge 也是 Linux 内核实现的一个工作在二层的虚拟网络设备，但不同于 TAP/TUN 这种单端口的设备，Bridge 实现为多端口，本质上是一个虚拟交换机，具备和物理交换机类似的功能。</p>
<p>Bridge 可以绑定其他 Linux 网络设备作为从设备，并将这些从设备虚拟化为端口，当一个从设备被绑定到 Bridge 上时，就相当于真实网络中的交换机端口上插入了一根连有终端的网线。</p>
<p>如下图所示，Bridge 设备 br0 绑定了实际设备 eth0 和 虚拟设备设备 tap0/tap1，当这些从设备接收到数据时，会发送给 br0 ，br0 会根据 MAC 地址与端口的映射关系进行转发。</p>
<center><img src="/images/virt/net_br.png" alt=""><br>Bridge 与 TAP/TUN 的关系<br></center>

<p>因为 Bridge 工作在二层，所以绑定到它上面的从设备 eth0、tap0、tap1 均不需要设 IP，但是需要为 br0 设置 IP，因为对于上层路由器来说，这些设备位于同一个子网，需要一个统一的 IP 将其加入路由表中。</p>
<p>这里有人可能会有疑问，Bridge 不是工作在二层吗，为什么会有 IP 的说法？其实 Bridge 虽然工作在二层，但它只是 Linux 网络设备抽象的一种，能设 IP 也不足为奇。</p>
<p>对于实际设备 eth0 来说，本来它是有自己的 IP 的，但是绑定到 br0 之后，其 IP 就生效了，就和 br0 共享一个 IP 网段了，在设路由表的时候，就需要将 br0 设为目标网段的地址。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr>
<p>传统网络架构到虚拟化的网络架构，可以看作是宏观网络到微观网络的过渡</p>
<p>TAP/TUN/VETH、Bridge 这些虚拟的网络设备是 Linux 为了实现网络虚拟化而实现的网络设备模块，很多的云开源项目的网络功能都是基于这些技术做的，比如 Neutron、Docker network 等。</p>
<p>OVS 是一个开源的成熟的产品级分布式虚拟交换机，基于 SDN 的思想，被大量应用在生产环境中。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/虚拟化/"><i class="fas fa-hashtag fa-fw"></i>虚拟化</a>
                
                    <a href="/tags/OVS/"><i class="fas fa-hashtag fa-fw"></i>OVS</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
                    <a href="/tags/Bridge/"><i class="fas fa-hashtag fa-fw"></i>Bridge</a>
                
                    <a href="/tags/TAP-TUN-VETH/"><i class="fas fa-hashtag fa-fw"></i>TAP/TUN/VETH</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2017/12/10/云计算/2017_年除了人工智能，这门技术也在茁壮生长/">
              
                  2017 年除了人工智能，这门技术也在茁壮生长
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2017-12-10
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/01-云计算/">01 云计算</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>没错，这个标题就是个标题党，目的就是为了让你点进来看看。</p>
<p>2017 年是人工智能元年，我们也能看到各大互联网公司对于人工智能的大布局。但人工智能再怎么牛逼，别忘了它的底层基础设施是什么，没错，就是云计算，少了云计算的底层支撑，人工智能也只能活在书本里。虽然说云计算经过这么多年的沉淀，技术总体上来说已经比较成熟，但有一门技术仍然呈现欣欣向荣之势，未来发展仍有无限可能。</p>
<p>它就是以 docker 为首的容器技术。</p>
<p>不知道大家关注最近这两天的 KubeCon 2017 北美峰会没有，会上提得最多的也是容器技术，其中最大的新闻莫过于 OpenStack 基金会发布了最新开源容器项目 Kata Containers。看到这个消息，我的第一反应是「为啥最近容器圈的动作这么多」。</p>
<p>我们把时间拉回到 11 月底的中国开源年会上，当时阿里正式开源了自家自研容器项目 Pouch。再把时间拉到差不多两个月前（10月17日）的 DockerCon EU 2017 大会上，彼时 docker 官方宣布全面支持 kubernates。类似这样大大小小的动作，在 2017 年还发生了很多，这一切的动作都在说明容器的重要性，未来的可期待指数可以说不亚于人工智能。</p>
<p>下面简单介绍下 Kata 和阿里的 Pouch 到底是个怎样的角色，我也仅限于网络上各种资讯的了解，信息传达难免会有失误，如果大家觉得有问题可以留言指出。</p>
<h3 id="Kata-Containers-是什么"><a href="#Kata-Containers-是什么" class="headerlink" title="Kata Containers 是什么"></a>Kata Containers 是什么</h3><hr>
<p>Kata 官方宣称这是同时兼具容器的速度和虚拟机安全的全新容器解决方案，旨在将虚拟机的安全优势与容器的速度和可管理性统一起来，其建立在 Intel 的 Clear Containers 技术和 Hyper 的 runV 虚拟机管理程序运行时基础之上。</p>
<center><img src="/images/cloud/kata.jpg" alt=""></center>

<p>Kata 的特点是什么？总体来讲，Kata 解决的是容器的安全问题。众所周知，当前容器技术旨在实现在一个虚拟机之上运行多个用户的、多个应用的容器实例，不同实例之间共享同一个虚拟机操作系统内核并采用 Namespaces 来隔离，但这种方式很难保证各实例彼此之间的完全隔离，存在安全隐患。</p>
<p>Kata 的解决方案是意图为每个容器实例提供一个专属的、高度轻量化的虚拟机操作系统内核来解决这个问题。让某一个用户的、一个应用的一个或多个容器实例单独跑在这个专属的虚拟机内核之上，这样不同用户、不同应用之间都是使用独占的虚拟机，不会共享同一个操作系统内核，这样就确保了安全性。</p>
<center><img src="/images/cloud/kata_hyper.png" alt=""></center>

<p>另外还有一点值得注意的是，Kata 的设计初衷强调了能够无缝、便捷的与 OpenStack 和 Kubernetes 集成的能力，这为 OpenStack 、Kubernetes 和 Container 更好的融合铺平了道路。</p>
<center><img src="/images/cloud/kata_arch.jpg" alt=""></center>

<p>更详细的内容可以访问：</p>
<p><a href="http://www/katacontainers.io/" target="_blank" rel="noopener">http://www/katacontainers.io/</a><br><a href="https://github.com/hyperhq/runv" target="_blank" rel="noopener">https://github.com/hyperhq/runv</a></p>
<h3 id="Pouch-是什么"><a href="#Pouch-是什么" class="headerlink" title="Pouch 是什么"></a>Pouch 是什么</h3><hr>
<p>相比 Kata，阿里的 Pouch 就没那么新鲜了，只不过是换了个马甲而已。</p>
<p>为什么这么说，因为 Pouch 并不是全新的容器解决方案，而是已经在阿里内部经过千锤百炼的老牌容器技术 t4。2011 年，Linux 内核的 namespace、cgroup 等技术开始成熟，LXC 等容器运行时技术也在同期诞生，阿里作为一家技术公司，在当时便基于 LXC 自研了自己的容器技术 t4，并以产品的形式给内部提供服务。</p>
<p>t4 就是 Pouch 的前身，从时间节点上看，t4 面世比 docker 要早两年，但 t4 有很多问题没有解决，譬如说没有镜像机制。2013 年，docker 横空出世，其带有镜像创新的容器技术，似一阵飓风，所到之处，国内外无不叫好，阿里也不例外，便在现有技术体系结构的基础上融入了 docker 的镜像技术，慢慢打磨，演变成今天的 Pouch。</p>
<p>Pouch 针对自身的业务场景对镜像的下载和分发进行了创新。由于阿里的业务体量庞大，集群规模数以万计，这就会存在一个问题就是镜像的下载和分发效率会很低，所以针对此，阿里在 Pouch 中集成了一个镜像分发工具蜻蜓（Dragonfly），蜻蜓基于智能 P2P 技术的文件分发系统，解决了大规模文件分发场景下分发耗时、成功率低、带宽浪费等难题。</p>
<center><img src="/images/cloud/pouch.jpg" alt=""></center>

<p>Pouch 的架构主要考虑到两个方面，一方面是如何对接容器编排系统，另一方面是如何加强容器运行时，第一点让 Pouch 有了对外可扩展的能力，譬如可以原生支持 Kubernetes 等编排系统。第二点可以增加 Pouch 对虚拟机和容器的统一管理，让其适应更多的业务场景。</p>
<center><img src="/images/cloud/pouch_arch.jpg" alt=""></center>

<p>更详细的内容可以访问：</p>
<p><a href="https://github.com/alibaba/pouch" target="_blank" rel="noopener">https://github.com/alibaba/pouch</a></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr>
<p>从几种举动中，我们可以看出，未来容器发展具有两大重要方向，分别是容器编排技术和容器的安全加强。这些都是在寻求一种更好的、更有效率的方式来为上层的业务提供更可靠、更安全的支撑。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/容器/"><i class="fas fa-hashtag fa-fw"></i>容器</a>
                
                    <a href="/tags/Docker/"><i class="fas fa-hashtag fa-fw"></i>Docker</a>
                
                    <a href="/tags/Kata/"><i class="fas fa-hashtag fa-fw"></i>Kata</a>
                
                    <a href="/tags/Pouch/"><i class="fas fa-hashtag fa-fw"></i>Pouch</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2017/12/06/云计算/I:O_虚拟化的三种形式/">
              
                  I/O 虚拟化的三种形式
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2017-12-06
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/03-虚拟化/">03 虚拟化</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>上文「I/O 虚拟化」简单介绍了 I/O 虚拟化的做法，本文重点关注一下三种 I/O 虚拟化的分类——全虚拟化、半虚拟化和 I/O 直通（透传）。</p>
<h3 id="I-O-全虚拟化"><a href="#I-O-全虚拟化" class="headerlink" title="I/O 全虚拟化"></a>I/O 全虚拟化</h3><hr>
<p>这种方式比较好理解，简单来说，就是通过纯软件的形式来模拟虚拟机的 I/O 请求。以 qemu-kvm 来举例，内核中的 kvm 模块负责截获 I/O 请求，然后通过事件通知告知给用户空间的设备模型 qemu，qemu 负责完成本次 I/O 请求的模拟。更具体的内容可以翻阅前文。</p>
<center><img src="/images/virt/iofull.jpg" alt=""></center>

<p><strong>优点：</strong>  </p>
<p>不需要对操作系统做修改，也不需要改驱动程序，因此这种方式对于多种虚拟化技术的「可移植性」和「兼容性」比较好。</p>
<p><strong>缺点：</strong></p>
<p>纯软件形式模拟，自然性能不高，另外，虚拟机发出的 I/O 请求需要虚拟机和 VMM 之间的多次交互，产生大量的上下文切换，造成巨大的开销。</p>
<h3 id="I-O-半虚拟化"><a href="#I-O-半虚拟化" class="headerlink" title="I/O 半虚拟化"></a>I/O 半虚拟化</h3><hr>
<p>针对 I/O 全虚拟化纯软件模拟性能不高这一点，I/O 半虚拟化前进了一步。它提供了一种机制，使得 Guest 端与 Host 端可以建立连接，直接通信，摒弃了截获模拟这种方式，从而获得较高的性能。</p>
<p>值得注意的有两点：1）采用 I/O 环机制，使得 Guest 端和 Host 端可以共享内存，减少了虚拟机与 VMM 之间的交互；2）采用事件和回调的机制来实现 Guest 与 Host VMM 之间的通信。这样，在进行中断处理时，就可以直接采用事件和回调机制，无需进行上下文切换，减少了开销。</p>
<p>要实现这种方式， Guest 端和 Host 端需要采用类似于 C/S 的通信方式建立连接，这也就意味着要修改 Guest 和 Host 端操作系统内核相应的代码，使之满足这样的要求。为了描述方便，我们统称 Guest 端为前端，Host 端为后端。</p>
<center><img src="/images/virt/iohalf.png" alt=""></center>

<p>前后端通常采用的实现方式是驱动的方式，即前后端分别构建通信的驱动模块，前端实现在内核的驱动程序中，后端实现在 qemu 中，然后前后端之间采用共享内存的方式传递数据。关于这方面一个比较好的开源实现是 virtio，后面会有专门的文章来讲述之。</p>
<p><strong>优点：</strong></p>
<p>性能较 I/O 全虚拟化有了较大的提升</p>
<p><strong>缺点：</strong></p>
<p>要修改操作系统内核以及驱动程序，因此会存在移植性和适用性方面的问题，导致其使用受限。</p>
<h3 id="I-O-直通或透传技术"><a href="#I-O-直通或透传技术" class="headerlink" title="I/O 直通或透传技术"></a>I/O 直通或透传技术</h3><hr>
<p>上面两种虚拟化方式，还是从软件层面上来实现，性能自然不会太高。最好的提高性能的方式还是从硬件上来解决。如果让虚拟机独占一个物理设备，像宿主机一样使用物理设备，那无疑性能是最好的。</p>
<p>I/O 直通技术就是提出来完成这样一件事的。它通过硬件的辅助可以让虚拟机直接访问物理设备，而不需要通过 VMM 或被 VMM 所截获。</p>
<p>由于多个虚拟机直接访问物理设备，会涉及到内存的访问，而内存又是共享的，那怎么来隔离各个虚拟机对内存的访问呢，这里就要用到一门技术——IOMMU，简单说，IOMMU 就是用来隔离虚拟机对内存资源访问的。</p>
<p>I/O 直通技术需要硬件支持才能完成，这方面首选是 Intel 的 VT-d 技术，它通过对芯片级的改造来达到这样的要求，这种方式固然对性能有着质的提升，不需要修改操作系统，移植性也好。</p>
<p>但该方式也是有一定限制的，这种方式仅限于物理资源丰富的机器，因为这种方式仅仅能满足一个设备分配给一个虚拟机，一旦一个设备被虚拟机占用了，其他虚拟机时无法使用该设备的。</p>
<center><img src="/images/virt/iommu.png" alt=""></center>

<p>为了解决这个问题，使一个物理设备能被更多的虚拟机所共享。学术界和工业界都对此作了大量的改进，PCI-SIG 发布了 SR-IOV  (Single Root I/O Virtualizmion) 规范，其中详细阐述了硬件供应商在多个虚拟机中如何共享单个 I/O 设备硬件。</p>
<p>SR-IOV标准定义了设备原生共享所需的「软硬件支持」。硬件支持包括芯片组对 SR-IOV 设备的识别，为保证对设备的安全、隔离访问还需要北桥芯片的 VT-d 支持，为保证虚拟机有独立的内存空间，CPU 要支持 IOMMU。软件方面，VMM 将驱动管理权限交给 Guest，Guest 操作系统必须支持 SR-IOV 功能。</p>
<p>SR-IOV 单独引入了两种软件实体功能：</p>
<ul>
<li><p>PF（physical function）：包含轻量级的 PCIe 功能，负责管理 SR-IOV 设备的特殊驱动，其主要功能是为 Guest 提供设备访问功能和全局贡献资源配置的功能。</p>
</li>
<li><p>VF（virtual function）：包含轻量级的 PCIe 功能。其功能包含三个方面：向虚拟机操作系统提供的接口；数据的发送、接收功能；与 PF 进行通信，完成全局相关操作。</p>
</li>
</ul>
<p>每个 SR-IOV 设备都可有一个物理功能 PF，并且每个 PF 最多可有 64,000 个与其关联的虚拟功能  VF。</p>
<p>一般，Guest 通过物理功能 PF 驱动发现设备的 SR-IOV 功能后将包括发送、接收队列在内的物理资源依据 VF 数目划分成多个子集，然后 PF 驱动将这些资源子集抽象成 VF 设备，这样，VF 设备就可以通过某种通信机制分配给虚拟机了。</p>
<center><img src="/images/virt/iosriov.jpg" alt=""></center>

<p>尽管 I/O 直通技术消除了虚拟机 I/O 中 VMM 干预引起的额外开销，但在 I/O 操作中 I/O 设备会产生大量的中断，出于安全等因素考虑，虚拟机无法直接处理中断，因此中断请求需要由 VMM 安全、隔离地路由至合适的虚拟机。所以，其实实际使用中，都是软硬件虚拟化方式结合使用的。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/虚拟化/"><i class="fas fa-hashtag fa-fw"></i>虚拟化</a>
                
                    <a href="/tags/KVM/"><i class="fas fa-hashtag fa-fw"></i>KVM</a>
                
                    <a href="/tags/Qemu/"><i class="fas fa-hashtag fa-fw"></i>Qemu</a>
                
                    <a href="/tags/I-O/"><i class="fas fa-hashtag fa-fw"></i>I/O</a>
                
                    <a href="/tags/SR-IOV/"><i class="fas fa-hashtag fa-fw"></i>SR-IOV</a>
                
                    <a href="/tags/IOMMU/"><i class="fas fa-hashtag fa-fw"></i>IOMMU</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2017/12/02/云计算/I:O_虚拟化/">
              
                  I/O 虚拟化
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2017-12-02
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/03-虚拟化/">03 虚拟化</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>I/O 虚拟化在虚拟化技术中算是比较复杂，也是最重要的一部分。从整体上看，I/O 虚拟化也包括基于软件的虚拟化和硬件辅助的虚拟化，软件虚拟化部分又可以分为全虚拟化和半虚拟化，如果根据设备类型再细分的话，又可以分为字符设备 I/O 虚拟化（键盘、鼠标、显示器）、块设备 I/O 虚拟化（磁盘、光盘）和网络设备 I/O 虚拟化（网卡）等。</p>
<p>我们先看下在没有虚拟机存在的情况下，I/O 设备请求是怎样的。某个应用或进程发出 I/O 请求，通过系统调用等方式进入内核，调用相应的驱动程序，请求到具体的 I/O 设备，然后再将结果返回到调用者进行回显。</p>
<p>那么存在虚拟机的情况下，又是怎样的呢。按理说，虚拟机是宿主机上的一个进程，应该可以以类似的 I/O 请求方式访问到宿主机上的 I/O 设备，但别忘了，虚拟机处在非 Root 的虚拟化模式下，请求无法直接下发到宿主机，必须借助于 VMM 来截获并模拟虚拟机的 I/O 请求。</p>
<p>至于怎么截获并模拟，每一种 VMM 的实现方案都不一样，像 qemu-kvm ，截获操作是由内核态的 kvm 来完成，模拟操作是由用户态的 qemu 来完成的，这也是 kvm 不同于其他 VMM 实现方案的地方。kvm 这样做也是为了提升性能和保持内核的纯净性，关于这块的知识不清楚的可以查阅前文。</p>
<p>从层次上看，虚拟机发出 I/O 请求到完成相应的 I/O 操作，中间要经过虚拟机的设备驱动，到 VMM 的设备模型，再到宿主机的设备驱动，最终才到真正的 I/O 设备。</p>
<p>那什么是设备模型，设备模型就是 VMM 中进行设备模拟，并处理所有设备请求和响应的逻辑模块，对于 qemu-kvm，qemu 其实就可以看做是一个设备模型。</p>
<center><img src="/images/virt/io_virt.png" alt=""></center>

<p>上图显示的就是设备模型的逻辑层次关系，对于不同构造的虚拟机，其逻辑层次是类似的：VMM 截获虚拟机的 I/O 操作，将这些操作传递给设备模型进行处理，设备模型运行在一个特定的环境下，这可以是宿主机，可以是 VMM 本身，也可以是另一个虚拟机。</p>
<p>下图显示的就是在宿主机中设备模型的实现，也就是 qemu-kvm 的实现方案，在这个例子中，VMM 主要部分实现为内核模块，设备模型实现为一个用户态进程，当虚拟机发生 I/O 之后，VMM 作为内核模块将其截获后，会通过内核态-用户态接口传递给用户态的设备模型处理，设备模型运行与宿主机操作系统之上，可以使用相应的系统调用和所有运行时库，宿主机操作系统就是设备模型的运行环境。</p>
<center><img src="/images/virt/io_module.png" alt=""></center>

<p>所以，设备模型在这里起着一个桥梁的作用，由虚拟机设备驱动发出的 I/O 请求先通过设备模型转化为物理 I/O 设备的请求，再通过调用物理设备驱动来完成相应的 I/O 操作。反过来，设备驱动将 I/O 操作结果通过设备模型，返回给虚拟机的虚拟设备驱动程序。</p>
<p>上面说的这种方式是纯软件模拟的，或者说得再专业一点就是全虚拟化，全虚拟化就是 VMM 完全虚拟出一套宿主机的设备模型，宿主机有什么就虚拟出什么，这样，虚拟机发出的任何 I/O 请求都是无感知的，也是说虚拟机认为自己在“直接”使用物理的 I/O 设备，其实不是，全是虚拟出来的。</p>
<p>有了全虚拟化，自然就有半虚拟化，半虚拟化的提出就是解决全虚拟化的性能问题的。通过上面的分析，不难看出，这种截获再模拟的方式导致一次 I/O 请求要经过多次的内核态和用户态的切换，性能肯定不理想。半虚拟化就是尽量避免这种情况发生。</p>
<p>半虚拟化中，虚拟机能够感知到自己是处于虚拟化状态，虚拟机和宿主机之间通过某种机制来达成这种感知，也就是两者之间需要建立一套通信接口，虚拟机的 I/O 请求走这套接口，而不是走截获模拟那种方式，这样就可以提升性能。这套接口一个比较好的实现就是 virtio，Linux 2.6.30 版本之后就被集成到了 Linux 内核模块中。</p>
<p>半虚拟化虽然提升了性能，但是还是基于软件模拟的方式，性能上还是无法与直接访问物理 I/O 设备相抗衡，那能不能做到呢，答案是一定的，那就是从硬件上去入手了。</p>
<p>以 Intel VT-d 为首的技术就是硬件辅助的 I/O 虚拟化技术，但是业界一般不是直接使用硬件，而是配合相应的软件技术来完成，比较常用的两门技术是 PCI Pass-Through 和 SR-IOV。</p>
<p>本文仅是简单总结下 I/O 虚拟化的方式，分类，以及存在的技术问题。后面会针对具体的类别或问题进行展开。</p>
<p>PS：如果你觉得本文对你有一点帮助，点赞，转发，不胜感激。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/虚拟化/"><i class="fas fa-hashtag fa-fw"></i>虚拟化</a>
                
                    <a href="/tags/KVM/"><i class="fas fa-hashtag fa-fw"></i>KVM</a>
                
                    <a href="/tags/Qemu/"><i class="fas fa-hashtag fa-fw"></i>Qemu</a>
                
                    <a href="/tags/I-O/"><i class="fas fa-hashtag fa-fw"></i>I/O</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2017/11/27/云计算/内存虚拟化/">
              
                  内存虚拟化
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2017-11-27
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/03-虚拟化/">03 虚拟化</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><hr>
<p>我们知道，早期的计算机内存，只有物理内存，而且空间是极其有限的，每个应用或进程在使用内存时都得小心翼翼，不能覆盖别的进程的内存区。</p>
<p>为了避免这些问题，就提出了虚拟内存的概念，其抽象了物理内存，相当于对物理内存进行了虚拟化，保证每个进程都被赋予一块连续的，超大的（根据系统结构来定，32 位系统寻址空间为 2^32，64 位系统为 2^64）虚拟内存空间，进程可以毫无顾忌地使用内存，不用担心申请内存会和别的进程冲突，因为底层有机制帮忙处理这种冲突，能够将虚拟地址根据一个页表映射成相应的物理地址。</p>
<p>这种机制正是虚拟化软件做的事，也就是 MMU 内存管理单元。</p>
<center><img src="/images/virt/mem_virt.png" alt=""></center>

<p>本文要说的不是这种虚拟内存，而是基于虚拟机的内存虚拟化，它们本质上是一样的，通过对虚拟内存的理解，再去理解内存虚拟化就比较容易了。</p>
<p>结合前面的文章，我们知道，虚拟化分为软件虚拟化和硬件虚拟化，而且遵循 intercept 和 virtualize 的规律。</p>
<p>内存虚拟化也分为基于软件的内存虚拟化和硬件辅助的内存虚拟化，其中，常用的基于软件的内存虚拟化技术为「影子页表」技术，硬件辅助内存虚拟化技术为 Intel 的 EPT（Extend Page Table，扩展页表）技术。</p>
<p>为了讲清楚这两门技术，我们从简易到复杂，循序渐进，逐步揭开其神秘面纱。</p>
<h3 id="常规软件内存虚拟化"><a href="#常规软件内存虚拟化" class="headerlink" title="常规软件内存虚拟化"></a>常规软件内存虚拟化</h3><hr>
<p>虚拟机本质上是 Host 机上的一个进程，按理说应该可以使用 Host 机的虚拟地址空间，但由于在虚拟化模式下，虚拟机处于非 Root 模式，无法直接访问 Root 模式下的 Host 机上的内存。</p>
<p>这个时候就需要 VMM 的介入，VMM 需要 intercept （截获）虚拟机的内存访问指令，然后 virtualize（模拟）Host 上的内存，相当于 VMM 在虚拟机的虚拟地址空间和 Host 机的虚拟地址空间中间增加了一层，即虚拟机的物理地址空间，也可以看作是 Qemu 的虚拟地址空间（稍微有点绕，但记住一点，虚拟机是由 Qemu 模拟生成的就比较清楚了）。</p>
<p>所以，内存软件虚拟化的目标就是要将虚拟机的虚拟地址（Guest Virtual Address, GVA）转化为 Host 的物理地址（Host Physical Address, HPA），中间要经过虚拟机的物理地址（Guest Physical Address, GPA）和 Host 虚拟地址（Host Virtual Address）的转化，即：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GVA -&gt; GPA -&gt; HVA -&gt; HPA</span><br></pre></td></tr></table></figure></p>
<p>其中前两步由虚拟机的系统页表完成，中间两步由 VMM 定义的映射表（由数据结构 kvm_memory_slot 记录）完成，它可以将连续的虚拟机物理地址映射成非连续的 Host 机虚拟地址，后面两步则由 Host 机的系统页表完成。如下图所示。</p>
<center><img src="/images/virt/mem_shadow.png" alt=""></center>

<p>这样做得目的有两个：</p>
<ol>
<li><p>提供给虚拟机一个从零开始的连续的物理内存空间。</p>
</li>
<li><p>在各虚拟机之间有效隔离、调度以及共享内存资源。</p>
</li>
</ol>
<h3 id="影子页表技术"><a href="#影子页表技术" class="headerlink" title="影子页表技术"></a>影子页表技术</h3><hr>
<p>接上图，我们可以看到，传统的内存虚拟化方式，虚拟机的每次内存访问都需要 VMM 介入，并由软件进行多次地址转换，其效率是非常低的。因此才有了影子页表技术和 EPT 技术。</p>
<p>影子页表简化了地址转换的过程，实现了 Guest 虚拟地址空间到 Host 物理地址空间的直接映射。</p>
<p>要实现这样的映射，必须为 Guest 的系统页表设计一套对应的影子页表，然后将影子页表装入 Host 的 MMU 中，这样当 Guest 访问 Host 内存时，就可以根据 MMU 中的影子页表映射关系，完成 GVA 到 HPA 的直接映射。而维护这套影子页表的工作则由 VMM 来完成。</p>
<p>由于 Guest 中的每个进程都有自己的虚拟地址空间，这就意味着 VMM 要为 Guest 中的每个进程页表都维护一套对应的影子页表，当 Guest 进程访问内存时，才将该进程的影子页表装入 Host 的 MMU 中，完成地址转换。</p>
<p>我们也看到，这种方式虽然减少了地址转换的次数，但本质上还是纯软件实现的，效率还是不高，而且 VMM 承担了太多影子页表的维护工作，设计不好。</p>
<p>为了改善这个问题，就提出了基于硬件的内存虚拟化方式，将这些繁琐的工作都交给硬件来完成，从而大大提高了效率。</p>
<h3 id="EPT-技术"><a href="#EPT-技术" class="headerlink" title="EPT 技术"></a>EPT 技术</h3><hr>
<p>这方面 Intel 和 AMD 走在了最前面，Intel 的 EPT 和 AMD 的 NPT 是硬件辅助内存虚拟化的代表，两者在原理上类似，本文重点介绍一下 EPT 技术。</p>
<p>如下图是 EPT 的基本原理图示，EPT 在原有 CR3 页表地址映射的基础上，引入了 EPT 页表来实现另一层映射，这样，GVA-&gt;GPA-&gt;HPA 的两次地址转换都由硬件来完成。</p>
<center><img src="/images/virt/ept.png" alt=""></center>

<p>这里举一个小例子来说明整个地址转换的过程。假设现在 Guest 中某个进程需要访问内存，CPU 首先会访问 Guest 中的 CR3 页表来完成 GVA 到 GPA 的转换，如果 GPA 不为空，则 CPU 接着通过 EPT 页表来实现 GPA 到 HPA 的转换（实际上，CPU 会首先查看硬件 EPT TLB 或者缓存，如果没有对应的转换，才会进一步查看 EPT 页表），如果 HPA 为空呢，则 CPU 会抛出 EPT Violation 异常由 VMM 来处理。</p>
<p>如果 GPA 地址为空，即缺页，则 CPU 产生缺页异常，注意，这里，如果是软件实现的方式，则会产生 VM-exit，但是硬件实现方式，并不会发生 VM-exit，而是按照一般的缺页中断处理，这种情况下，也就是交给 Guest 内核的中断处理程序处理。</p>
<p>在中断处理程序中会产生 EXIT_REASON_EPT_VIOLATION，Guest  退出，VMM 截获到该异常后，分配物理地址并建立 GVA 到 HPA 的映射，并保存到 EPT 中，这样在下次访问的时候就可以完成从 GVA 到 HPA 的转换了。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr>
<p>内存虚拟化经历从虚拟内存，到传统软件辅助虚拟化，影子页表，再到硬件辅助虚拟化，EPT 技术的进化，效率越来越高。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/虚拟化/"><i class="fas fa-hashtag fa-fw"></i>虚拟化</a>
                
                    <a href="/tags/KVM/"><i class="fas fa-hashtag fa-fw"></i>KVM</a>
                
                    <a href="/tags/内存/"><i class="fas fa-hashtag fa-fw"></i>内存</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
</section>


    <br>
    <div class="prev-next">
        <div class="prev-next">
            
                <a class="prev" rel="prev" href="/page/4/">
                    <section class="post prev">
                        <i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;上一页&nbsp;
                    </section>
                </a>
            
            <p class="current">
                5 / 10
            </p>
            
                <a class="next" rel="next" href="/page/6/">
                    <section class="post next">
                        &nbsp;下一页&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i>
                    </section>
                </a>
            

        </div>
    </div>



<!-- 根据主题中的设置决定是否在archive中针对摘要部分的MathJax公式加载mathjax.js文件 -->





        </div>
        <aside class='l_side'>
            
  
  
    
      
      
        <section class="plain">
  
<header class="pure">
  <div><i class="fas fa-bullhorn fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;公告</div>
  
</header>

  <div class="content pure">
    <p>你好，这里是<a href="https://chambai.github.io/materialblog/">小白的网志空间</a>，我在这里分享技术和生活，专注于云计算/网络/CC++/Python/Go等技术栈，平常喜欢左手Coding，右手Writing，欢迎关注我的公众号「cloud_dev」，期待与你相遇~</p>

  </div>
</section>

      
    
  
    
      
      
        <section class="author">
  <div class="content pure">
    
      <div class="avatar">
        <img class="avatar" src="/images/cloud.png">
      </div>
    
    
      <div class="text">
        
        
          <p>公众号：CloudDeveloper</p>

        
        
      </div>
    
    
  </div>
</section>

      
    
  
    
      
      
        
  <section class="category">
    
<header class="pure">
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;所有分类</div>
  
</header>

    <div class="content pure">
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/01-云计算/" href="/categories/01-云计算/"><div class="name">01 云计算</div><div class="badge">(7)</div></a></li>
        
          <li><a class="flat-box" title="/categories/02-雾计算/" href="/categories/02-雾计算/"><div class="name">02 雾计算</div><div class="badge">(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/03-虚拟化/" href="/categories/03-虚拟化/"><div class="name">03 虚拟化</div><div class="badge">(10)</div></a></li>
        
          <li><a class="flat-box" title="/categories/04-算法/" href="/categories/04-算法/"><div class="name">04 算法</div><div class="badge">(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/05-工具/" href="/categories/05-工具/"><div class="name">05 工具</div><div class="badge">(9)</div></a></li>
        
          <li><a class="flat-box" title="/categories/DPDK/" href="/categories/DPDK/"><div class="name">DPDK</div><div class="badge">(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Docker/" href="/categories/Docker/"><div class="name">Docker</div><div class="badge">(9)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Kubernetes/" href="/categories/Kubernetes/"><div class="name">Kubernetes</div><div class="badge">(13)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Linux/" href="/categories/Linux/"><div class="name">Linux</div><div class="badge">(11)</div></a></li>
        
          <li><a class="flat-box" title="/categories/NFV/" href="/categories/NFV/"><div class="name">NFV</div><div class="badge">(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/OVS/" href="/categories/OVS/"><div class="name">OVS</div><div class="badge">(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Python/" href="/categories/Python/"><div class="name">Python</div><div class="badge">(8)</div></a></li>
        
          <li><a class="flat-box" title="/categories/影评/" href="/categories/影评/"><div class="name">影评</div><div class="badge">(8)</div></a></li>
        
          <li><a class="flat-box" title="/categories/杂谈/" href="/categories/杂谈/"><div class="name">杂谈</div><div class="badge">(6)</div></a></li>
        
          <li><a class="flat-box" title="/categories/科技互联/" href="/categories/科技互联/"><div class="name">科技互联</div><div class="badge">(4)</div></a></li>
        
          <li><a class="flat-box" title="/categories/读书/" href="/categories/读书/"><div class="name">读书</div><div class="badge">(3)</div></a></li>
        
      </ul>
    </div>
  </section>


      
    
  
    
      
      
        
  <section class="tagcloud">
    
<header class="pure">
  <div><i class="fas fa-fire fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;热门标签</div>
  
</header>

    <div class="content pure">
      <a href="/tags/Bridge/" style="font-size: 15.82px; color: #8d8d8d">Bridge</a> <a href="/tags/CNM/" style="font-size: 14px; color: #999">CNM</a> <a href="/tags/CPU/" style="font-size: 16.73px; color: #868686">CPU</a> <a href="/tags/Cgroup/" style="font-size: 14.91px; color: #939393">Cgroup</a> <a href="/tags/DPDK/" style="font-size: 15.82px; color: #8d8d8d">DPDK</a> <a href="/tags/Django/" style="font-size: 14px; color: #999">Django</a> <a href="/tags/Docker/" style="font-size: 21.27px; color: #686868">Docker</a> <a href="/tags/I-O/" style="font-size: 16.73px; color: #868686">I/O</a> <a href="/tags/IOMMU/" style="font-size: 14px; color: #999">IOMMU</a> <a href="/tags/KVM/" style="font-size: 19.45px; color: #747474">KVM</a> <a href="/tags/Kata/" style="font-size: 14px; color: #999">Kata</a> <a href="/tags/Kubernetes/" style="font-size: 23.09px; color: #5b5b5b">Kubernetes</a> <a href="/tags/LaTeX/" style="font-size: 14.91px; color: #939393">LaTeX</a> <a href="/tags/Linux/" style="font-size: 23.09px; color: #5b5b5b">Linux</a> <a href="/tags/Markdown/" style="font-size: 14px; color: #999">Markdown</a> <a href="/tags/NFV/" style="font-size: 14px; color: #999">NFV</a> <a href="/tags/NUMA/" style="font-size: 15.82px; color: #8d8d8d">NUMA</a> <a href="/tags/Namespace/" style="font-size: 14.91px; color: #939393">Namespace</a> <a href="/tags/Numpy/" style="font-size: 14px; color: #999">Numpy</a> <a href="/tags/OVS/" style="font-size: 16.73px; color: #868686">OVS</a> <a href="/tags/OpenFlow/" style="font-size: 14.91px; color: #939393">OpenFlow</a> <a href="/tags/OpenStack/" style="font-size: 14px; color: #999">OpenStack</a> <a href="/tags/Pouch/" style="font-size: 14px; color: #999">Pouch</a> <a href="/tags/Python/" style="font-size: 20.36px; color: #6e6e6e">Python</a> <a href="/tags/Qemu/" style="font-size: 18.55px; color: #7a7a7a">Qemu</a> <a href="/tags/SDN/" style="font-size: 14px; color: #999">SDN</a> <a href="/tags/SPDK/" style="font-size: 14px; color: #999">SPDK</a> <a href="/tags/SR-IOV/" style="font-size: 14px; color: #999">SR-IOV</a> <a href="/tags/Sublime-Text/" style="font-size: 14px; color: #999">Sublime Text</a> <a href="/tags/TAP-TUN-VETH/" style="font-size: 14px; color: #999">TAP/TUN/VETH</a> <a href="/tags/UIO/" style="font-size: 14px; color: #999">UIO</a> <a href="/tags/VLAN/" style="font-size: 14px; color: #999">VLAN</a> <a href="/tags/VPP/" style="font-size: 14px; color: #999">VPP</a> <a href="/tags/cacico/" style="font-size: 14px; color: #999">cacico</a> <a href="/tags/flannel/" style="font-size: 14px; color: #999">flannel</a> <a href="/tags/fstack/" style="font-size: 14px; color: #999">fstack</a> <a href="/tags/git/" style="font-size: 15.82px; color: #8d8d8d">git</a> <a href="/tags/github/" style="font-size: 14.91px; color: #939393">github</a> <a href="/tags/hexo/" style="font-size: 14px; color: #999">hexo</a> <a href="/tags/libnetwork/" style="font-size: 14px; color: #999">libnetwork</a> <a href="/tags/mTCP/" style="font-size: 14.91px; color: #939393">mTCP</a> <a href="/tags/macvlan/" style="font-size: 14px; color: #999">macvlan</a> <a href="/tags/matplotlib/" style="font-size: 14px; color: #999">matplotlib</a> <a href="/tags/overlay/" style="font-size: 14px; color: #999">overlay</a> <a href="/tags/vhost/" style="font-size: 14.91px; color: #939393">vhost</a> <a href="/tags/vhost-user/" style="font-size: 14px; color: #999">vhost_user</a> <a href="/tags/virtio/" style="font-size: 14.91px; color: #939393">virtio</a> <a href="/tags/weave/" style="font-size: 14px; color: #999">weave</a> <a href="/tags/乱码/" style="font-size: 14px; color: #999">乱码</a> <a href="/tags/云计算/" style="font-size: 24px; color: #555">云计算</a> <a href="/tags/人民的名义/" style="font-size: 14px; color: #999">人民的名义</a> <a href="/tags/共享经济/" style="font-size: 14px; color: #999">共享经济</a> <a href="/tags/内存/" style="font-size: 14.91px; color: #939393">内存</a> <a href="/tags/创新/" style="font-size: 14px; color: #999">创新</a> <a href="/tags/大页内存/" style="font-size: 14px; color: #999">大页内存</a> <a href="/tags/容器/" style="font-size: 21.27px; color: #686868">容器</a> <a href="/tags/容器网络/" style="font-size: 17.64px; color: #808080">容器网络</a> <a href="/tags/小米/" style="font-size: 14px; color: #999">小米</a> <a href="/tags/微服务/" style="font-size: 14px; color: #999">微服务</a> <a href="/tags/性能分析/" style="font-size: 18.55px; color: #7a7a7a">性能分析</a> <a href="/tags/情怀/" style="font-size: 14px; color: #999">情怀</a> <a href="/tags/手机/" style="font-size: 14px; color: #999">手机</a> <a href="/tags/技能图谱/" style="font-size: 14.91px; color: #939393">技能图谱</a> <a href="/tags/推荐系统/" style="font-size: 14px; color: #999">推荐系统</a> <a href="/tags/数据结构/" style="font-size: 14px; color: #999">数据结构</a> <a href="/tags/杂谈/" style="font-size: 14px; color: #999">杂谈</a> <a href="/tags/架构/" style="font-size: 14px; color: #999">架构</a> <a href="/tags/混合云/" style="font-size: 14px; color: #999">混合云</a> <a href="/tags/电影推荐/" style="font-size: 14px; color: #999">电影推荐</a> <a href="/tags/简历/" style="font-size: 14px; color: #999">简历</a> <a href="/tags/算法/" style="font-size: 14px; color: #999">算法</a> <a href="/tags/网络/" style="font-size: 22.18px; color: #616161">网络</a> <a href="/tags/罗胖/" style="font-size: 14px; color: #999">罗胖</a> <a href="/tags/老罗/" style="font-size: 15.82px; color: #8d8d8d">老罗</a> <a href="/tags/虚拟化/" style="font-size: 23.09px; color: #5b5b5b">虚拟化</a> <a href="/tags/读书/" style="font-size: 14.91px; color: #939393">读书</a> <a href="/tags/边缘计算/" style="font-size: 14.91px; color: #939393">边缘计算</a> <a href="/tags/锤子科技/" style="font-size: 15.82px; color: #8d8d8d">锤子科技</a> <a href="/tags/阿里巴巴/" style="font-size: 14px; color: #999">阿里巴巴</a> <a href="/tags/集群/" style="font-size: 14px; color: #999">集群</a> <a href="/tags/零拷贝/" style="font-size: 14px; color: #999">零拷贝</a> <a href="/tags/雾计算/" style="font-size: 15.82px; color: #8d8d8d">雾计算</a>
    </div>
  </section>


      
    
  
    
      
      
        <section class="list">
  
<header class="pure">
  <div><i class="fas fa-medal fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;精选项目</div>
  
    <a class="rightBtn" target="_blank" rel="external nofollow noopener noreferrer" href="https://xaoxuu.com/projects/" title="https://xaoxuu.com/projects/">
    <i class="fas fa-arrow-right fa-fw"></i></a>
  
</header>

  <div class="content pure">
    <ul class="entry">
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/axkit/" href="https://xaoxuu.com/wiki/axkit/">
          <div class="name">
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;AXKit
          </div>
          
            <div class="badge">(iOS开源库)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/noticeboard/" href="https://xaoxuu.com/wiki/noticeboard/">
          <div class="name">
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;NoticeBoard
          </div>
          
            <div class="badge">(iOS开源库)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/heartmate/" href="https://xaoxuu.com/heartmate/">
          <div class="name">
            
              <i class="fas fa-heartbeat fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;HeartMate
          </div>
          
            <div class="badge">(iOS应用程序)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/material-x/" href="https://xaoxuu.com/wiki/material-x/">
          <div class="name">
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Material X
          </div>
          
            <div class="badge">(Hexo博客主题)</div>
          
        </a></li>
      
    </ul>
  </div>
</section>

      
    
  
    
      
      
        <section class="list">
  
<header class="pure">
  <div><i class="fas fa-link fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;友链</div>
  
</header>

  <div class="content pure">
    <ul class="entry">
      
        <li><a class="flat-box" title="https://xaoxuu.com/about/" href="https://xaoxuu.com/about/">
          <div class="name">
            
              <i class="fas fa-comment-dots fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;关于我 / 留言板
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

      
    
  
    
      
      
        



      
    
  
    
      
      
        

      
    
  


        </aside>
        <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
    </div>
    <footer id="footer" class="clearfix">
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>本站使用 <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a> 作为主题，总访问量为 <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span> 次。
  </div>
</footer>

    <script>setLoadingBarProgress(80);</script>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>


  
    <script src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
    <script type="text/javascript">
      $(function() {
        const $reveal = $('.reveal');
    		if ($reveal.length === 0) return;
    		const sr = ScrollReveal({ distance: 0 });
    		sr.reveal('.reveal');
      });
    </script>
  
  
    <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
    <script type="text/javascript">
      $(function() {
        Waves.attach('.flat-btn', ['waves-button']);
        Waves.attach('.float-btn', ['waves-button', 'waves-float']);
        Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
        Waves.attach('.flat-box', ['waves-block']);
        Waves.attach('.float-box', ['waves-block', 'waves-float']);
        Waves.attach('.waves-image');
        Waves.init();
      });
    </script>
  
  
    <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>
  
  
  


  
  
  
    
  
  
    <script src="/js/app.js"></script>
<script src="/js/search.js"></script>
  






    <script>setLoadingBarProgress(100);</script>
</body>
</html>
