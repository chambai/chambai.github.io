<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>小白的网志空间</title>
  
  
  <meta name="description" content="专注于云计算/网络/CC++/Python/Go等技术栈，分享读书、写作、思维、认知等话题">
  

  <link rel="alternate" href="/atom.xml" title="小白的网志空间">

  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  
  
  <meta name="theme-color" content="#f24e32">
  
  <meta name="msapplication-TileColor" content="#f24e32">
  
  <meta name="msapplication-config" content="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/browserconfig.xml">
  
  
  <!-- link -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.6.3/css/all.min.css">

  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-fonts@master/Ubuntu/Ubuntu-Regular.ttf">
  
  <link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicon.ico" type="image/x-icon">
  
  <link rel="icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/favicon-32x32.png" type="image/x-icon" sizes="32x32">
  
  <link rel="apple-touch-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/apple-touch-icon.png" type="image/png" sizes="180x180">
  
  <link rel="mask-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/safari-pinned-tab.svg" color="#f24e32">
  
  <link rel="manifest" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/site.webmanifest">
  
  

  
  <link rel="stylesheet" href="/style.css">
  

  



  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
</head>

<body>
    <div id="loading-bar-wrapper">
  <div id="loading-bar" class="pure"></div>
</div>

    <script>setLoadingBarProgress(20)</script>
    <header class="l_header pure">
	<div class="wrapper">
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href="/">
        
          小白的网志空间
        
      </a>
			<div class="menu">
				<ul class="h-list">
          
  					
  						<li>
								<a id="home" class="nav flat-box" href="/">
									<i class="fas fa-home fa-fw"></i>&nbsp;主页
								</a>
							</li>
      			
  						<li>
								<a id="archives" class="nav flat-box" href="/archives/">
									<i class="fas fa-archive fa-fw"></i>&nbsp;归档
								</a>
							</li>
      			
  						<li>
								<a id="friends" class="nav flat-box" href="/friends/">
									<i class="fas fa-users fa-fw"></i>&nbsp;导航
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索">
						<span class="icon"><i class="fas fa-search fa-fw"></i></span>
					</form>
				</div>
			
			<ul class="switcher h-list">
				
					<li class="s-search"><a class="fas fa-search fa-fw" href="javascript:void(0)"></a></li>
				
				<li class="s-menu"><a class="fas fa-bars fa-fw" href="javascript:void(0)"></a></li>
			</ul>
		</div>

		<div class="nav-sub container container--flex">
			<a class="logo flat-box"></a>
			<ul class="switcher h-list">
				<li class="s-comment"><a class="flat-btn fas fa-comments fa-fw" href="javascript:void(0)"></a></li>
				<li class="s-toc"><a class="flat-btn fas fa-list fa-fw" href="javascript:void(0)"></a></li>
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu">
      <ul>
          
              
                  <li>
										<a id="home" class="nav flat-box" href="/">
											<i class="fas fa-home fa-fw"></i>&nbsp;主页
										</a>
                  </li>
              
                  <li>
										<a id="archives" class="nav flat-box" href="/archives/">
											<i class="fas fa-archive fa-fw"></i>&nbsp;归档
										</a>
                  </li>
              
                  <li>
										<a id="friends" class="nav flat-box" href="/friends/">
											<i class="fas fa-users fa-fw"></i>&nbsp;导航
										</a>
                  </li>
              
       
      </ul>
		</nav>
    </header>
	</aside>

    <script>setLoadingBarProgress(40);</script>
    <div class="l_body">
    <div class='container clearfix'>
        <div class='l_main'>
            

<section class="post-list">
    
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/05/18/技术/一文掌握Linux性能分析之内存篇/">
              
                  一文掌握 Linux 性能分析之内存篇
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-05-18
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Linux/">Linux</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<h3 id="内存信息"><a href="#内存信息" class="headerlink" title="内存信息"></a>内存信息</h3><hr>
<p>同样在分析内存之前，我们得知到怎么查看系统内存信息，有以下几种方法。</p>
<h4 id="proc-meminfo"><a href="#proc-meminfo" class="headerlink" title="/proc/meminfo"></a><strong>/proc/meminfo</strong></h4><p>这个文件记录着比较详细的内存配置信息，使用 cat /proc/meminfo 查看。</p>
<center><img src="/images/linux/meminfo.jpg" alt=""></center>

<p>我们比较关心的是下面几个字段：</p>
<ul>
<li>MemTotal：系统总内存，由于 BIOS、内核等会占用一些内存，所以这里和配置声称的内存会有一些出入，比如我这里配置有 2G，但其实只有 1.95G 可用。</li>
<li>MemFree：系统空闲内存。</li>
<li>MemAvailable：应用程序可用内存。有人会比较奇怪和 MemFree 的区别，可以从两个层面来区分，MemFree 是系统层面的，而 MemAvailable 是应用程序层面的。系统中有些内存虽然被使用了但是有一部分是可以回收的，比如 Buffers、Cached 及 Slab 这些内存，这部分可以回收的内存加上 MemFree 才是 MemAvailable 的内存值，这是内核通过特定算法算出来的，是一个估算值。</li>
<li>Buffers：缓冲区内存</li>
<li>Cached：缓存</li>
</ul>
<p>上面信息没有 MemUsed 的值，虽然可以用现有的值大致估算出来，但是我们想一步到位，就用下面的 free 命令。</p>
<h4 id="free"><a href="#free" class="headerlink" title="free"></a><strong>free</strong></h4><p>这个命令估计用的人就多了（我一般都是用这个命令）。</p>
<center><img src="/images/linux/free.png" alt=""></center>

<p>这里存在一个计算公式：</p>
<p><strong>MemTotal = used + free + buff/cache（单位 K）</strong></p>
<p>几个字段和上面 /proc/meminfo 的字段是对应的。还有个 shared 字段，这个是多进程的共享内存空间，不常用。</p>
<p>我们注意到 free 很小，buff/cache 却很大，这是 Linux 的内存设计决定的，Linux 的想法是内存闲着反正也是闲着，不如拿出来做系统缓存和缓冲区，提高数据读写的速率。但是当系统内存不足时，buff/cache 会让出部分来，非常灵活的操作。</p>
<p>要看比较直观的值，可以加 -h 参数：</p>
<center><img src="/images/linux/freeh.png" alt=""></center>

<h4 id="dmidecode"><a href="#dmidecode" class="headerlink" title="dmidecode"></a><strong>dmidecode</strong></h4><p>同样可以使用这个命令，对于内存，可以使用 dmidecode -t memory 查看：</p>
<center><img src="/images/linux/dmimem.jpg" alt=""></center>

<h4 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a><strong>vmstat</strong></h4><p>这个命令也是非常常用了。但对于内存，显示信息有限。它更多是用于进行系统全局分析和 CPU 分析。详细可以看 CPU 分析一文。</p>
<center><img src="/images/linux/vmstatmem.jpg" alt=""></center>

<h3 id="进程内存使用情况分析"><a href="#进程内存使用情况分析" class="headerlink" title="进程内存使用情况分析"></a>进程内存使用情况分析</h3><hr>
<p>最常用的两个命令 ps 和 top，虽然很简单的两个命令，但还是有不少学问的。</p>
<h4 id="top-htop"><a href="#top-htop" class="headerlink" title="top/htop"></a><strong>top/htop</strong></h4><p>top 命令运行时默认是按照 CPU 利用率进行排序的，如果要按照内存排序，该怎么操作呢？两种方法，一种直接按 “M”（相应的按 “P” 是 CPU），另外一种是在键入 top 之后，按下 “F”，然后选择要排序的字段，再按下 “s” 确认即可。</p>
<center><img src="/images/linux/topmem.jpg" alt=""></center>

<p>可以看到，我按照 “%MEM” 排序的结果。这个结果对于查看系统占用内存较多的哪些进程是比较有用的。</p>
<p>然后这里我们会重点关注几个地方，上面横排区，和前面几个命令一样可以查看系统内存信息，中间标注的横条部分，和内存相关的有三个字段：VIRT、RES、SHR。</p>
<ul>
<li>VIRT：virtual memory usage，进程占用的虚拟内存大小。</li>
<li>RES：resident memory usage，进程常驻内存大小，也就是实际内存占用情况，一般我们看进程占用了多少内存，就是看的这个值。</li>
<li>SHR：shared memory，共享内存大小，不常用。</li>
</ul>
<h4 id="ps"><a href="#ps" class="headerlink" title="ps"></a><strong>ps</strong></h4><p>ps 同样可以查看进程占用内存情况，一般常用来查看 Top n 进程占用内存情况，如：</p>
<p><code>ps aux --sort=rss | head -n</code>，表示按 rss 排序，取 Top n。</p>
<center><img src="/images/linux/psmem.jpg" alt=""></center>

<p>这里也关注三个字段：</p>
<ul>
<li>%MEM：进程使用物理内存所占百分比。</li>
<li>VSZ：进程使用虚拟内存大小。</li>
<li>RSS：进程使用物理内存大小，我们会重点关注这个值。</li>
</ul>
<h4 id="pmap"><a href="#pmap" class="headerlink" title="pmap"></a><strong>pmap</strong></h4><p>这个命令用于查看进程的内存映像信息，能够查看进程在哪些地方用了多少内存。常用 <code>pmap -x pid</code> 来查看。</p>
<center><img src="/images/linux/pmap1.jpg" alt=""></center><br><center><img src="/images/linux/pmap2.jpg" alt=""></center>

<p>可以看到该进程内存被哪些库、哪些文件所占用，据此我们定位程序对内存的使用。</p>
<p>几个字段介绍一下：</p>
<ul>
<li>Address：占用内存的文件的内存起始地址。</li>
<li>Kbytes：占用内存的字节数。</li>
<li>RSS：实际占用内存大小。</li>
<li>Dirty：脏页大小。</li>
<li>Mapping：占用内存的文件，[anon] 为已分配的内存，[stack] 为程序堆栈</li>
<li>最后的 total 为统计的总值。我们可以使用 pmap -x pid | tail -1 这样只显示最后一行，循环显示最后一行，达到监控该进程的目的。使用：<br><code>while true; do pmap -x pid | tail -1; sleep 1; done</code></li>
</ul>
<p>OK，以上工具都是 Linux 自带的，当然还有很多高阶的工具，比如 atop、memstat 等等，对于内存泄漏有一个比较常用的检测工具 Valgrind，这些等之后再找时间跟大家分享了。</p>
<p>通过以上手段，我们基本上就能定位内存问题所在了，究竟是内存太小，还是进程占用内存太多，有哪些进程占用较多，这些进程又究竟有哪些地方占用较多，这些问题通过以上方法都能解决。</p>
<p>最后简单总结下，以上不少工具可能有人会犯选择困难症了。对于我来说，查看系统内存用 free -h，分析进程内存占用用 ps 或者 top（首选 ps），深入分析选择 pmap，就酱。</p>
<p>Reference：<br>1.Linux下查看内存使用情况的多种方法：<br><a href="http://stor.51cto.com/art/201804/570236.htm" target="_blank" rel="noopener">http://stor.51cto.com/art/201804/570236.htm</a></p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/Linux/"><i class="fas fa-hashtag fa-fw"></i>Linux</a>
                
                    <a href="/tags/性能分析/"><i class="fas fa-hashtag fa-fw"></i>性能分析</a>
                
                    <a href="/tags/内存/"><i class="fas fa-hashtag fa-fw"></i>内存</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/05/10/技术/CPU拓扑：从SMP谈到NUMA（实践篇）/">
              
                  CPU 拓扑：从 SMP 谈到 NUMA （理论篇）
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-05-10
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Linux/">Linux</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>本文的一些概念依赖于上一篇 “CPU 拓扑：从 SMP 谈到 NUMA （理论篇）”，如果你对这块还没有概念，建议看完那篇再来看，如果对这块有自己独到的见解，欢迎探讨。</p>
<p>本文主要会侧重 NUMA 这块，我会通过自己的环境验证 NUMA  的几个概念，以便对 NUMA 的架构有个较深的印象。</p>
<h3 id="NUMA-的几个概念：Node，Socket，Core，Thread"><a href="#NUMA-的几个概念：Node，Socket，Core，Thread" class="headerlink" title="NUMA 的几个概念：Node，Socket，Core，Thread"></a>NUMA 的几个概念：Node，Socket，Core，Thread</h3><hr>
<p>NUMA 技术的主要思想是将 CPU 进行分组，Node 即是分组的抽象，一个 Node 表示一个分组，一个分组可以由多个 CPU 组成。每个 Node 都有自己的本地资源，包括内存、IO 等。每个 Node 之间通过互联模块（QPI）进行通信，因此每个 Node 除了可以访问自己的本地内存之外，还可以访问远端 Node 的内存，只不过性能会差一些，一般用 distance 这个抽象的概念来表示各个 Node 之间互访资源的开销。</p>
<center><img src="/images/linux/numaarch1.jpg" alt=""></center>

<p>Node 是一个逻辑上的概念，与之相对的 Socket，是物理上的概念。它表示一颗物理 CPU 的封装，是主板上的 CPU 插槽，所以，一般就称之为插槽（敲黑板，这 Y 不是套接字吗？emmm……）</p>
<p>Core 就是 Socket 里独立的一组程序执行单元，称之为物理核。有了物理核，自然就有逻辑核，Thread 就是逻辑核。更专业的说法应该称之为超线程。</p>
<p>超线程是为了进一步提高 CPU 的处理能力，Intel 提出的新型技术。它能够将一个 Core 从逻辑上划分成多个逻辑核（一般是两个），每个逻辑核有独立的寄存器和中断逻辑，但是一个 Core 上的多个逻辑核共享 Core 内的执行单元和 Cache，频繁调度可能会引起资源竞争，影响性能。超线程必须要 CPU 支持才能开启。</p>
<center><img src="/images/linux/ht.jpg" alt=""></center>

<p>综上所述，一个 NUMA Node 可以有一个或者多个 Socket，每个 Socket 也可以有一个（单核）或者多个（多核）Core，一个 Core 如果打开超线程，则会变成两个逻辑核（Logical Processor，简称 Processor）。</p>
<p>所以，几个概念从大到小排序依次是：</p>
<p><strong>Node &gt; Socket &gt; Core &gt; Processor</strong>。</p>
<h3 id="验证-CPU-拓扑"><a href="#验证-CPU-拓扑" class="headerlink" title="验证 CPU 拓扑"></a>验证 CPU 拓扑</h3><hr>
<p>了解了以上基本概念，下面在实际环境中查看这些概念并验证。</p>
<h4 id="Node"><a href="#Node" class="headerlink" title="Node"></a><strong>Node</strong></h4><p>用 <code>numactl --hardware</code> 查看当前系统的 NUMA Node（numactl 是设定进程 NUMA 策略的命令行工具）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Linux # numactl --hardware</span><br><span class="line">available: 2 nodes (0-1)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 12 13 14 15 16 17</span><br><span class="line">node 0 size: 49043 MB</span><br><span class="line">node 0 free: 20781 MB</span><br><span class="line">node 1 cpus: 6 7 8 9 10 11 18 19 20 21 22 23</span><br><span class="line">node 1 size: 49152 MB</span><br><span class="line">node 1 free: 31014 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0   1 </span><br><span class="line">0:  10  21 </span><br><span class="line">1:  21  10</span><br></pre></td></tr></table></figure></p>
<p>可以得出的信息有：1）系统的 Node 数为 2；2）每个 Node 包含的 Processor 数为 12；3）每个 Node 的总内存大小和空闲内存大小；4）每个 Node 之间的 distance。</p>
<p>还可以查看 <code>/sys/devices/system/node/</code> 目录，这里记录着具体哪些 Node。</p>
<h4 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a><strong>Socket</strong></h4><p><code>/proc/cpuinfo</code> 中记录着 Socket 信息，用 “physical id” 表示，可以用 <code>cat /proc/cpuinfo | grep &quot;physical id&quot;</code> 查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Linux # cat /proc/cpuinfo | grep &quot;physical id&quot;</span><br><span class="line">physical id     : 0</span><br><span class="line">physical id     : 0</span><br><span class="line">physical id     : 0</span><br><span class="line">physical id     : 0</span><br><span class="line">physical id     : 1</span><br><span class="line">physical id     : 1</span><br><span class="line">physical id     : 1</span><br><span class="line">physical id     : 1</span><br></pre></td></tr></table></figure>
<p>可以看到有 2 个 Socket，我们还可以查看以下这几种变种：</p>
<p><strong>1）查看有几个 Socket</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Linux # grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;&#123;print $2 | &quot;sort -un&quot;&#125;&apos;</span><br><span class="line"></span><br><span class="line">0</span><br><span class="line">1</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux # grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;&#123;print $2 | &quot;sort -un&quot;&#125;&apos; | wc -l</span><br><span class="line"></span><br><span class="line">2</span><br></pre></td></tr></table></figure>
<p><strong>2）查看每个 Socket 有几个 Processor</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Linux # grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;&#123;print $2&#125;&apos; | sort | uniq -c</span><br><span class="line"></span><br><span class="line">12 0</span><br><span class="line">12 1</span><br></pre></td></tr></table></figure></p>
<p><strong>3）查看每个 Socket 有哪几个 Processor</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Linux # awk -F: &apos;&#123; </span><br><span class="line">&gt;     if ($1 ~ /processor/) &#123;</span><br><span class="line">&gt;         gsub(/ /,&quot;&quot;,$2);</span><br><span class="line">&gt;         p_id=$2;</span><br><span class="line">&gt;     &#125; else if ($1 ~ /physical id/)&#123;</span><br><span class="line">&gt;         gsub(/ /,&quot;&quot;,$2);</span><br><span class="line">&gt;         s_id=$2;</span><br><span class="line">&gt;         arr[s_id]=arr[s_id] &quot; &quot; p_id</span><br><span class="line">&gt;     &#125;</span><br><span class="line">&gt; &#125; </span><br><span class="line">&gt; </span><br><span class="line">&gt; END&#123;</span><br><span class="line">&gt;     for (i in arr) </span><br><span class="line">&gt;         print arr[i];</span><br><span class="line">&gt; &#125;&apos; /proc/cpuinfo | cut -c2-</span><br><span class="line"></span><br><span class="line">0 1 2 3 4 5 12 13 14 15 16 17</span><br><span class="line">6 7 8 9 10 11 18 19 20 21 22 23</span><br></pre></td></tr></table></figure></p>
<h4 id="Core"><a href="#Core" class="headerlink" title="Core"></a><strong>Core</strong></h4><p>同样在 /proc/cpuinfo 中查看 Core 信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Linux # cat /proc/cpuinfo |grep &quot;core id&quot; | sort -u</span><br><span class="line">core id    : 0</span><br><span class="line">core id    : 1</span><br><span class="line">core id    : 2</span><br><span class="line">core id    : 3</span><br><span class="line">core id    : 4</span><br><span class="line">core id    : 5</span><br></pre></td></tr></table></figure>
<p>上面的结果表明一个 Socket 有 5 个 Core。上面查到有 2 个 Socket，则一共就有 10 个 Core。</p>
<h4 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a><strong>Processor</strong></h4><p>上面查看 Socket 信息时已经能够得到 Processor 的信息，总共有 24 个 Processor，不过也可以直接从 /proc/cpuinfo 中获取：</p>
<p><strong>1）获取总的 Processor 数，查看 “processor” 字段：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux # cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l</span><br><span class="line"></span><br><span class="line">24</span><br></pre></td></tr></table></figure></p>
<p><strong>2）获取每个 Socket 的 Processor 数，查看 “siblings” 字段：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux # cat /proc/cpuinfo | grep &quot;siblings&quot; | sort -u</span><br><span class="line"></span><br><span class="line">12</span><br></pre></td></tr></table></figure></p>
<h4 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a><strong>Cache</strong></h4><p>Cache 也一样通过 /proc/cpuinfo 查看：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">processor  : 0</span><br><span class="line"></span><br><span class="line">cache size  : 15360 KB</span><br><span class="line"></span><br><span class="line">cache_alignment  : 64</span><br></pre></td></tr></table></figure></p>
<p>不过这里的值 cache size 比较粗略，我们并不知道这个值是哪一级的 Cache 值（L1？L2？L3？），这种方法不能确定，我们换一种方法。</p>
<p>其实详细的 Cache 信息可以通过 <code>sysfs</code> 查看，如下：</p>
<p>比如查看 cpu0 的 cache 情况：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux # ls /sys/devices/system/cpu/cpu0/cache/</span><br><span class="line"></span><br><span class="line">index0/  index1/  index2/  index3/</span><br></pre></td></tr></table></figure></p>
<p>其中包含四个目录：<br>index0 存 L1 数据 Cache，index1 存 L1 指令 Cache，index2 存 L2 Cache，index3 存 L3 Cache。每个目录里面包含一堆描述 Cache 信息的文件。我们选 index0 具体看下：</p>
<center><img src="/images/linux/cache_info.jpg" alt=""></center>

<p>其中，shared_cpu_list 和 shared_cpu_map 表示意思是一样的，都表示该 cache 被哪几个 processor 共享。对 <strong>shared_cpu_map</strong> 具体解释一下。</p>
<p>这个值表面上看是二进制，但其实是 16 进制，每个数字有 4 个bit，代表 4 个 cpu。比如上面的 001001 拆开后是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0000 0000 0001 0000 0000 0001，1 bit 处即对应 cpu 标号，即 cpu0 和 cpu12。</span><br></pre></td></tr></table></figure></p>
<p>同样我们可以对其他 index 进行统计，可以得出：<br><strong>/proc/cpuinfo 中的 cache size 对应的 L3 Cache size</strong>。</p>
<p>最后，综合以上所有信息我们可以绘制出一下的 CPU 拓扑图：</p>
<center><img src="/images/linux/cpu_topo.jpg" alt=""></center>

<p>我们发现以上命令用得不太顺手，要获取多个数据需要输入多条命令，能不能一条命令就搞定，当然是有的，lscpu 就可以做到，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Linux # lscpu </span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                24   //共有24个逻辑CPU（threads）</span><br><span class="line">On-line CPU(s) list:   0-23</span><br><span class="line">Thread(s) per core:    2   //每个 Core 有 2 个 Threads</span><br><span class="line">Core(s) per socket:    12  //每个 Socket 有 12 个 Threads</span><br><span class="line">Socket(s):             2  //共有 2 个 Sockets</span><br><span class="line">NUMA node(s):          2  //共有 2 个 Nodes</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 63</span><br><span class="line">Stepping:              2</span><br><span class="line">CPU MHz:               2401.000</span><br><span class="line">BogoMIPS:             4803.16</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K  //L1 data cache 32k</span><br><span class="line">L1 cache:             32K   //L1 instruction cache 32k  </span><br><span class="line">L2 cache:              256K //L2 instruction cache 256k</span><br><span class="line">L3 cache:              15360K //L3 instruction cache 15M</span><br><span class="line">NUMA node0 CPU(s):     0-5,12-17</span><br><span class="line">NUMA node1 CPU(s):     6-11,18-23</span><br></pre></td></tr></table></figure></p>
<p>当然了，没有完美的命令，lscpu 也只能显示一些宽泛的信息，只是相对比较全面而已，更详细的信息，比如 Core 和 Cache 信息就得借助 cpuinfo 和 sysfs 了。</p>
<p>下面给大家提供一个脚本，能够比较直观的显示以上所有信息，有 shell 版的和 python 版的（不是我写的，文末附上了引用出处）。</p>
<p>大家有需要可以回复 “CPU” 获取，我就不贴出来了，显示的结果大概就是长下面这个样子：</p>
<p><strong>python 版：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">============================================================</span><br><span class="line">Core and Socket Information (as reported by &apos;/proc/cpuinfo&apos;)</span><br><span class="line">============================================================</span><br><span class="line"></span><br><span class="line">cores =  [0, 1, 2, 3, 4, 5]</span><br><span class="line"></span><br><span class="line">sockets =  [0, 1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Socket 0        Socket 1        </span><br><span class="line"></span><br><span class="line">--------        --------        </span><br><span class="line"></span><br><span class="line">Core 0 [0, 12]         [6, 18]         </span><br><span class="line"></span><br><span class="line">Core 1 [1, 13]         [7, 19]         </span><br><span class="line"></span><br><span class="line">Core 2 [2, 14]         [8, 20]         </span><br><span class="line"></span><br><span class="line">Core 3 [3, 15]         [9, 21]         </span><br><span class="line"></span><br><span class="line">Core 4 [4, 16]         [10, 22]        </span><br><span class="line"></span><br><span class="line">Core 5 [5, 17]         [11, 23]</span><br></pre></td></tr></table></figure></p>
<p>Reference：</p>
<ol>
<li>玩转 CPU 拓扑：<br><a href="http://blog.itpub.net/645199/viewspace-1421876/" target="_blank" rel="noopener">http://blog.itpub.net/645199/viewspace-1421876/</a></li>
<li>NUMA 体系结构详解<br><a href="https://blog.csdn.net/ustc_dylan/article/details/45667227（shell" target="_blank" rel="noopener">https://blog.csdn.net/ustc_dylan/article/details/45667227（shell</a> 代码引用）</li>
<li>dpdk 源代码（python 代码引用）</li>
</ol>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/CPU/"><i class="fas fa-hashtag fa-fw"></i>CPU</a>
                
                    <a href="/tags/Linux/"><i class="fas fa-hashtag fa-fw"></i>Linux</a>
                
                    <a href="/tags/NUMA/"><i class="fas fa-hashtag fa-fw"></i>NUMA</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/05/03/技术/CPU拓扑从SMP谈到NUMA（理论篇）/">
              
                  CPU 拓扑：从 SMP 谈到 NUMA （理论篇）
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-05-03
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Linux/">Linux</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>随着计算机技术（特别是以芯片为主的硬件技术）的快速发展，CPU 架构逐步从以前的单核时代进阶到如今的多核时代，在多核时代里，多颗 CPU 核心构成了一个小型的“微观世界”。每颗 CPU 各司其职，并与其他 CPU 交互，共同支撑起了一个“物理世界”。从这个意义上来看，我们更愿意称这样的“微观世界”为 CPU 拓扑，就像一个物理网络一样，各个网络节点通过拓扑关系建立起连接来支撑起整个通信系统。</p>
<center><img src="/images/linux/processor.jpg" alt=""></center>

<h3 id="单核-or-多核-or-多-CPU-or-超线程"><a href="#单核-or-多核-or-多-CPU-or-超线程" class="headerlink" title="单核 or 多核 or 多 CPU or 超线程 ?"></a>单核 or 多核 or 多 CPU or 超线程 ?</h3><hr>
<p>在单核时代，为了提升 CPU 的处理能力，普遍的做法是提高 CPU 的主频率，但一味地提高频率对于 CPU 的功耗也是影响很大的（CPU 的功耗正比于主频的三次方）。</p>
<p>另外一种做法是提高 IPC （每个时钟周期内执行的指令条数），这种做法要么是提高指令的并行度，要么是增加核数。显然，后一种方法更具有可扩展性，这也是摩尔定律的必然性。</p>
<p>CPU 的性能到底是如何体现的呢？为了弄清楚这个问题，我们结合单核 CPU 和多核 CPU 的结构来进一步剖析。</p>
<p>首先，对于一个单核结构，即一颗物理 CPU 封装里只集成了一个物理核心，其主要组件可以简化为：CPU 寄存器集合、中断逻辑、执行单元和 Cache，如下图：</p>
<center><img src="/images/linux/one_core.jpg" alt=""></center>

<p>对于一个多线程程序，主要通过时间片轮转的方式来获得 CPU 的执行权，从内部来看，这其实是串行执行的，性能自然并不怎么高。</p>
<p>其次，对于多核结构，则是在一颗物理 CPU 封装里集成了多个对等的物理核心，所谓对等，就是每个核心都有相同的内部结构。多个核心之间通过芯片内部总线来完成通信。随着 CPU 制造工艺的提升，每颗 CPU 封装中集成的物理核心也在不断提高。</p>
<center><img src="/images/linux/multi_core.jpg" alt=""></center>

<p>对于一个多线程程序，这种情况能够实现真正的并发，但线程在不同核之间切换会存在一定的开销，但由于走的是芯片内部总线，开销相对会比较小。</p>
<p>除了上述两种结构，还有另外一种结构是多 CPU 结构，也就是多颗单独封装的 CPU 通过外部总线相连，构成的一个统一的计算平台。每个 CPU 都需要独立的电路支持，有自己的 Cache。它们之间的通信通过主板上的总线来完成。</p>
<center><img src="/images/linux/multi_cpu.jpg" alt=""></center>

<p>同样对于一个多线程程序，不同于上一种情况的是，线程间的切换走的是外部总线，延迟较大，开销自然较大，而且对于有共享的数据还会因 Cache 一致性带来一定的开销（关于 Cache 下一小节说明）。</p>
<p>上述结构，一个 CPU 核心同一时间内只能执行一个线程，效率低下，为了提高 CPU 的利用率，CPU 芯片厂商又推出超线程（Hyper-Thread-ing）技术，即让一个物理核心同时执行多个线程，使整体性能得到提升。虽然物理上只有一个核心，但逻辑上被划分了多个逻辑核心，它们之间是完全隔离的。</p>
<center><img src="/images/linux/ht.jpg" alt=""></center>

<p>对于每个逻辑核心，拥有完整独立的寄存器集合和中断逻辑，共享执行单元和 Cache。由于是共享执行单元，所以对高 IPC 的应用，其性能提升有限。</p>
<h3 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h3><hr>
<p>Cache 是一种 SRAM （Static Random Access Memory，静态访问随机存储器）。出于成本和生产工艺考虑，一般将 Cache 分为三级。一级（L1）访问速度最快，但是容量最小，一般只有几十 KB；二级（L2）次之，一般有几百 KB 到几 MB 不等，三级（LLC，Last Level Cache）最慢，但是容量也最大，一般有几 MB 到几十 MB。</p>
<p>一级 Cache 又分为数据 Cache 和指令 Cache，顾名思义，数据 Cache 用来存数据，指令 Cache 用来存指令。下图是一个简单的 Cache 系统逻辑示意图。</p>
<center><img src="/images/linux/cache.jpg" alt=""></center>

<p>在多核结构中，每个物理核心都拥有独立的一级 Cache 和二级 Cache，而三级 Cache 是所有核心共享。这种共享需要解决的一个问题是公平地为每个核心分配 Cache 大小，避免 Cache 命中率低的问题。</p>
<p>对于现代计算机系统，说到 Cache，不得不提 TLB（Translation Look-aside Buffer） Cache。简单理解，如果说 Cache 存放的是内存中的内容，那么 TLB Cache 存放的是页表项。</p>
<p>为什么页表项需要用 Cache 存，原因当然是快。你可能觉得用三级 Cache 存就行了，为什么还要专门上 TLB Cache。</p>
<p>这里有两点考虑，一点是 TLB 采用基于内容的访问存储器 CAM，这种存储器能做到根据虚拟地址查询直接返回物理地址，效率极高，不需要像传统方式那样采用多级页表查询。另外一点是 Cache 的“淘汰”机制决定，Cache 会根据算法淘汰掉那些不常使用的内容，这对于页表这种需要频繁访问（每次程序寻址都要访问页表）的特性显然是矛盾的，所以就需要专门为页表提供一种特殊的 Cache，即 TLB Cache。</p>
<h3 id="SMP-or-NUMA-or-MPP"><a href="#SMP-or-NUMA-or-MPP" class="headerlink" title="SMP or NUMA or MPP?"></a>SMP or NUMA or MPP?</h3><hr>
<p>如果说前面咱们讨论的是 CPU 内部的“微观世界”，那么本节将跳出来，探讨一个系统级的“宏观世界”。</p>
<p>首先是 SMP，对称多处理器系统，指的是一种多个 CPU 处理器共享资源的电脑硬件架构，其中，每个 CPU 没有主从之分，地位平等，它们共享相同的物理资源，包括总线、内存、IO、操作系统等。每个 CPU 访问内存所用时间都是相同的，因此这种系统也被称为一致存储访问结构（UMA，Uniform Memory Access）。</p>
<center><img src="/images/linux/smp.jpg" alt=""></center>

<p>这种系统由于共享资源，不可避免地要加锁来解决资源竞争的问题，带来一定的性能开销，另外，扩展能力还非常有限，实验证明，SMP 系统最好的情况是有 2-4 个 CPU，适用于 PC、笔记本电脑和小型服务器等。</p>
<p>tips: 查看系统是否是 SMP 结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /sys/devices/system/node/     # 如果只看到一个 node0 那就是 SMP 架构</span><br></pre></td></tr></table></figure></p>
<p>为了应对大规模的系统要求（特别是云计算环境），就研制出了 NUMA 结构，即非一致存储访问结构。</p>
<p>这种结构引入了 CPU 分组的概念，用 Node 来表示，一个 Node 可能包含多个物理 CPU 封装，从而包含多个 CPU 物理核心。每个 Node 有自己独立的资源，包括内存、IO 等。每个 Node 之间可以通过互联模块总线（QPI）进行通信，所以，也就意味着每个 Node 上的 CPU 都可以访问到整个系统中的所有内存，但很显然，访问远端 Node 的内存比访问本地内存要耗时很多，这也是 NUMA 架构的问题所在，我们在基于 NUMA 架构开发上层应用程序要尽可能避免跨 Node 内存访问。</p>
<center><img src="/images/linux/numaarch.jpg" alt=""></center>

<p>NUMA 架构在 SMP 架构的基础上通过分组的方式增强了可扩展性，但从性能上看，随着 CPU 数量的增加，并不能线性增加系统性能，原因就在于跨 Node 内存访问的问题。所以，一般 NUMA 架构最多支持几百个 CPU 就不错了。</p>
<p>但对于很多大型计算密集型的系统来说，NUMA 显然有些吃力，所以，后来又出现了 MPP 架构，即海量并行处理架构。这种架构也有分组的概念，但和 NUMA 不同的是，它不存在异地内存访问的问题，每个分组内的 CPU 都有自己本地的内存、IO，并且不与其他 CPU 共享，是一种完全无共享的架构，因此它的扩展性最好，可以支持多达数千个 CPU 的量级。</p>
<center><img src="/images/linux/mpp.jpg" alt=""></center>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr>
<p>1、在芯片技术已然发展成熟的今天，性能低，上核不是问题，但上核就一定能提高性能吗，另外上核怎么很好地利用多核来完成自身进化，这些问题都值得深思。</p>
<p>2、NUMA 架构算是多核时代应用较大的一种 CPU 架构，本文从核心谈到系统，让大家有个全面的了解，下文会特别针对 NUMA 架构做一些实验验证。</p>
<p>Reference：<br>1.《深入浅出 DPDK》</p>
<ol start="2">
<li>SMP、NUMA、MPP体系结构介绍：<br><a href="https://www.cnblogs.com/yubo/archive/2010/04/23/1718810.html" target="_blank" rel="noopener">https://www.cnblogs.com/yubo/archive/2010/04/23/1718810.html</a></li>
</ol>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/CPU/"><i class="fas fa-hashtag fa-fw"></i>CPU</a>
                
                    <a href="/tags/Linux/"><i class="fas fa-hashtag fa-fw"></i>Linux</a>
                
                    <a href="/tags/NUMA/"><i class="fas fa-hashtag fa-fw"></i>NUMA</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/26/技术/一文掌握Linux性能分析之CPU篇/">
              
                  一文掌握 Linux 性能分析之 CPU 篇
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-26
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Linux/">Linux</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>平常工作会涉及到一些 Linux 性能分析的问题，因此决定总结一下常用的一些性能分析手段，仅供参考。</p>
<p>说到性能分析，基本上就是 CPU、内存、磁盘 IO 以及网络这几个部分，本文先来看 CPU 这个部分。</p>
<h3 id="1-CPU-基础信息"><a href="#1-CPU-基础信息" class="headerlink" title="1 CPU 基础信息"></a>1 CPU 基础信息</h3><hr>
<p>进行性能分析之前，首先得知道 CPU 有哪些信息，可以通过以下方法查看 CPU 配置信息。</p>
<h4 id="lscpu"><a href="#lscpu" class="headerlink" title="lscpu"></a><strong>lscpu</strong></h4><p>在 Linux 下，类似 lsxxx 这样的命令都是用来查看基本信息的，如 ls 查看当前目录文件信息，lscpu 就用来查看 CPU 信息，类似还有 lspci 查看 PCI 信息。</p>
<center><img src="/images/linux/lscpu.jpg" alt=""></center>

<p>可以看到我的机器配置很低，1 核 2.5GHz（在阿里云买的最低配的服务器）。</p>
<h4 id="proc-cpuinfo"><a href="#proc-cpuinfo" class="headerlink" title="/proc/cpuinfo"></a><strong>/proc/cpuinfo</strong></h4><p>/proc 目录是内核透传出来给用户态使用的，里面记录着很多信息文件，比如还有内存文件 meminfo 等。可以使用 cat /proc/cpuinfo 查看 CPU 信息。</p>
<center><img src="/images/linux/cpuinfo.jpg" alt=""></center>

<p>这里显示的信息可以具体到每个逻辑核上，由于我只有一个核，所以只显示一组信息。</p>
<h4 id="dmidecode"><a href="#dmidecode" class="headerlink" title="dmidecode"></a><strong>dmidecode</strong></h4><p>这个命令是用来获取 DMI（Desktop Management Interface）硬件信息的，包括 BIOS、系统、主板、处理器、内存、缓存等等。对于 CPU 信息，可以使用 dmidecode -t processor 来查看。</p>
<center><img src="/images/linux/dmi.jpg" alt=""></center>

<h3 id="2-CPU-使用情况分析"><a href="#2-CPU-使用情况分析" class="headerlink" title="2 CPU 使用情况分析"></a>2 CPU 使用情况分析</h3><hr>
<p>知道了 CPU 的基本信息，我们就可以使用另外的命令来对 CPU 的使用情况分析一通了。</p>
<h4 id="top"><a href="#top" class="headerlink" title="top"></a><strong>top</strong></h4><p>相信大家对下面这玩意不陌生，Windows 的任务管理器，top 的作用和它是一样的。</p>
<center><img src="/images/linux/top_w.jpg" alt=""></center>

<p>top 显示的效果虽说不像它这么华丽，但已然让人惊呼他俩怎么长得这么像。</p>
<center><img src="/images/linux/top.jpg" alt=""></center>

<p>我们重点关注这么几个字段：</p>
<ul>
<li>load average：三个数字分别表示最近 1 分钟，5 分钟和 15 分钟的负责，数值越大负载越重。一般要求不超过核数，比如对于单核情况要 &lt; 1。如果机器长期处于高于核数的情况，说明机器 CPU 消耗严重了。</li>
<li>%Cpu(s)：表示当前 CPU 的使用情况，如果要查看所有核（逻辑核）的使用情况，可以按下数字 “1” 查看。这里有几个参数，表示如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- us    用户空间占用 CPU 时间比例</span><br><span class="line">- sy    系统占用 CPU 时间比例</span><br><span class="line">- ni    用户空间改变过优先级的进程占用 CPU 时间比例</span><br><span class="line">- id    CPU 空闲时间比</span><br><span class="line">- wa    IO等待时间比（IO等待高时，可能是磁盘性能有问题了）</span><br><span class="line">- hi    硬件中断</span><br><span class="line">- si    软件中断</span><br><span class="line">- st    steal time</span><br></pre></td></tr></table></figure>
<p>每个进程的使用情况：这里可以罗列每个进程的使用情况，包括内存和 CPU 的，如果要看某个具体的进程，可以使用 <code>top -p pid</code> 查看。</p>
<p>和 top 一样的还有一个改进版的工具：htop，功能和 top 一样的，只不过比 top 表现更炫酷，使用更方便，可以看下它的效果。</p>
<center><img src="/images/linux/htop.jpg" alt=""></center>

<h4 id="ps"><a href="#ps" class="headerlink" title="ps"></a><strong>ps</strong></h4><p>可能很多人会忽略这个命令，觉得这不是查看进程状态信息的吗，其实非也，这个命令配合它的参数能显示很多功能。比如 ps aux。如果配合 watch，可以达到跟 top 一样的效果，如：<code>watch -n 1 &quot;ps aux&quot;</code>（-n 1 表示每隔 1s 更新一次）</p>
<center><img src="/images/linux/ps.jpg" alt=""></center>

<h4 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a><strong>vmstat</strong></h4><p>这个命令基本能看出当前机器的运行状态和问题，非常强大。可以使用 <code>vmstat n</code> 后面跟一个数字，表示每隔 ns 显示系统的状态，信息包括 CPU、内存和 IO 等。</p>
<center><img src="/images/linux/vmstat.jpg" alt=""></center>

<p>几个关键的字段：</p>
<ul>
<li>r 值：表示在 CPU 运行队列中等待的进程数，如果这个值很大，表示很多进程在排队等待执行，CPU 压力山大。</li>
<li>in 和 cs 值：表示中断次数和上下文切换次数，这两个值越大，表示系统在进行大量的进程（或线程）切换。切换的开销是非常大的，这时候应该减少系统进程（或线程）数。</li>
<li>us、sy、id、wa 值：这些值上面也提到过，分别表示用户空间进程，系统进程，空闲和 IO 等待的 CPU 占比，这里只有 id 很高是好的，表示系统比较闲，其他值飚高都不好。</li>
</ul>
<p>这个工具强大之处在于它不仅可以分析 CPU，还可以分析内存、IO 等信息，犹如瑞士军刀。</p>
<h4 id="dstat"><a href="#dstat" class="headerlink" title="dstat"></a><strong>dstat</strong></h4><p>这个命令也很强大，能显示 CPU 使用情况，磁盘 IO 情况，网络发包情况和换页情况，而且输出是彩色的，可读性比较强，相对于 vmstat 更加详细和直观。使用时可以直接输入命令，也可以带相关参数。</p>
<center><img src="/images/linux/dstat.jpg" alt=""></center>

<h3 id="3-进程使用-CPU-情况分析"><a href="#3-进程使用-CPU-情况分析" class="headerlink" title="3 进程使用 CPU 情况分析"></a>3 进程使用 CPU 情况分析</h3><hr>
<p>上面说的是系统级的分析，现在来看单个进程的 CPU 使用情况分析，以便于我们能对占用 CPU 过多的进程进行调试和分析，优化程序性能。</p>
<p>其实前面 top 和 ps 这样的命令就可以看每个进程的 CPU 使用情况，但我们需要更专业的命令。</p>
<h4 id="pidstat"><a href="#pidstat" class="headerlink" title="pidstat"></a><strong>pidstat</strong></h4><p>这个命令默认统计系统信息，也包括 CPU、内存和 IO 等，我们常用 pidstat -u -p pid [times] 来显示 CPU 统计信息。如下统计 pid = 802 的 CPU 信息。</p>
<center><img src="/images/linux/pidstat.jpg" alt=""></center>

<h4 id="strace"><a href="#strace" class="headerlink" title="strace"></a><strong>strace</strong></h4><p>这个命令用来分析进程的系统调用情况，可以看进程都调用了哪些库和哪些系统调用，进而可以进一步优化程序。比如我们分析 ls 的系统调用情况，就可以用 strace ls：</p>
<center><img src="/images/linux/strace.jpg" alt=""></center>

<p>可以看到，一个简单的 ls 命令，其实有不少系统调用的操作。</p>
<p>此外，还可以 attach（附着）到一个正在运行的进程上进行分析，比如我 attach 到 802 这个进程显示：</p>
<center><img src="/images/linux/stracep.jpg" alt=""></center>

<p>根据这些输出信息，其实就能够很好地帮我们分析问题，从而定位到问题所在了。</p>
<p>OK，以上就是平常比较常用的一些工具，当然除了这些，还有很多很多工具，下面放一张图，来自 Linux 大牛，Netflix 高级性能架构师 Brendan Gregg。看完了，你也许会感叹“这世界太疯狂了（just crazy）”。</p>
<center><img src="/images/linux/tools.jpg" alt=""></center>

<p>Reference：<br>[1]. <a href="http://rdc.hundsun.com/portal/article/731.html" target="_blank" rel="noopener">http://rdc.hundsun.com/portal/article/731.html</a></p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/CPU/"><i class="fas fa-hashtag fa-fw"></i>CPU</a>
                
                    <a href="/tags/Linux/"><i class="fas fa-hashtag fa-fw"></i>Linux</a>
                
                    <a href="/tags/性能分析/"><i class="fas fa-hashtag fa-fw"></i>性能分析</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/20/技术/各种容器网络方案对比/">
              
                  各种容器网络方案对比
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-20
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Docker/">Docker</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>前面单主机容器网络和多主机容器网络两篇文章，咱们已经从原理上总结了多种容器网络方案，也通过这篇文章探讨了容器网络的背后原理。本文再基于一个宏观的视角，对比几种网络方案，让大家有个完整的认识。</p>
<p>单主机网络就不多说了，因为也比较简单，我们重点对比几种多主机网络方案。</p>
<p>对比的维度有以下几种：网络模型，IP 地址池管理（IP Address Management，IPAM），服务发现，连通与隔离，性能。</p>
<p>网络模型指的是构成跨主机通信的网络结构和实现技术，比如是纯二层转发，还是纯三层转发；是 overlay 网络还是 underlay 网络等等。</p>
<p>IPAM 指的是如何管理容器网络的 IP 池。当容器集群比较大，管理的主机比较多的时候，如何分配各个主机上容器的 IP 是一个比较棘手的问题。Docker 网络有个 subnet 的概念，通常一个主机分配一个 subnet，但也有多个主机共用一个 subnet 的情况，具体的网络方案有不同的考量。具体看下面的表格总结。</p>
<p>服务发现本质上是一个分布式的 key-value 存储系统，用于跨主机通信时保存并同步各主机的网络信息，便于快速建立起各主机之间的网络连接。由于各网络方案实现上各有千秋，并不是所有的跨主机网络方案都要依据服务发现。</p>
<p>连通与隔离指的是容器跨主机之间是否能够互相通信，以及容器与外网（外网不一定指 Internet）之间如何通信。</p>
<p>性能具体指的是通信的时延，我们仅从各个网络方案的原理上来分析得出结论，所以这里的结论并不一定正确，因为不同的部署环境会对性能有一些影响，建议大家还是根据自己的环境动手实验验证为妙。</p>
<p>从原理上说，underlay 网络性能要优于 overlay 网络，因为 overlay 网络存在封包和拆包操作，存在额外的 CPU 和网络开销，所以，几种方案中，macvlan、flannel host-gw、calico 的性能会优于 overlay、flannel vxlan 和 weave。但是这个也不能作为最终生产环境采用的标准，因为 overlay 网络采用 vxlan 作为隧道的话，能支持更多的二层网段，安全性也更高，所以，需要综合考虑。</p>
<p>通过以上分析，我们可以得出以下的结论：</p>
<center><img src="/images/docker/docker_net_com.jpg" alt=""></center>

<p>参考：<br><a href="http://www.cnblogs.com/CloudMan6/p/7587532.html" target="_blank" rel="noopener">http://www.cnblogs.com/CloudMan6/p/7587532.html</a></p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/容器/"><i class="fas fa-hashtag fa-fw"></i>容器</a>
                
                    <a href="/tags/Docker/"><i class="fas fa-hashtag fa-fw"></i>Docker</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
                    <a href="/tags/容器网络/"><i class="fas fa-hashtag fa-fw"></i>容器网络</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/18/技术/那么多容器网络的解决方案，其背后的原理到底是什么？/">
              
                  那么多容器网络的解决方案，其背后的原理到底是什么？
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-18
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Docker/">Docker</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>知其然而不知其所以然，不知也。老古人说得多好，学知识不懂得知识背后的原理，等于白学。</p>
<p>通过前面两篇文章，我们知道了容器的单主机网络和多主机网络，对于这么多网络方案，我们看到对 Docker 的整体网络结构好像没有改动，都是水平扩展的，那 Docker 网络究竟是怎么集成这么多网络方案而不改变自身原有的结构呢？本文就来一探究竟。</p>
<h3 id="1-Docker-的总体框架"><a href="#1-Docker-的总体框架" class="headerlink" title="1 Docker 的总体框架"></a>1 Docker 的总体框架</h3><hr>
<p>要回答这个问题，得从 Docker 的总体框架说起。</p>
<center><img src="/images/docker/docker_netarch.jpg" alt=""></center>

<p>容器和虚拟机一样，都是虚拟化的产品，都包括计算虚拟化，存储虚拟化和 IO 虚拟化。容器作为轻量级的进程，不像虚拟机那般复杂，这三块分别靠三个 Driver 来完成的，execdriver 负责计算虚拟化，networkdriver 负责网络虚拟化，graphdriver 负责存储虚拟化。由此可见，Docker 靠 Driver 这种设计思想来支撑起它的基础平台，再往深了挖，它的每个子模块都随处可见这种设计思想，就网络这个子模块来看，也是如此。</p>
<h3 id="2-Docker-的网络模型"><a href="#2-Docker-的网络模型" class="headerlink" title="2 Docker 的网络模型"></a>2 Docker 的网络模型</h3><hr>
<h4 id="docker-engine-libcontainer"><a href="#docker-engine-libcontainer" class="headerlink" title="docker engine + libcontainer"></a>docker engine + libcontainer</h4><hr>
<p>期初的 Docker 网络子模块的代码是分散在 docker daemon 和 libcontainer 中的，libcontainer 是一个独立的容器管理包，execdriver 和 networkdriver 都是通过 libcontainer 来实现对容器的具体操作。</p>
<p>随着业务场景越来越复杂，这种内嵌的方式很难针对不同的网络场景进行扩展。后来，Docker 收购了一个做多主机网络解决方案的公司 SocketPlane，然后让那帮人专门来解决这个问题。这就是接下来要介绍的 libnetwork。</p>
<h4 id="libnetwork-amp-amp-CNM"><a href="#libnetwork-amp-amp-CNM" class="headerlink" title="libnetwork &amp;&amp; CNM"></a>libnetwork &amp;&amp; CNM</h4><hr>
<p>libnetwork 起初的做法很简单，就是将 docker engine 和 libcontainer 中网络相关的代码抽出来，合并成一个单独的库，做成网络抽象层，并对外提供 API。Docker 的愿景就是希望 libnetwork 能够做像 libcontainer 那样，成为一个多平台的容器网络基础包。</p>
<p>后来受一个 GitHub issue ( <a href="https://github.com/moby/moby/issues/9983" target="_blank" rel="noopener">https://github.com/moby/moby/issues/9983</a>) 的启发，libnetwork 引入容器网络模型（Container Network Model，CNM），该模型进一步对 Docker 的网络结构进行了细分，提出了三个概念：network、sandbox 和 endpoint。</p>
<h5 id="network"><a href="#network" class="headerlink" title="network"></a><strong>network</strong></h5><p>network 是一个抽象的概念，你可以把它理解成一个网络的插件，或者是网络的 Driver，比如说单主机网络的 Driver 就有 none、host、bridge，joined container 这四种，多主机网络就有 overlay、macvlan、flannel 这些。network 可以独立出去做，只需调用 Docker 对外提供的 API 就可以作为插件集成到 Docker 网络中使用。</p>
<h5 id="sandbox"><a href="#sandbox" class="headerlink" title="sandbox"></a><strong>sandbox</strong></h5><p>sandbox 实现了容器内部的网络栈，它定义容器的虚拟网卡，路由表和 DNS 等配置，其实就是一个标准的 linux network namespace 实现。</p>
<h5 id="endpoint"><a href="#endpoint" class="headerlink" title="endpoint"></a><strong>endpoint</strong></h5><p>network 实现了一个第三方的网络栈，sandbox 则实现了容器内部的网络栈，那这两者怎么联系起来呢？答案就是通过 endpoint，endpoint 实现了 veth pair，一个 endpoint 就表示一对 veth pair，一端挂在容器中，另一端挂在 network 中。</p>
<center><img src="/images/docker/cnm.jpg" alt=""></center>

<p>network、sandbox 和 endpoint 三者之间的关系：<br>一个 network 可以包含多个 endpoint，自然也就包含多个 sandbox。<br>一个 sandbox 可以包含多个 endpoint，可以属于多个 network。<br>一个 endpoint 只可以属于一个 network，并且只属于一个 sandbox。</p>
<p>如上图显示三个容器，每个容器一个 sandbox，除了第二个容器有两个 endpoint 分别接入 network1 和 network2 之外，其余 sandbox 都只有一个 endpoint 分别接入不同的 network。</p>
<p>到此，我们就可以解答文章开篇提到的问题，“不同的网络方案如何集成到 Docker 网络模型中而不改变原有结构？”</p>
<p>答案就是基于 libnetwork CNM，将各个网络模型以插件或 Driver 的形式集成到 Docker 网络中来，与 docker daemon 协同工作，实现容器网络。Docker 原生的 Driver 包括单主机的 none、bridge、joined container 和 多主机的 overlay、macvlan，第三方 Driver 就包括多主机的 flannel、weave、calico 等。</p>
<center><img src="/images/docker/libnetwork.png" alt=""></center>

<h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h3><hr>
<ol>
<li>libnetwork 基于 CNM 模型将 Docker 网络结构从原生方案中抽离出来，增强了网络扩展性，以至于现在各种网络方案层出不穷，都可以轻松集成到 Docker 中。</li>
<li>network，sandbox，endpoint 三个概念。</li>
</ol>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/容器/"><i class="fas fa-hashtag fa-fw"></i>容器</a>
                
                    <a href="/tags/Docker/"><i class="fas fa-hashtag fa-fw"></i>Docker</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
                    <a href="/tags/容器网络/"><i class="fas fa-hashtag fa-fw"></i>容器网络</a>
                
                    <a href="/tags/libnetwork/"><i class="fas fa-hashtag fa-fw"></i>libnetwork</a>
                
                    <a href="/tags/CNM/"><i class="fas fa-hashtag fa-fw"></i>CNM</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/16/技术/容器网络之多主机网络/">
              
                  容器网络之多主机网络
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-16
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Docker/">Docker</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>上篇文章介绍了容器网络的单主机网络，本文将进一步介绍多主机网络，也就是跨主机的网络。总结下来，多主机网络解决方案包括但不限于以下几种：overlay、macvlan、flannel、weave、cacico 等，下面将分别一一介绍这几种网络，</p>
<p>PS：本文仅从原理上对几种网络进行简单的对比总结，不涉及太多的细节。</p>
<h3 id="1-overlay"><a href="#1-overlay" class="headerlink" title="1 overlay"></a>1 overlay</h3><hr>
<p>俗称隧道网络，它是基于 VxLAN 协议来将二层数据包封装到 UDP 中进行传输的，目的是扩展二层网段，因为 VLAN 使用 12bit 标记 VLAN ID，最多支持 4094 个 VLAN，这对于大型云网络会成为瓶颈，而 VxLAN ID 使用 24bit 来标记，支持多达 16777216 个二层网段，所以 VxLAN 是扩展了 VLAN，也叫做大二层网络。</p>
<p>overlay 网络需要一个全局的“上帝”来记录它网络中的信息，比如主机地址，子网等，这个上帝在 Docker 中是由服务发现协议来完成的，服务发现本质上是一个 key-value 数据库，要使用它，首先需要向它告知（注册）一些必要的信息（如网络中需要通信的主机），然后它就会自动去收集、同步网络的信息，同时，还会维护一个 IP 地址池，分配给主机中的容器使用。Docker 中比较有名的服务发现有 Consul、Etcd 和 ZooKeeper。overlay 网络常用 Consul。</p>
<center><img src="/images/docker/overlay.jpg" alt=""></center>

<p>创建 overlay 网络会创建一个 Linux bridge br0，br0 会创建两个接口，一个 veth2 作为与容器的虚拟网卡相连的 veth pair，另一个 vxlan1 负责与其他 host 建立 VxLAN 隧道，跨主机的容器就通过这个隧道来进行通信。</p>
<p>为了保证 overlay 网络中的容器与外网互通，Docker 会创建另一个 Linux bridge docker_gwbridge，同样，该 bridge 也存在一对 veth pair，要与外围通信的容器可以通过这对 veth pair 到达 docker_gwbridge，进而通过主机 NAT 访问外网。</p>
<h3 id="2-macvlan"><a href="#2-macvlan" class="headerlink" title="2 macvlan"></a>2 macvlan</h3><hr>
<p>macvlan 就如它的名字一样，是一种网卡虚拟化技术，它能够将一个物理网卡虚拟出多个接口，每个接口都可以配置 MAC 地址，同样每个接口也可以配自己的 IP，每个接口就像交换机的端口一样，可以为它划分 VLAN。</p>
<p>macvlan 的做法其实就是将这些虚拟出来的接口与 Docker 容器直连来达到通信的目的。一个 macvlan 网络对应一个接口，不同的 macvlan 网络分配不同的子网，因此，相同的 macvlan 之间可以互相通信，不同的 macvlan 网络之间在二层上不能通信，需要借助三层的路由器才能完成通信，如下，显示的就是两个不同的 macvlan 网络之间的通信流程。</p>
<center><img src="/images/docker/macvlan.jpg" alt=""></center>

<p>我们用一个 Linux 主机，通过配置其路由表和 iptables，将其配成一个路由器（当然是虚拟的），就可以完成不同 macvlan 网络之间的数据交换，当然用物理路由器也是没毛病的。</p>
<h3 id="3-flannel"><a href="#3-flannel" class="headerlink" title="3 flannel"></a>3 flannel</h3><hr>
<p>flannel 网络也需要借助一个全局的上帝来同步网络信息，一般使用的是 etcd。</p>
<p>flannel 网络不会创建新的 bridge，而是用默认的 docker0，但创建 flannel 网络会在主机上创建一个虚拟网卡，挂在 docker0 上，用于跨主机通信。</p>
<center><img src="/images/docker/flannel.jpg" alt=""></center>

<p>组件方式让 flannel 多了几分灵活性，它可以使用二层的 VxLAN 隧道来封装数据包完成跨主机通信，也可以使用纯三层的方案来通信，比如 host-gw，只需修改一个配置文件就可以完成转化。</p>
<h3 id="4-weave"><a href="#4-weave" class="headerlink" title="4 weave"></a>4 weave</h3><hr>
<p>weave 网络没有借助服务发现协议，也没有 macvlan 那样的虚拟化技术，只需要在不同主机上启动 weave 组件就可以完成通信。</p>
<p>创建 weave 网络会创建两个网桥，一个是 Linux bridge weave，一个是 datapath，也就是 OVS，weave 负责将容器加入 weave 网络中，OVS 负责将跨主机通信的数据包封装成 VxLAN 包进行隧道传输。</p>
<center><img src="/images/docker/weave.jpg" alt=""></center>

<p>同样，weave 网络也不支持与外网通信，Docker 提供 docker0 来满足这个需求。</p>
<p>weave 网络通过组件化的方式使得网络分层比较清晰，两个网桥的分工也比较明确，一个用于跨主机通信，相当于一个路由器，一个负责将本地网络加入 weave 网络。</p>
<h3 id="5-calico"><a href="#5-calico" class="headerlink" title="5 calico"></a>5 calico</h3><hr>
<p>calico 是一个纯三层的网络，它没有创建任何的网桥，它之所以能完成跨主机的通信，是因为它记住 etcd 将网络中各网段的路由信息写进了主机中，然后创建的一对的 veth pair，一块留在容器的 network namespace 中，一块成了主机中的虚拟网卡，加入到主机路由表中，从而打通不同主机中的容器通信。</p>
<center><img src="/images/docker/calico.jpg" alt=""></center>

<p>calico 相较其他几个网络方案最大优点是它提供 policy 机制，用户可以根据自己的需求自定义 policy，一个 policy 可能对应一条 ACL，用于控制进出容器的数据包，比如我们建立了多个 calico 网络，想控制其中几个网络可以互通，其余不能互通，就可以修改 policy 的配置文件来满足要求，这种方式大大增加了网络连通和隔离的灵活性。</p>
<h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h3><hr>
<p>1、除了以上的几种方案，跨主机容器网络方案还有很多，比如：Romana，Contiv 等，本文就不作过多展开了，大家感兴趣可以查阅相关资料了解。</p>
<p>2、跨主机的容器网络通常要为不同主机的容器维护一个 IP 池，所以大多方案需要借助第三方的服务发现方案。</p>
<p>3、跨主机容器网络按传输方式可以分为纯二层网络，隧道网络（大二层网络），以及纯三层网络。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/容器/"><i class="fas fa-hashtag fa-fw"></i>容器</a>
                
                    <a href="/tags/Docker/"><i class="fas fa-hashtag fa-fw"></i>Docker</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
                    <a href="/tags/容器网络/"><i class="fas fa-hashtag fa-fw"></i>容器网络</a>
                
                    <a href="/tags/overlay/"><i class="fas fa-hashtag fa-fw"></i>overlay</a>
                
                    <a href="/tags/macvlan/"><i class="fas fa-hashtag fa-fw"></i>macvlan</a>
                
                    <a href="/tags/flannel/"><i class="fas fa-hashtag fa-fw"></i>flannel</a>
                
                    <a href="/tags/weave/"><i class="fas fa-hashtag fa-fw"></i>weave</a>
                
                    <a href="/tags/cacico/"><i class="fas fa-hashtag fa-fw"></i>cacico</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/11/技术/virtio_user_简介/">
              
                  virtio-user 简介
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-11
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/DPDK/">DPDK</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>在看本文之前，建议先看 virtio 简介，vhost 简介，vhost-user 简介。</p>
<p>virtio-user 是 DPDK 针对特定场景提出的一种解决方案，它主要有两种场景的用途，一种是用于 DPDK 应用容器对 virtio 的支持，这是 DPDK v16.07 开始支持的；另一种是用于和内核通信，这是 DPDK v17.02 推出的。</p>
<h3 id="1-virtio-user-用于容器网络"><a href="#1-virtio-user-用于容器网络" class="headerlink" title="1 virtio_user 用于容器网络"></a>1 virtio_user 用于容器网络</h3><hr>
<p>我们知道，对于虚拟机，有 virtio 这套半虚拟化的标准协议来指导虚拟机和宿主机之间的通信，但对于容器的环境，直接沿用 virtio 是不行的，原因是虚拟机是通过 Qemu 来模拟的，Qemu 会将它虚拟出的整个 KVM 虚拟机的信息共享给宿主机，但对于 DPDK 加速的容器化环境来说显然是不合理的。因为 DPDK 容器与宿主机的通信只用得到虚拟内存中的大页内存部分，其他都是用不到的，全部共享也没有任何意义，DPDK 主要基于大页内存来收发数据包的。</p>
<p>所以，virtio_user 其实就是在 virtio PMD 的基础上进行了少量修改形成的，简单来说，就是添加大页共享的部分逻辑，并精简了整块共享内存部分的逻辑。</p>
<p>有兴趣可以对照 /driver/net/virtio 中的代码和 DPDK virtio_user 代码，其实大部分是相同的。</p>
<p>从 DPDK 的角度看，virtio_user 是作为一个虚拟设备（vdev）来加载的，它充当的是一个 virtio 前端驱动，与之对应的后端通信驱动，是用户态的 vhost_user，在使用的时候，我们只需要定义好相应的适配接口即可，如下：</p>
<center><img src="/images/docker/virtio_user.jpg" alt=""></center>

<p>vhost 和 vhost_user 本质上是采用共享内存的 IPC 方式，通过在 host 端创建 vhost_user 共享内存文件，然后 virtio_user 启动的时候指定该文件即可，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）首先创建 vhost_user 共享内存文件</span><br><span class="line">--vdev &apos;eth_vhost_user0,iface=/tmp/vhost_user0&apos;</span><br><span class="line">2）启动 virtio_user 指定文件路径</span><br><span class="line">--vdev=virtio_user0,path=/tmp/vhost_user0</span><br></pre></td></tr></table></figure></p>
<h3 id="2-virtio-user-作为-exception-path-用于与内核通信"><a href="#2-virtio-user-作为-exception-path-用于与内核通信" class="headerlink" title="2 virtio_user 作为 exception path 用于与内核通信"></a>2 virtio_user 作为 exception path 用于与内核通信</h3><hr>
<p>virtio_user 的一个用途就是作为 exception path 用于与内核通信。我们知道，DPDK 是旁路内核的转包方案，这也是它高性能的原因，但有些时候从 DPDK 收到的包（如控制报文）需要丢到内核网络协议栈去做进一步的处理，这个路径在 DPDK 中就被称为 exception path。</p>
<p>在这之前，已经存在几种 exception path 的方案，如传统的 Tun/Tap，KNI（Kernel NIC Interface），AF_Packet 以及基于 SR-IOV 的 Flow Bifurcation。这些方案就不做过多介绍了，感兴趣的可看 DPDK 官网，上面都有介绍。</p>
<center><img src="/images/docker/virtio_user1.jpg" alt=""></center>

<p>和容器网络的方案使用 vhost_user 作为后端驱动一样，要使得 virtio_user 和内核通信，只需加载内核模块 vhost.ko，让它充当的是 virtio_user 的后端通信驱动即可。</p>
<p>所以，我们可以看到，其实这两种方案本质上是一样，只是换了个后端驱动而已，这也是 virtio 的优势所在，定义一套通用的接口标准，需要什么类型的通信方式只需加载相应驱动即可，改动非常少，扩展性非常高。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/虚拟化/"><i class="fas fa-hashtag fa-fw"></i>虚拟化</a>
                
                    <a href="/tags/DPDK/"><i class="fas fa-hashtag fa-fw"></i>DPDK</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
                    <a href="/tags/容器网络/"><i class="fas fa-hashtag fa-fw"></i>容器网络</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/10/技术/DPDK_入门最佳指南/">
              
                  DPDK 入门最佳指南
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-10
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/DPDK/">DPDK</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<h3 id="01-写在前面"><a href="#01-写在前面" class="headerlink" title="01 写在前面"></a>01 写在前面</h3><hr>
<p>我的读者当中应该有一部分人是做 DPDK 相关的，我自己虽然现在已经不做 DPDK 了，但对这块仍然有兴趣，今天这篇文章就来总结下 DPDK 的技术栈。注意：这篇文章是小白文，不适合大神哦。</p>
<p>文章从 DPDK 的产生背景，到核心技术，再到应用场景，都进行了阐述，有可能是你见过的讲得最全面的文章了，当然，讲得全面自然会少了深度，你如果不屑忽略就好了。</p>
<p>其实，本文之前已经发过，但在整理的时候不小心删了，索性就重发一次，但这一次多了一些内容，文末我还会推荐一些学习资料，有需要的可以拉到最下面查看获取方法。</p>
<h3 id="02-高性能网络技术"><a href="#02-高性能网络技术" class="headerlink" title="02 高性能网络技术"></a>02 高性能网络技术</h3><hr>
<p>随着云计算产业的异军突起，网络技术的不断创新，越来越多的网络设备基础架构逐步向基于通用处理器平台的架构方向融合，从传统的物理网络到虚拟网络，从扁平化的网络结构到基于 SDN 分层的网络结构，无不体现出这种创新与融合。</p>
<p>这在使得网络变得更加可控制和成本更低的同时，也能够支持大规模用户或应用程序的性能需求，以及海量数据的处理。究其原因，其实是高性能网络编程技术随着网络架构的演进不断突破的一种必然结果。</p>
<h3 id="03-C10K-到-C10M-问题的演进"><a href="#03-C10K-到-C10M-问题的演进" class="headerlink" title="03 C10K 到 C10M 问题的演进"></a>03 C10K 到 C10M 问题的演进</h3><hr>
<p>说到高性能网络编程，一定逃不过 C10K 问题（即单机 1 万个并发连接问题），不过这个问题已经成为历史了，很多技术可以解决它，如常用的 I/O 多路复用模型，select, poll, epoll 等。在此基础上也出了很多优秀的框架，比如 Nginx 基于事件驱动的 Web 服务框架，以及基于 Python 开发的 Tornado 和 Django 这种非阻塞的 Web 框架。</p>
<p>如今，关注的更多是 C10M 问题（即单机 1 千万个并发连接问题）。很多计算机领域的大佬们从硬件上和软件上都提出了多种解决方案。从硬件上，比如说，现在的类似很多 40Gpbs、32-cores、256G RAM 这样配置的 X86 服务器完全可以处理 1 千万个以上的并发连接。</p>
<p>但是从硬件上解决问题就没多大意思了，首先它成本高，其次不通用，最后也没什么挑战，无非就是堆砌硬件而已。所以，抛开硬件不谈，我们看看从软件上该如何解决这个世界难题呢？</p>
<p>这里不得不提一个人，就是 Errata Security 公司的 CEO Robert Graham，他在 Shmoocon 2013 大会上很巧妙地解释了这个问题。有兴趣可以查看其 YouTube 的演进视频： C10M Defending The Internet At Scale。</p>
<center><img src="/images/sdn/c10m.jpg" alt=""></center>

<p>他提到了 UNIX 的设计初衷其实为电话网络的控制系统而设计的，而不是一般的服务器操作系统，所以，它仅仅是一个负责数据传送的系统，没有所谓的控制层面和数据层面的说法，不适合处理大规模的网络数据包。最后他得出的结论是：</p>
<blockquote>
<p>OS 的内核不是解决 C10M 问题的办法，恰恰相反 OS 的内核正式导致 C10M 问题的关键所在。</p>
</blockquote>
<h3 id="04-为什么这么说？基于-OS-内核的数据传输有什么弊端？"><a href="#04-为什么这么说？基于-OS-内核的数据传输有什么弊端？" class="headerlink" title="04 为什么这么说？基于 OS 内核的数据传输有什么弊端？"></a>04 为什么这么说？基于 OS 内核的数据传输有什么弊端？</h3><hr>
<p><strong>1、中断处理：</strong> 当网络中大量数据包到来时，会产生频繁的硬件中断请求，这些硬件中断可以打断之前较低优先级的软中断或者系统调用的执行过程，如果这种打断频繁的话，将会产生较高的性能开销。</p>
<p><strong>2、内存拷贝：</strong> 正常情况下，一个网络数据包从网卡到应用程序需要经过如下的过程：数据从网卡通过 DMA 等方式传到内核开辟的缓冲区，然后从内核空间拷贝到用户态空间，在 Linux 内核协议栈中，这个耗时操作甚至占到了数据包整个处理流程的 57.1%。</p>
<p><strong>3、上下文切换：</strong> 频繁到达的硬件中断和软中断都可能随时抢占系统调用的运行，这会产生大量的上下文切换开销。另外，在基于多线程的服务器设计框架中，线程间的调度也会产生频繁的上下文切换开销，同样，锁竞争的耗能也是一个非常严重的问题。</p>
<p><strong>4、局部性失效：</strong> 如今主流的处理器都是多个核心的，这意味着一个数据包的处理可能跨多个 CPU 核心，比如一个数据包可能中断在 cpu0，内核态处理在 cpu1，用户态处理在 cpu2，这样跨多个核心，容易造成 CPU 缓存失效，造成局部性失效。如果是 NUMA 架构，更会造成跨 NUMA 访问内存，性能受到很大影响。</p>
<p><strong>5、内存管理：</strong> 传统服务器内存页为 4K，为了提高内存的访问速度，避免 cache miss，可以增加 cache 中映射表的条目，但这又会影响 CPU 的检索效率。</p>
<p>综合以上问题，可以看出内核本身就是一个非常大的瓶颈所在。那很明显解决方案就是想办法绕过内核。</p>
<h3 id="05-解决方案探讨"><a href="#05-解决方案探讨" class="headerlink" title="05 解决方案探讨"></a>05 解决方案探讨</h3><hr>
<p>针对以上弊端，分别提出以下技术点进行探讨。</p>
<p><strong>1、控制层和数据层分离：</strong> 将数据包处理、内存管理、处理器调度等任务转移到用户空间去完成，而内核仅仅负责部分控制指令的处理。这样就不存在上述所说的系统中断、上下文切换、系统调用、系统调度等等问题。</p>
<p><strong>2、多核技术：</strong> 使用多核编程技术代替多线程技术，并设置 CPU 的亲和性，将线程和 CPU 核进行一比一绑定，减少彼此之间调度切换。</p>
<p><strong>3、NUMA 亲和性：</strong> 针对 NUMA 系统，尽量使 CPU 核使用所在 NUMA 节点的内存，避免跨内存访问。</p>
<p><strong>4、大页内存：</strong> 使用大页内存代替普通的内存，减少 cache-miss。</p>
<p><strong>5、无锁技术：</strong> 采用无锁技术解决资源竞争问题。</p>
<p>经研究，目前业内已经出现了很多优秀的集成了上述技术方案的高性能网络数据处理框架，如 6wind、Windriver、Netmap、DPDK 等，其中，Intel 的 DPDK 在众多方案脱颖而出，一骑绝尘。</p>
<center><img src="/images/sdn/dpdk.png" alt=""></center>

<p>DPDK 为 Intel 处理器架构下用户空间高效的数据包处理提供了库函数和驱动的支持，它不同于 Linux 系统以通用性设计为目的，而是专注于网络应用中数据包的高性能处理。</p>
<p>也就是 DPDK 绕过了 Linux 内核协议栈对数据包的处理过程，在用户空间实现了一套数据平面来进行数据包的收发与处理。在内核看来，DPDK 就是一个普通的用户态进程，它的编译、连接和加载方式和普通程序没有什么两样。</p>
<h3 id="06-DPDK-的突破"><a href="#06-DPDK-的突破" class="headerlink" title="06 DPDK 的突破"></a>06 DPDK 的突破</h3><hr>
<p>相对传统的基于内核的网络数据处理，DPDK 对从内核层到用户层的网络数据流程进行了重大突破，我们先看看传统的数据流程和 DPDK 中的网络流程有什么不同。</p>
<p><strong>传统 Linux 内核网络数据流程：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">硬件中断---&gt;取包分发至内核线程---&gt;软件中断---&gt;内核线程在协议栈中处理包---&gt;处理完毕通知用户层</span><br><span class="line">用户层收包--&gt;网络层---&gt;逻辑层---&gt;业务层</span><br></pre></td></tr></table></figure></p>
<p><strong>dpdk 网络数据流程：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">硬件中断---&gt;放弃中断流程</span><br><span class="line">用户层通过设备映射取包---&gt;进入用户层协议栈---&gt;逻辑层---&gt;业务层</span><br></pre></td></tr></table></figure></p>
<p>下面就具体看看 DPDK 做了哪些突破？</p>
<h4 id="6-1-UIO-（用户空间的-I-O-技术）的加持。"><a href="#6-1-UIO-（用户空间的-I-O-技术）的加持。" class="headerlink" title="6.1 UIO （用户空间的 I/O 技术）的加持。"></a><strong>6.1 UIO （用户空间的 I/O 技术）的加持。</strong></h4><hr>
<p>DPDK 能够绕过内核协议栈，本质上是得益于 UIO 技术，通过 UIO 能够拦截中断，并重设中断回调行为，从而绕过内核协议栈后续的处理流程。</p>
<p>UIO 设备的实现机制其实是对用户空间暴露文件接口，比如当注册一个 UIO 设备 uioX，就会出现文件 /dev/uioX，对该文件的读写就是对设备内存的读写。除此之外，对设备的控制还可以通过 /sys/class/uio 下的各个文件的读写来完成。</p>
<center><img src="/images/sdn/uio.jpg" alt=""></center>

<h4 id="6-2-内存池技术"><a href="#6-2-内存池技术" class="headerlink" title="6.2 内存池技术"></a><strong>6.2 内存池技术</strong></h4><hr>
<p>DPDK 在用户空间实现了一套精巧的内存池技术，内核空间和用户空间的内存交互不进行拷贝，只做控制权转移。这样，当收发数据包时，就减少了内存拷贝的开销。</p>
<h4 id="6-3-大页内存管理"><a href="#6-3-大页内存管理" class="headerlink" title="6.3 大页内存管理"></a><strong>6.3 大页内存管理</strong></h4><hr>
<p>DPDK 实现了一组大页内存分配、使用和释放的 API，上层应用可以很方便使用 API 申请使用大页内存，同时也兼容普通的内存申请。</p>
<h4 id="6-4-无锁环形队列"><a href="#6-4-无锁环形队列" class="headerlink" title="6.4 无锁环形队列"></a><strong>6.4 无锁环形队列</strong></h4><hr>
<p>DPDK 基于 Linux 内核的无锁环形缓冲 kfifo 实现了自己的一套无锁机制。支持单生产者入列/单消费者出列和多生产者入列/多消费者出列操作，在数据传输的时候，降低性能的同时还能保证数据的同步。</p>
<h4 id="6-5-poll-mode-网卡驱动"><a href="#6-5-poll-mode-网卡驱动" class="headerlink" title="6.5 poll-mode 网卡驱动"></a><strong>6.5 poll-mode 网卡驱动</strong></h4><hr>
<p>DPDK 网卡驱动完全抛弃中断模式，基于轮询方式收包，避免了中断开销。</p>
<h4 id="6-6-NUMA"><a href="#6-6-NUMA" class="headerlink" title="6.6 NUMA"></a><strong>6.6 NUMA</strong></h4><hr>
<p>DPDK 内存分配上通过 proc 提供的内存信息，使 CPU 核心尽量使用靠近其所在节点的内存，避免了跨 NUMA 节点远程访问内存的性能问题。</p>
<h4 id="6-7-CPU-亲和性"><a href="#6-7-CPU-亲和性" class="headerlink" title="6.7 CPU 亲和性"></a><strong>6.7 CPU 亲和性</strong></h4><hr>
<p>DPDK 利用 CPU 的亲和性将一个线程或多个线程绑定到一个或多个 CPU 上，这样在线程执行过程中，就不会被随意调度，一方面减少了线程间的频繁切换带来的开销，另一方面避免了 CPU 缓存的局部失效性，增加了 CPU 缓存的命中率。</p>
<h4 id="6-8-多核调度框架"><a href="#6-8-多核调度框架" class="headerlink" title="6.8 多核调度框架"></a><strong>6.8 多核调度框架</strong></h4><hr>
<p>DPDK 基于多核架构，一般会有主从核之分，主核负责完成各个模块的初始化，从核负责具体的业务处理。</p>
<p>除了上述之外，DPDK 还有很多的技术突破，可以用下面这张图来概之。</p>
<center><img src="/images/sdn/dpdkarch.jpg" alt=""></center>

<h3 id="07-DPDK-的应用"><a href="#07-DPDK-的应用" class="headerlink" title="07 DPDK 的应用"></a>07 DPDK 的应用</h3><hr>
<p>DPDK 作为优秀的用户空间高性能数据包加速套件，现在已经作为一个“胶水”模块被用在多个网络数据处理方案中，用来提高性能。如下是众多的应用。</p>
<center><img src="/images/sdn/dpdkapp.jpg" alt=""></center>

<h4 id="数据面（虚拟交换机）"><a href="#数据面（虚拟交换机）" class="headerlink" title="数据面（虚拟交换机）"></a><strong>数据面（虚拟交换机）</strong></h4><hr>
<p><strong>OVS</strong></p>
<p>Open vSwitch 是一个多核虚拟交换机平台，支持标准的管理接口和开放可扩展的可编程接口，支持第三方的控制接入。</p>
<p><a href="https://github.com/openvswitch/ovs" target="_blank" rel="noopener">https://github.com/openvswitch/ovs</a></p>
<p><strong>VPP</strong></p>
<p>VPP 是 cisco 开源的一个高性能的包处理框架，提供了 交换/路由 功能，在虚拟化环境中，使它可以当做一个虚拟交换机来使用。在一个类 SDN 的处理框架中，它往往充当数据面的角色。经研究表明，VPP 性能要好于 OVS+DPDK 的组合，但它更适用于 NFV，适合做特定功能的网络模块。</p>
<p><a href="https://wiki.fd.io/view/VPP" target="_blank" rel="noopener">https://wiki.fd.io/view/VPP</a></p>
<p><strong>Lagopus</strong></p>
<p>Lagopus 是另一个多核虚拟交换的实现，功能和 OVS 差不多，支持多种网络协议，如 Ethernet，VLAN，QinQ，MAC-in-MAC，MPLS 和 PBB，以及隧道协议，如 GRE，VxLan 和 GTP。</p>
<p><a href="https://github.com/lagopus/lagopus/blob/master/QUICKSTART.md" target="_blank" rel="noopener">https://github.com/lagopus/lagopus/blob/master/QUICKSTART.md</a></p>
<p><strong>Snabb</strong></p>
<p>Snabb 是一个简单且快速的数据包处理工具箱。</p>
<p><a href="https://github.com/SnabbCo/snabbswitch/blob/master/README.md" target="_blank" rel="noopener">https://github.com/SnabbCo/snabbswitch/blob/master/README.md</a></p>
<h4 id="数据面（虚拟路由器）"><a href="#数据面（虚拟路由器）" class="headerlink" title="数据面（虚拟路由器）"></a><strong>数据面（虚拟路由器）</strong></h4><hr>
<p><strong>OPENCONTRAIL</strong></p>
<p>一个集成了 SDN 控制器的虚拟路由器，现在多用在 OpenStack 中，结合 Neutron 为 OpenStack 提供一站式的网络支持。</p>
<p><a href="http://www.opencontrail.org/" target="_blank" rel="noopener">http://www.opencontrail.org/</a></p>
<p><strong>CloudRouter</strong></p>
<p>一个分布式的路由器。</p>
<p><a href="https://cloudrouter.org/" target="_blank" rel="noopener">https://cloudrouter.org/</a></p>
<h4 id="用户空间协议栈"><a href="#用户空间协议栈" class="headerlink" title="用户空间协议栈"></a><strong>用户空间协议栈</strong></h4><hr>
<p><strong>mTCP</strong></p>
<p>mTCP 是一个针对多核系统的高可扩展性的用户空间 TCP/IP 协议栈。</p>
<p><a href="https://github.com/eunyoung14/mtcp/blob/master/README" target="_blank" rel="noopener">https://github.com/eunyoung14/mtcp/blob/master/README</a></p>
<p><strong>IwIP</strong></p>
<p>IwIP 针对 RAM 平台的精简版的 TCP/IP 协议栈实现。</p>
<p><a href="http://git.savannah.gnu.org/cgit/lwip.git/tree/README" target="_blank" rel="noopener">http://git.savannah.gnu.org/cgit/lwip.git/tree/README</a></p>
<p><strong>Seastar</strong></p>
<p>Seastar 是一个开源的，基于 C++ 11/14 feature，支持高并发和低延迟的异步编程高性能库。</p>
<p><a href="http://www.seastar-project.org/" target="_blank" rel="noopener">http://www.seastar-project.org/</a></p>
<p><strong>f-stack</strong></p>
<p>腾讯开源的用户空间协议栈，移植于 FreeBSD协议栈，粘合了 POSIX API，上层应用（协程框架，Nginx,Redis），纯 C 编写，易上手。</p>
<p><a href="https://github.com/f-stack/f-stack" target="_blank" rel="noopener">https://github.com/f-stack/f-stack</a></p>
<h4 id="存储加速"><a href="#存储加速" class="headerlink" title="存储加速"></a><strong>存储加速</strong></h4><hr>
<p><strong>SPDK</strong></p>
<p>SPDK 是 DPDK 的孪生兄弟，专注存储性能加速，目前的火热程度丝毫不亚于 DPDK，Intel 近来对 SPDK 特别重视，隔三差五就发布新版本。</p>
<p><a href="https://github.com/spdk/spdk" target="_blank" rel="noopener">https://github.com/spdk/spdk</a></p>
<h3 id="08-总结"><a href="#08-总结" class="headerlink" title="08 总结"></a>08 总结</h3><hr>
<p>DPDK 绕过了 Linux 内核协议栈，加速数据的处理，用户可以在用户空间定制协议栈，满足自己的应用需求，目前出现了很多基于 DPDK 的高性能网络框架，OVS 和 VPP 是常用的数据面框架，mTCP 和 f-stack 是常用的用户态协议栈，SPDK 是存储性能加速器，很多大公司都在使用 DPDK 来优化网络性能。</p>
<blockquote>
<p>PS：本文所有的图来自网络，侵权必删。</p>
</blockquote>
<h3 id="09-DPDK-资料推荐"><a href="#09-DPDK-资料推荐" class="headerlink" title="09 DPDK 资料推荐"></a>09 DPDK 资料推荐</h3><hr>
<p>在我看来，DPDK 最好的学习资料是官网，没有之一：</p>
<p><a href="http://core.dpdk.org/doc/" target="_blank" rel="noopener">http://core.dpdk.org/doc/</a></p>
<p>其次是看 Intel 技术专家出的书 《深入浅出 DPDK》。</p>
<center><img src="/images/sdn/dpdkbook.jpg" alt=""></center>

<p>本书详细介绍了DPDK 技术发展趋势，数据包处理，硬件加速技术，包处理和虚拟化 ，以及 DPDK 技术在 SDN，NFV ，网络存储等领域的实际应用。</p>
<p>本书是国内第一本全面的阐述网络数据面的核心技术的书籍，面向 IT 网络通讯行业的从业人员，以及大专院校的学生，用通俗易懂的文字打开了一扇通向新一代网络处理架构的大门。</p>
<p>本书我有电子版（但只有一部分，推荐大家买书），需要的公众号后台回复 “DPDK” 查看获取方式。</p>
<p>除了书之外，就是看大牛的博客，加入相关的群和优秀的人一起学习，我整理了几份网上较好的博客资料，和书一起附赠，如果想加群学习，回复 “加群”。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/NUMA/"><i class="fas fa-hashtag fa-fw"></i>NUMA</a>
                
                    <a href="/tags/SDN/"><i class="fas fa-hashtag fa-fw"></i>SDN</a>
                
                    <a href="/tags/DPDK/"><i class="fas fa-hashtag fa-fw"></i>DPDK</a>
                
                    <a href="/tags/性能分析/"><i class="fas fa-hashtag fa-fw"></i>性能分析</a>
                
                    <a href="/tags/UIO/"><i class="fas fa-hashtag fa-fw"></i>UIO</a>
                
                    <a href="/tags/大页内存/"><i class="fas fa-hashtag fa-fw"></i>大页内存</a>
                
                    <a href="/tags/OVS/"><i class="fas fa-hashtag fa-fw"></i>OVS</a>
                
                    <a href="/tags/VPP/"><i class="fas fa-hashtag fa-fw"></i>VPP</a>
                
                    <a href="/tags/mTCP/"><i class="fas fa-hashtag fa-fw"></i>mTCP</a>
                
                    <a href="/tags/fstack/"><i class="fas fa-hashtag fa-fw"></i>fstack</a>
                
                    <a href="/tags/SPDK/"><i class="fas fa-hashtag fa-fw"></i>SPDK</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class="post-wrapper">
          <article class="post reveal ">
    
<section class="meta">
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/06/技术/Docker_容器网络之单主机网络/">
              
                  Docker 容器网络之单主机网络
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-06
      </time>
    

    
      
    
    <div class="metatag cats">
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Docker/">Docker</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <blockquote>
<p>文章首发于我的公众号「CloudDeveloper」，欢迎关注，第一时间掌握技术干货！</p>
</blockquote>
<p>当容器逐步向容器集群，容器云技术演进的时候，一个不得不面对的问题就是各个容器的管理问题，有些容器需要交互，有些容器需要隔离，如何保证这些操作能够顺利地进行，这个时候，很多容器管理和编排的平台就应运而生了。首先，当然是 Docker 社区自己开发的 Swarm+Machine+Compose 的集群管理套件，然后还有 Twitter 主推 Apache 的 Mesos，最有名的应该为 Google 开源的 Kubernetes。</p>
<p>这些平台综合了容器集群资源管理，服务发现和扩容缩融等问题，是一个集大成的解决方案，但其实这些问题本质上都离不开网络，资源在各容器之间调度需要网络，服务发现需要网络，所以，可以说网络是容器集群环境下最基础的一环。</p>
<p>Docker 容器网络根据容器的部署位置，可以分为单主机网络（host）和多主机网络（multi-host），本文先看 Docker host 网络。</p>
<p>Docker host 网络分为 none，host，joined container 和 bridge 网络。</p>
<p>none 网络模式下，Docker 容器拥有自己的 network namespace，但是并不为容器进行任何的网络配置，也就是说容器除了 network namespace 本身自带的 localback 网卡外什么都没有，包括网卡、IP、路由等信息。用户如何要使用 none 网络，就需要自己添加特定的网卡，并配置 IP、路由等信息，但一般不会这么干，none 网络很好地做到了隔离，一般是用来跑那些对安全性要求极高且不需要联网的应用。</p>
<p>比如某个容器的唯一用途是生成随机密码，就可以放到 none 网络中避免密码被窃取。</p>
<p>想要使 Docker 使用 none 网络，只需要在创建容器的时候附带 –network = none 即可。</p>
<p>host 网络，顾名思义就是 Docker 容器使用宿主机的网络，相当于和 host 共用了同一个 network namespace，Docker 容器使用 host 的网卡、IP 和路由等功能对外通信。</p>
<p>虽然这种模式下 Docker 没有创建独立的 network namespace，但其他 namespace 仍然是隔离的，如文件系统、进程列表等。host 网络最大的好处就是使得 Docker 容器对外通信变得简单，直接使用 host 的 IP 进行通信即可，但缺点也很明显，就是降低了隔离性，同时还会存在和其他容器对网络资源的竞争与冲突的问题。</p>
<p>同样要使用 host 网络，只需创建容器时附带 –network = host 即可。</p>
<center><img src="/images/docker/net_host.png" alt=""></center>

<p>joined container 网络和 host 网络不同的是，它是和其他的 container 共享一个 network namespace，一个 network namespace 可以被一个或者多个 Docker 容器共享。在这种模式下，两个容器之间可以通过 localback 回环网卡通信，增加了容器间通信的便利性。</p>
<p>同样可以在创建容器时，使用参数 –network = container:another_container_name 来和另外容器共享网络。</p>
<center><img src="/images/docker/net_con.png" alt=""></center>

<p>bridge 网络是最常用的网络模式，它兼顾了安全性和功能的完备性，但其与外部通信要通过 NAT 转换，在复杂的网络场景下会存在诸多不便。</p>
<p>bridge 网络在创建的时候通过 –network = bridge 指定，Docker daemon 会为创建的容器自动创建一个 Docker 网桥——docker0（也可以人为指定名称 –driver bridge my_net），这个 docker0 就用来联结 Docker 容器和 host 的桥梁。</p>
<p>然后，Docker daemon 会创建一对虚拟网卡 veth pair，一块网卡留在宿主机的 root network namespace 中，并绑定到 docker0 上，另一块放在新创建的 network namespace 中，命名为 eth0，并为其配置 IP 和路由，将其网关设为 docker0，如下，这样整个容器的 bridge 网络通信环境就建立起来了，其他容器的建立方式也是如此，最终各容器之间就通过 docker0 这个桥梁互联互通了。</p>
<center><img src="/images/docker/net_bridge.jpg" alt=""></center>

<p>下文将讨论更为复杂的 multi-host 网络。</p>
<p>PS：文章未经我允许，不得转载，否则后果自负。</p>
<center>–END–</center>

<hr>
<blockquote>
<p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p>
</blockquote>
<p><img src="/images/weichat.png" alt=""></p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/云计算/"><i class="fas fa-hashtag fa-fw"></i>云计算</a>
                
                    <a href="/tags/容器/"><i class="fas fa-hashtag fa-fw"></i>容器</a>
                
                    <a href="/tags/Docker/"><i class="fas fa-hashtag fa-fw"></i>Docker</a>
                
                    <a href="/tags/网络/"><i class="fas fa-hashtag fa-fw"></i>网络</a>
                
                    <a href="/tags/容器网络/"><i class="fas fa-hashtag fa-fw"></i>容器网络</a>
                
                    <a href="/tags/Bridge/"><i class="fas fa-hashtag fa-fw"></i>Bridge</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
</section>


    <br>
    <div class="prev-next">
        <div class="prev-next">
            
                <a class="prev" rel="prev" href="/page/2/">
                    <section class="post prev">
                        <i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;上一页&nbsp;
                    </section>
                </a>
            
            <p class="current">
                3 / 6
            </p>
            
                <a class="next" rel="next" href="/page/4/">
                    <section class="post next">
                        &nbsp;下一页&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i>
                    </section>
                </a>
            

        </div>
    </div>



<!-- 根据主题中的设置决定是否在archive中针对摘要部分的MathJax公式加载mathjax.js文件 -->





        </div>
        <aside class='l_side'>
            
  
  
    
      
      
        <section class="plain">
  
<header class="pure">
  <div><i class="fas fa-bullhorn fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;公告</div>
  
</header>

  <div class="content pure">
    <p>你好，这里是<a href="https://chambai.github.io/materialblog/">小白的网志空间</a>，我在这里分享技术和生活，专注于云计算/网络/CC++/Python/Go等技术栈，平常喜欢左手Coding，右手Writing，欢迎关注我的公众号「cloud_dev」，期待与你相遇~</p>

  </div>
</section>

      
    
  
    
      
      
        <section class="author">
  <div class="content pure">
    
      <div class="avatar">
        <img class="avatar" src="/images/cloud.png">
      </div>
    
    
      <div class="text">
        
        
          <p>公众号：CloudDeveloper</p>

        
        
      </div>
    
    
  </div>
</section>

      
    
  
    
      
      
        
  <section class="category">
    
<header class="pure">
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;所有分类</div>
  
</header>

    <div class="content pure">
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/DPDK/" href="/categories/DPDK/"><div class="name">DPDK</div><div class="badge">(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Diary/" href="/categories/Diary/"><div class="name">Diary</div><div class="badge">(0)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Docker/" href="/categories/Docker/"><div class="name">Docker</div><div class="badge">(9)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Kubernetes/" href="/categories/Kubernetes/"><div class="name">Kubernetes</div><div class="badge">(13)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Linux/" href="/categories/Linux/"><div class="name">Linux</div><div class="badge">(11)</div></a></li>
        
          <li><a class="flat-box" title="/categories/NFV/" href="/categories/NFV/"><div class="name">NFV</div><div class="badge">(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/OVS/" href="/categories/OVS/"><div class="name">OVS</div><div class="badge">(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/云计算/" href="/categories/云计算/"><div class="name">云计算</div><div class="badge">(7)</div></a></li>
        
          <li><a class="flat-box" title="/categories/虚拟化/" href="/categories/虚拟化/"><div class="name">虚拟化</div><div class="badge">(10)</div></a></li>
        
          <li><a class="flat-box" title="/categories/雾计算/" href="/categories/雾计算/"><div class="name">雾计算</div><div class="badge">(2)</div></a></li>
        
      </ul>
    </div>
  </section>


      
    
  
    
      
      
        
  <section class="tagcloud">
    
<header class="pure">
  <div><i class="fas fa-fire fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;热门标签</div>
  
</header>

    <div class="content pure">
      <a href="/tags/Bridge/" style="font-size: 16px; color: #8b8b8b">Bridge</a> <a href="/tags/CNM/" style="font-size: 14px; color: #999">CNM</a> <a href="/tags/CPU/" style="font-size: 17px; color: #858585">CPU</a> <a href="/tags/Cgroup/" style="font-size: 15px; color: #929292">Cgroup</a> <a href="/tags/DPDK/" style="font-size: 16px; color: #8b8b8b">DPDK</a> <a href="/tags/Docker/" style="font-size: 21px; color: #696969">Docker</a> <a href="/tags/I-O/" style="font-size: 17px; color: #858585">I/O</a> <a href="/tags/IOMMU/" style="font-size: 14px; color: #999">IOMMU</a> <a href="/tags/KVM/" style="font-size: 20px; color: #707070">KVM</a> <a href="/tags/Kata/" style="font-size: 14px; color: #999">Kata</a> <a href="/tags/Kubernetes/" style="font-size: 23px; color: #5c5c5c">Kubernetes</a> <a href="/tags/Linux/" style="font-size: 23px; color: #5c5c5c">Linux</a> <a href="/tags/NFV/" style="font-size: 14px; color: #999">NFV</a> <a href="/tags/NUMA/" style="font-size: 16px; color: #8b8b8b">NUMA</a> <a href="/tags/Namespace/" style="font-size: 15px; color: #929292">Namespace</a> <a href="/tags/OVS/" style="font-size: 17px; color: #858585">OVS</a> <a href="/tags/OpenFlow/" style="font-size: 15px; color: #929292">OpenFlow</a> <a href="/tags/OpenStack/" style="font-size: 14px; color: #999">OpenStack</a> <a href="/tags/Pouch/" style="font-size: 14px; color: #999">Pouch</a> <a href="/tags/Qemu/" style="font-size: 19px; color: #777">Qemu</a> <a href="/tags/SDN/" style="font-size: 14px; color: #999">SDN</a> <a href="/tags/SPDK/" style="font-size: 14px; color: #999">SPDK</a> <a href="/tags/SR-IOV/" style="font-size: 14px; color: #999">SR-IOV</a> <a href="/tags/TAP-TUN-VETH/" style="font-size: 14px; color: #999">TAP/TUN/VETH</a> <a href="/tags/UIO/" style="font-size: 14px; color: #999">UIO</a> <a href="/tags/VLAN/" style="font-size: 14px; color: #999">VLAN</a> <a href="/tags/VPP/" style="font-size: 14px; color: #999">VPP</a> <a href="/tags/cacico/" style="font-size: 14px; color: #999">cacico</a> <a href="/tags/flannel/" style="font-size: 14px; color: #999">flannel</a> <a href="/tags/fstack/" style="font-size: 14px; color: #999">fstack</a> <a href="/tags/libnetwork/" style="font-size: 14px; color: #999">libnetwork</a> <a href="/tags/mTCP/" style="font-size: 15px; color: #929292">mTCP</a> <a href="/tags/macvlan/" style="font-size: 14px; color: #999">macvlan</a> <a href="/tags/overlay/" style="font-size: 14px; color: #999">overlay</a> <a href="/tags/vhost/" style="font-size: 15px; color: #929292">vhost</a> <a href="/tags/vhost-user/" style="font-size: 14px; color: #999">vhost_user</a> <a href="/tags/virtio/" style="font-size: 15px; color: #929292">virtio</a> <a href="/tags/weave/" style="font-size: 14px; color: #999">weave</a> <a href="/tags/云计算/" style="font-size: 24px; color: #555">云计算</a> <a href="/tags/内存/" style="font-size: 15px; color: #929292">内存</a> <a href="/tags/大页内存/" style="font-size: 14px; color: #999">大页内存</a> <a href="/tags/容器/" style="font-size: 21px; color: #696969">容器</a> <a href="/tags/容器网络/" style="font-size: 18px; color: #7e7e7e">容器网络</a> <a href="/tags/微服务/" style="font-size: 14px; color: #999">微服务</a> <a href="/tags/性能分析/" style="font-size: 19px; color: #777">性能分析</a> <a href="/tags/技能图谱/" style="font-size: 15px; color: #929292">技能图谱</a> <a href="/tags/混合云/" style="font-size: 14px; color: #999">混合云</a> <a href="/tags/网络/" style="font-size: 22px; color: #636363">网络</a> <a href="/tags/虚拟化/" style="font-size: 23px; color: #5c5c5c">虚拟化</a> <a href="/tags/边缘计算/" style="font-size: 15px; color: #929292">边缘计算</a> <a href="/tags/集群/" style="font-size: 14px; color: #999">集群</a> <a href="/tags/零拷贝/" style="font-size: 14px; color: #999">零拷贝</a> <a href="/tags/雾计算/" style="font-size: 16px; color: #8b8b8b">雾计算</a>
    </div>
  </section>


      
    
  
    
      
      
        <section class="list">
  
<header class="pure">
  <div><i class="fas fa-medal fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;精选项目</div>
  
    <a class="rightBtn" target="_blank" rel="external nofollow noopener noreferrer" href="https://xaoxuu.com/projects/" title="https://xaoxuu.com/projects/">
    <i class="fas fa-arrow-right fa-fw"></i></a>
  
</header>

  <div class="content pure">
    <ul class="entry">
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/axkit/" href="https://xaoxuu.com/wiki/axkit/">
          <div class="name">
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;AXKit
          </div>
          
            <div class="badge">(iOS开源库)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/noticeboard/" href="https://xaoxuu.com/wiki/noticeboard/">
          <div class="name">
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;NoticeBoard
          </div>
          
            <div class="badge">(iOS开源库)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/heartmate/" href="https://xaoxuu.com/heartmate/">
          <div class="name">
            
              <i class="fas fa-heartbeat fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;HeartMate
          </div>
          
            <div class="badge">(iOS应用程序)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/material-x/" href="https://xaoxuu.com/wiki/material-x/">
          <div class="name">
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;Material X
          </div>
          
            <div class="badge">(Hexo博客主题)</div>
          
        </a></li>
      
    </ul>
  </div>
</section>

      
    
  
    
      
      
        <section class="list">
  
<header class="pure">
  <div><i class="fas fa-link fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;友链</div>
  
</header>

  <div class="content pure">
    <ul class="entry">
      
        <li><a class="flat-box" title="https://xaoxuu.com/about/" href="https://xaoxuu.com/about/">
          <div class="name">
            
              <i class="fas fa-comment-dots fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;关于我 / 留言板
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

      
    
  
    
      
      
        



      
    
  
    
      
      
        

      
    
  


        </aside>
        <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
    </div>
    <footer id="footer" class="clearfix">
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>本站使用 <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a> 作为主题，总访问量为 <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span> 次。
  </div>
</footer>

    <script>setLoadingBarProgress(80);</script>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>


  
    <script src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
    <script type="text/javascript">
      $(function() {
        const $reveal = $('.reveal');
    		if ($reveal.length === 0) return;
    		const sr = ScrollReveal({ distance: 0 });
    		sr.reveal('.reveal');
      });
    </script>
  
  
    <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
    <script type="text/javascript">
      $(function() {
        Waves.attach('.flat-btn', ['waves-button']);
        Waves.attach('.float-btn', ['waves-button', 'waves-float']);
        Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
        Waves.attach('.flat-box', ['waves-block']);
        Waves.attach('.float-box', ['waves-block', 'waves-float']);
        Waves.attach('.waves-image');
        Waves.init();
      });
    </script>
  
  
    <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>
  
  
  


  
  
  
  
    <script src="/js/app.js"></script>
<script src="/js/search.js"></script>
  






    <script>setLoadingBarProgress(100);</script>
</body>
</html>
