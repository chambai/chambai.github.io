<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Docker 网络模型之 macvlan 详解</title>
      <link href="/2019/04/14/tech/cloud/container/docker/docker-macvlan/"/>
      <url>/2019/04/14/tech/cloud/container/docker/docker-macvlan/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>上一篇文章我们详细介绍了 macvlan 这种技术，<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247485246&amp;idx=1&amp;sn=c42a3618c357ebf5f6b7b7ce78ae568f&amp;chksm=ea743386dd03ba90ad65940321385f68f9315fec16d82a08efa12c18501d8cadf95cf9e614a2&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">macvlan 详解</a>，由于它高效易配置的特性，被用在了 Docker 的网络方案设计中，这篇文章就来说说这个。</p><a id="more"></a><h2 id="01macvlan-用于-Docker-网络"><a href="#01macvlan-用于-Docker-网络" class="headerlink" title="01macvlan 用于 Docker 网络"></a>01macvlan 用于 Docker 网络</h2><p>在 Docker 中，macvlan 是众多 Docker 网络模型中的一种，并且是一种跨主机的网络模型，作为一种驱动（driver）启用（-d 参数指定），Docker macvlan 只支持 bridge 模式。</p><p>关于 Docker 的众多跨主机网络模型的科普，参照我之前写的一篇文章：<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247484056&amp;idx=1&amp;sn=d67971f00e5a19ea0cb880f84122bc59&amp;chksm=ea743620dd03bf363d5b38be69412e19c8ae6395f5310b189a44ce4078fae354d1fa0c786889&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">容器网络之多主机网络</a>。</p><p>下面我们做两个实验，分别验证相同 macvlan 网络和不同 macvlan 网络的连通性。</p><h3 id="1-1-相同-macvlan-网络之间的通信"><a href="#1-1-相同-macvlan-网络之间的通信" class="headerlink" title="1.1 相同 macvlan 网络之间的通信"></a>1.1 相同 macvlan 网络之间的通信</h3><p>首先准备两个主机节点的 Docker 环境，搭建如下拓扑图示：</p><p><img src="/images/virt/dockermacvlan1.png" alt="dockermacvlan1"></p><p>1 首先使用 <code>docker network create</code> 分别在两台主机上创建两个 macvlan 网络：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker network create -d macvlan --subnet=172.16.10.0/24 --gateway=172.16.10.1 -o parent=enp0s8 mac1</span></span><br></pre></td></tr></table></figure><p>这条命令中，</p><ul><li><code>-d</code> 指定 Docker 网络 driver</li><li><code>--subnet</code> 指定 macvlan 网络所在的网络</li><li><code>--gateway</code> 指定网关</li><li><code>-o parent</code> 指定用来分配 macvlan 网络的物理网卡</li></ul><p>之后可以看到当前主机的网络环境，其中出现了 macvlan 网络：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker network ls</span></span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">128956db798a        bridge              bridge              <span class="built_in">local</span></span><br><span class="line">19fb1af129e6        host                host                <span class="built_in">local</span></span><br><span class="line">2509b3717813        mac1                macvlan             <span class="built_in">local</span></span><br><span class="line">d5b0798e725e        none                null                <span class="built_in">local</span></span><br></pre></td></tr></table></figure><p>2 在 host1 运行容器 c1，并指定使用 macvlan 网络：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker run -itd --name c1 --ip=172.16.10.2 --network mac1 busybox</span></span><br></pre></td></tr></table></figure><p>这条命令中，</p><ul><li><code>--ip</code> 指定容器 c1 使用的 IP，这样做的目的是防止自动分配，造成 IP 冲突</li><li><code>--network</code> 指定 macvlan 网络</li></ul><p>同样在 host2 中运行容器 c2：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker run -itd --name c2 --ip=172.16.10.3 --network mac1 busybox</span></span><br></pre></td></tr></table></figure><p>3 在 host1 c1 中 ping host2 c2：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker exec c1 ping -c 2 172.16.10.3</span></span><br><span class="line">PING 172.16.10.3 (172.16.10.3): 56 data bytes</span><br><span class="line">64 bytes from 172.16.10.3: seq=0 ttl=64 time=0.641 ms</span><br><span class="line">64 bytes from 172.16.10.3: seq=1 ttl=64 time=0.393 ms</span><br><span class="line"></span><br><span class="line">--- 172.16.10.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line"></span><br><span class="line">round-trip min/avg/max = 0.393/0.517/0.641 ms</span><br></pre></td></tr></table></figure><blockquote><p>注意：以上的实验都需要物理网卡 enp0s8 开启混杂模式，不然会 ping 不通。</p></blockquote><h3 id="1-2-不同-macvlan-网络之间的通信"><a href="#1-2-不同-macvlan-网络之间的通信" class="headerlink" title="1.2 不同 macvlan 网络之间的通信"></a>1.2 不同 macvlan 网络之间的通信</h3><p>接下来，我们来看看不同 macvlan 网络之间的连通性，搭建以下的拓扑环境：</p><p><img src="/images/virt/dockermacvlan2.jpeg" alt="dockermacvlan2"></p><p>由于 macvlan 网络会独占物理网卡，也就是说一张物理网卡只能创建一个 macvlan 网络，如果我们想创建多个 macvlan 网络就得用多张网卡，但主机的物理网卡是有限的，怎么办呢？</p><p>好在 macvlan 网络也是支持 VLAN 子接口的，所以，我们可以通过 VLAN 技术将一个网口划分出多个子网口，这样就可以基于子网口来创建 macvlan 网络了，下面是具体的创建过程。</p><p>1 首先分别在两台主机上将物理网口 enp0s8 创建出两个 VLAN 子接口。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 vconfig 命令在 eth0 配置两个 VLAN</span></span><br><span class="line">root@ubuntu:~<span class="comment"># vconfig add enp0s8 100</span></span><br><span class="line">root@ubuntu:~<span class="comment"># vconfig add enp0s8 200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 VLAN 的 REORDER_HDR 参数，默认就行了</span></span><br><span class="line">root@ubuntu:~<span class="comment"># vconfig set_flag enp0s8.100 1 1</span></span><br><span class="line">root@ubuntu:~<span class="comment"># vconfig set_flag enp0s8.200 1 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用接口</span></span><br><span class="line">root@ubuntu:~<span class="comment"># ifconfig enp0s8.100 up</span></span><br><span class="line">root@ubuntu:~<span class="comment"># ifconfig enp0s8.200 up</span></span><br></pre></td></tr></table></figure><p>2 分别在 host1 和 host2 上基于两个 VLAN 子接口创建 2 个 macvlan 网络，mac10 和 mac20。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker network create -d macvlan --subnet=172.16.10.0/24 --gateway=172.16.10.1 -o parent=enp0s8.100 mac10</span></span><br><span class="line">root@ubuntu:~<span class="comment"># docker network create -d macvlan --subnet=172.16.20.0/24 --gateway=172.16.20.1 -o parent=enp0s8.200 mac20</span></span><br></pre></td></tr></table></figure><p>3 分别在 host1 和 host2 上运行容器，并指定不同的 macvlan 网络。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># host1</span></span><br><span class="line">root@ubuntu:~<span class="comment"># docker run -itd --name d1 --ip=172.16.10.10 --network mac10 busybox</span></span><br><span class="line">root@ubuntu:~<span class="comment"># docker run -itd --name d2 --ip=172.16.20.10 --network mac20 busybox</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># host2 </span></span><br><span class="line">root@ubuntu:~<span class="comment"># docker run -itd --name d3 --ip=172.16.10.11 --network mac10 busybox</span></span><br><span class="line">root@ubuntu:~<span class="comment"># docker run -itd --name d4 --ip=172.16.20.11 --network mac20 busybox</span></span><br></pre></td></tr></table></figure><p>通过验证，d1 和 d3，d2 和 d4 在同一 macvlan 网络下，互相可以 ping 通，d1 和 d2，d1 和 d4 在不同的 macvlan 网络下，互相 ping 不通。</p><p>这个原因也很明确，不同 macvlan 网络处于不同的网络，而且通过 VLAN 隔离，自然 ping 不了。</p><p>但这也只是在二层上通不了，通过三层的路由是可以通的，我们这就来验证下。</p><p>重新找一台主机 host3，通过打开 <code>ip_forward</code> 把它改造成一台路由器（至于为什么可以这样，可以参考我之前的一篇文章xxx），用来打通两个 macvlan 网络，大概的图示如下所示：</p><p><img src="/images/virt/dockermacvlan3.jpeg" alt="dockermacvlan3"></p><p>1 首先对 host3 执行 <code>sysctl -w net.ipv4.ip_forward=1</code> 打开路由开关。</p><p>2 然后创建两个 VLAN 子接口，一个作为 macvlan 网络 mac10 的网关，一个作为 mac20 的网关。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># vconfig add enp0s8 100</span></span><br><span class="line">[root@localhost ~]<span class="comment"># vconfig add enp0s8 200</span></span><br><span class="line">[root@localhost ~]<span class="comment"># vconfig set_flag enp0s8.100 1 1</span></span><br><span class="line">[root@localhost ~]<span class="comment"># vconfig set_flag enp0s8.200 1 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对 vlan 子接口配置网关 IP 并启用</span></span><br><span class="line">[root@localhost ~]<span class="comment"># ifconfig enp0s8.100 172.16.10.1 netmask 255.255.255.0 up</span></span><br><span class="line">[root@localhost ~]<span class="comment"># ifconfig enp0s8.200 172.16.20.1 netmask 255.255.255.0 up</span></span><br></pre></td></tr></table></figure><p>3 这样之后再从 d1 ping d2 和 d4，就可以 ping 通了。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker exec d1 ping -c 2 172.16.20.10</span></span><br><span class="line">PING 172.16.20.10 (172.16.20.10): 56 data bytes</span><br><span class="line">64 bytes from 172.16.20.10: seq=0 ttl=63 time=0.661 ms</span><br><span class="line">64 bytes from 172.16.20.10: seq=1 ttl=63 time=0.717 ms</span><br><span class="line"></span><br><span class="line">--- 172.16.20.10 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.661/0.689/0.717 ms</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker exec d1 ping -c 2 172.16.20.11</span></span><br><span class="line">PING 172.16.20.11 (172.16.20.11): 56 data bytes</span><br><span class="line">64 bytes from 172.16.20.11: seq=0 ttl=63 time=0.548 ms</span><br><span class="line">64 bytes from 172.16.20.11: seq=1 ttl=63 time=0.529 ms</span><br><span class="line"></span><br><span class="line">--- 172.16.20.11 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.529/0.538/0.548 ms</span><br></pre></td></tr></table></figure><p>PS：可能有些系统做了安全限制，可能 ping 不通，这时候可以添加以下 iptables 规则，目的是让系统能够转发不通 VLAN 的数据包。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -o enp0s8.100 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">iptables -t nat -A POSTROUTING -oenp0s8.200 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">iptables -A FORWARD -i enp0s8.100 -o enp0s8.200 -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A FORWARD -i enp0s8.200 -o enp0s8.100 -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iptables -A FORWARD -i enp0s8.100 -o enp0s8.200 -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A FORWARD -i enp0s8.200 -o enp0s8.100 -j ACCEPT</span><br></pre></td></tr></table></figure><p>为什么配置 VLAN 子接口，配上 IP 就可以通了，我们可以看下路由表就知道了。</p><p>首先看容器 d1 的路由：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># docker exec d1 ip route</span></span><br><span class="line">default via 172.16.10.1 dev eth0 </span><br><span class="line">172.16.10.0/24 dev eth0 scope link  src 172.16.10.10</span><br></pre></td></tr></table></figure><p>我们在创建容器的时候指定了网关 <code>172.16.10.1</code>，所以数据包自然会被路由到 host3 的接口。再来看下 host3 的路由：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># ip route</span></span><br><span class="line">default via 192.168.108.1 dev enp0s3 proto dhcp metric 100 </span><br><span class="line">172.16.10.0/24 dev enp0s8.100 proto kernel scope link src 172.16.10.1 </span><br><span class="line">172.16.20.0/24 dev enp0s8.200 proto kernel scope link src 172.16.20.1 </span><br><span class="line">192.168.56.0/24 dev enp0s8 proto kernel scope link src 192.168.56.122 metric 101 </span><br><span class="line">192.168.108.0/24 dev enp0s3 proto kernel scope link src 192.168.108.2 metric 100</span><br></pre></td></tr></table></figure><p>可以看到，去往 <code>172.16.10.0/24</code> 网段的数据包会从 enp0s8.100 出去，同理 <code>172.16.20.0/24</code> 网段也是，再加上 host3 的 <code>ip_forward</code> 打开，这就打通了两个 macvlan 网络之间的通路。</p><h2 id="02-总结"><a href="#02-总结" class="headerlink" title="02 总结"></a>02 总结</h2><p>在 Docker 中，macvlan 只支持 bridge 模式。</p><p>相同 macvlan 可以通信，不同 macvlan 二层无法通信，可以借助三层路由完成通信。</p><p><strong>参考：</strong></p><p><a href="https://www.cnblogs.com/CloudMan6/p/7400580.html" target="_blank" rel="noopener">https://www.cnblogs.com/CloudMan6/p/7400580.html</a></p><p><a href="https://blog.csdn.net/dog250/article/details/45788279" target="_blank" rel="noopener">https://blog.csdn.net/dog250/article/details/45788279</a></p><p><a href="https://www.hi-linux.com/posts/40904.html" target="_blank" rel="noopener">https://www.hi-linux.com/posts/40904.html</a></p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> Docker </tag>
            
            <tag> macvlan </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux网络命令必知必会之 tcpdump</title>
      <link href="/2019/04/11/tech/net/tools/linux%E7%BD%91%E7%BB%9C%E5%91%BD%E4%BB%A4%E4%B9%8Btcpdump/"/>
      <url>/2019/04/11/tech/net/tools/linux%E7%BD%91%E7%BB%9C%E5%91%BD%E4%BB%A4%E4%B9%8Btcpdump/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><h2 id="01-简介"><a href="#01-简介" class="headerlink" title="01 简介"></a>01 简介</h2><p><strong>tcpdump</strong> 是一款 Linux 平台的抓包工具。它可以抓取涵盖整个 TCP/IP 协议族的数据包，支持针对网络层、协议、主机、端口的过滤，并提供 and、or、not 等逻辑语句来过滤无用的信息。</p><p>tcpdump 是一个非常复杂的工具，掌握它的方方面面实属不易，也不推荐，能够用它来解决日常工作问题才是关系。</p><a id="more"></a><h2 id="02-tcpdump-命令选项"><a href="#02-tcpdump-命令选项" class="headerlink" title="02 tcpdump 命令选项"></a>02 tcpdump 命令选项</h2><p>tcpdump 有很多命令选项，想了解所有选项可以 Linux 命令行输入 <code>tcpdump -h</code>，<code>man tcpdump</code> 查看每个选项的意思。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]# tcpdump -h</span><br><span class="line">tcpdump version 4.9.2</span><br><span class="line">libpcap version 1.5.3</span><br><span class="line">OpenSSL 1.0.2k-fips  26 Jan 2017</span><br><span class="line">Usage: tcpdump [-aAbdDefhHIJKlLnNOpqStuUvxX#] [ -B size ] [ -c count ]</span><br><span class="line">[ -C file_size ] [ -E algo:secret ] [ -F file ] [ -G seconds ]</span><br><span class="line">[ -i interface ] [ -j tstamptype ] [ -M secret ] [ --number ]</span><br><span class="line">[ -Q|-P in|out|inout ]</span><br><span class="line">[ -r file ] [ -s snaplen ] [ --time-stamp-precision precision ]</span><br><span class="line">[ --immediate-mode ] [ -T type ] [ --version ] [ -V file ]</span><br><span class="line">[ -w file ] [ -W filecount ] [ -y datalinktype ] [ -z postrotate-command ]</span><br><span class="line">[ -Z user ] [ expression ]</span><br></pre></td></tr></table></figure><p>下面列举一些常用选项：</p><ul><li>-A 只使用 ASCII 打印报文的全部数据，不要和 <code>-X</code> 一起使用，获取 http 可以用 <code>tcpdump -nSA port 80</code></li><li>-b 在数据链路层上选择协议，包括 ip, arp, rarp, ipx 等</li><li>-c 指定要抓取包的数量</li><li>-D 列出操作系统所有可以用于抓包的接口</li><li>-i 指定监听的网卡，<code>-i any</code> 显示所有网卡</li><li>-n 表示不解析主机名，直接用 IP 显示，默认是用 hostname 显示</li><li>-nn 表示不解析主机名和端口，直接用端口号显示，默认显示是端口号对应的服务名</li><li>-p 关闭接口的混杂模式</li><li>-P 指定抓取的包是流入的包还是流出的，可以指定参数 in, out, inout 等，默认是 inout</li><li>-q 快速打印输出，即只输出少量的协议相关信息</li><li>-s len 设置要抓取数据包长度为 len，默认只会截取前 96bytes 的内容，<code>-s 0</code> 的话，会截取全部内容。</li><li>-S 将 TCP 的序列号以绝对值形式输出，而不是相对值</li><li>-t 不要打印时间戳</li><li>-vv 输出详细信息（比如 tos、ttl、checksum等）</li><li>-X 同时用 hex 和 ascii 显示报文内容</li><li>-XX 同 -X，但同时显示以太网头部</li></ul><h2 id="03-过滤器"><a href="#03-过滤器" class="headerlink" title="03 过滤器"></a>03 过滤器</h2><p>网络报文是很多的，很多时候我们在主机上抓包，会抓到很多我们并不关心的无用包，然后要从这些包里面去找我们需要的信息，无疑是一件费时费力的事情，tcpdump 提供了灵活的语法可以精确获取我们关心的数据，这些语法说得专业点就是过滤器。</p><p>过滤器简单可分为三类：协议（proto）、传输方向（dir）和类型（type）。</p><p>一般的<strong>表达式格式</strong>为：</p><p><img src="/images/net/tcpdump.png" alt="tcpdump"></p><p><a href="https://www.cnblogs.com/f-ck-need-u/p/7064286.html?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">图片来自</a></p><ul><li>关于 proto：可选有 ip, arp, rarp, tcp, udp, icmp, ether 等，默认是所有协议的包</li><li>关于 dir：可选有 src, dst, src or dst, src and dst，默认为 src or dst</li><li>关于 type：可选有 host, net, port, portrange（端口范围，比如 21-42），默认为 host</li></ul><h2 id="04-常用操作"><a href="#04-常用操作" class="headerlink" title="04 常用操作"></a>04 常用操作</h2><p>测试环境 IP：172.18.82.173</p><h3 id="4-1-抓取某主机的数据包"><a href="#4-1-抓取某主机的数据包" class="headerlink" title="4.1 抓取某主机的数据包"></a>4.1 抓取某主机的数据包</h3><p>抓取主机 172.18.82.173 上所有收到（DST_IP）和发出（SRC_IP）的所有数据包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump host 172.18.82.173</span><br></pre></td></tr></table></figure><p>抓取经过指定网口 interface ，并且 DST_IP 或 SRC_IP 是 172.18.82.173 的数据包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 host 172.18.82.173</span><br></pre></td></tr></table></figure><p>筛选 SRC_IP，抓取经过 interface 且从 172.18.82.173 发出的包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 src host 172.18.82.173</span><br></pre></td></tr></table></figure><p>筛选 DST_IP，抓取经过 interface 且发送到 172.18.82.173 的包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 dst host 172.18.82.173</span><br></pre></td></tr></table></figure><p>抓取主机 200.200.200.1 和主机 200.200.200.2 或 200.200.200.3 通信的包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump host 200.200.200.1 and \(200.200.200.2 or 200.200.200.3\)</span><br></pre></td></tr></table></figure><p>抓取主机 200.200.200.1 和除了主机 200.200.200.2 之外所有主机通信的包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump ip host 200.200.200.1 and ! 200.200.200.2</span><br></pre></td></tr></table></figure><h3 id="4-2-抓取某端口的数据包"><a href="#4-2-抓取某端口的数据包" class="headerlink" title="4.2 抓取某端口的数据包"></a>4.2 抓取某端口的数据包</h3><p>抓取所有端口，显示 IP 地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -nS</span><br></pre></td></tr></table></figure><p>抓取某端口上的包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump port 22</span><br></pre></td></tr></table></figure><p>抓取经过指定 interface，并且 DST_PORT 或 SRC_PORT 是 22 的数据包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 port 22</span><br></pre></td></tr></table></figure><p>筛选 SRC_PORT</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 src port 22</span><br></pre></td></tr></table></figure><p>筛选 DST_PORT</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 dst port 22</span><br></pre></td></tr></table></figure><p>比如希望查看发送到 host 172.18.82.173 的网口 eth0 的 22 号端口的包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]# tcpdump -i eth0 -nnt dst host 172.18.82.173 and port 22 -c 1 -vv</span><br><span class="line">tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">IP (tos 0x14, ttl 114, id 27674, offset 0, flags [DF], proto TCP (6), length 40)</span><br><span class="line">    113.98.59.61.51830 &gt; 172.18.82.173.22: Flags [.], cksum 0x7fe3 (correct), seq 19775964, ack 1564316089, win 2053, length 0</span><br></pre></td></tr></table></figure><h3 id="4-3-抓取某网络（网段）的数据包"><a href="#4-3-抓取某网络（网段）的数据包" class="headerlink" title="4.3 抓取某网络（网段）的数据包"></a>4.3 抓取某网络（网段）的数据包</h3><p>抓取经过指定 interface，并且 DST_NET 或 SRC_NET 是 172.18.82 的包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 net 172.18.82</span><br></pre></td></tr></table></figure><p>筛选 SRC_NET</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 src net 172.18.82</span><br></pre></td></tr></table></figure><p>筛选 DST_NET</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 dst net 172.18.82</span><br></pre></td></tr></table></figure><h3 id="4-4-抓取某协议的数据包"><a href="#4-4-抓取某协议的数据包" class="headerlink" title="4.4 抓取某协议的数据包"></a>4.4 抓取某协议的数据包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 icmp</span><br><span class="line">tcpdump -i eth0 ip</span><br><span class="line">tcpdump -i eth0 tcp</span><br><span class="line">tcpdump -i eth0 udp</span><br><span class="line">tcpdump -i eth0 arp</span><br></pre></td></tr></table></figure><h3 id="4-5-复杂的逻辑表达式抓取过滤条件"><a href="#4-5-复杂的逻辑表达式抓取过滤条件" class="headerlink" title="4.5 复杂的逻辑表达式抓取过滤条件"></a>4.5 复杂的逻辑表达式抓取过滤条件</h3><p>抓取经过 interface eth0 发送到 host 200.200.200.1 或 200.200.200.2 的 TCP 协议 22 号端口的数据包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 -nntvv -c 10 &apos;((tcp) and (port 22) and ((dst host 200.200.200.1) or (dst host 200.200.200.2)))&apos;</span><br></pre></td></tr></table></figure><p>PS：对于复杂的过滤器表达式，为了逻辑清晰，可以使用 <code>()</code>，不过默认情况下，tcpdump 会将 <code>()</code> 当做特殊字符，所以必须使用 <code>&#39;&#39;</code> 来消除歧义。</p><p>抓取经过 interface eth0， DST_MAC 或 SRC_MAC 地址是 00:16:3e:12:16:e7 的 ICMP 数据包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 &apos;((icmp) and ((ether host 00:16:3e:12:16:e7)))&apos; -nnvv</span><br></pre></td></tr></table></figure><p>抓取经过 interface eth0，目标网络是 172.18 但目标主机又不是 172.18.82.173 的 TCP 且非 22 号端口号的数据包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 -nntvv &apos;((dst net 172.18) and (not dst host 172.18.82.173) and (tcp) and (not port 22))&apos;</span><br></pre></td></tr></table></figure><p>抓取流入 interface eth0，host 为 172.18.82.173 且协议为 ICMP 的数据包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 -nntvv -P in host 172.18.82.173 and icmp</span><br></pre></td></tr></table></figure><p>抓取流出 interface eth0，host 为 172.18.82.173 且协议为 ICMP 的数据包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0 -nntvv -P out host 172.18.82.173 and icmp</span><br></pre></td></tr></table></figure><h2 id="05-与-wireshark、Snort-等工具的结合"><a href="#05-与-wireshark、Snort-等工具的结合" class="headerlink" title="05 与 wireshark、Snort 等工具的结合"></a>05 与 wireshark、Snort 等工具的结合</h2><p>tcpdump 抓包的时候，默认是打印到屏幕输出，如果是抓取包少还好，如果包很多，很多行数据，刷刷刷从眼前一闪而过，根本来不及看清内容。不过，tcpdump 提供了将抓取的数据保存到文件的功能，查看文件就方便分析多了，而且还能与其他图形工具一起配合分析，比如 wireshark、Snort 等。</p><ul><li>-w 选项表示把数据报文输出到文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -w capture_file.pcap port 80 # 把所有 80 端口的数据导出到文件</span><br></pre></td></tr></table></figure><ul><li>-r 选项表示读取文件里的数据报文，显示到屏幕上</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -nXr capture_file.pcap host host1</span><br></pre></td></tr></table></figure><p>PS：<code>.pcap</code> 格式的文件需要用 wireshark、Snort 等工具查看，使用 <code>vim</code> 或 <code>cat</code> 会出现乱码。</p><h2 id="06-tcpdump-的输出格式"><a href="#06-tcpdump-的输出格式" class="headerlink" title="06 tcpdump 的输出格式"></a>06 tcpdump 的输出格式</h2><p>tcpdump 的输出格式总体上为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">系统时间 源主机.端口 &gt; 目标主机.端口 数据包参数</span><br></pre></td></tr></table></figure><p>比如下面的例子，显示了 TCP 的三次握手过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">21:27:06.995846 IP (tos 0x0, ttl 64, id 45646, offset 0, flags [DF], proto TCP (6), length 64)</span><br><span class="line">    192.168.1.106.56166 &gt; 124.192.132.54.80: Flags [S], cksum 0xa730 (correct), seq 992042666, win 65535, options [mss 1460,nop,wscale 4,nop,nop,TS val 663433143 ecr 0,sackOK,eol], length 0</span><br><span class="line"></span><br><span class="line">21:27:07.030487 IP (tos 0x0, ttl 51, id 0, offset 0, flags [DF], proto TCP (6), length 44)</span><br><span class="line">    124.192.132.54.80 &gt; 192.168.1.106.56166: Flags [S.], cksum 0xedc0 (correct), seq 2147006684, ack 992042667, win 14600, options [mss 1440], length 0</span><br><span class="line"></span><br><span class="line">21:27:07.030527 IP (tos 0x0, ttl 64, id 59119, offset 0, flags [DF], proto TCP (6), length 40)</span><br><span class="line">    192.168.1.106.56166 &gt; 124.192.132.54.80: Flags [.], cksum 0x3e72 (correct), ack 2147006685, win 65535, length 0</span><br></pre></td></tr></table></figure><p>第一条是 <code>SYN</code> 报文，通过 <code>Flags[S]</code> 看出。第二条是 <code>[S.]</code>，表示 <code>SYN-ACK</code> 报文。常见的 TCP 报文的 Flags 如下：</p><ul><li><code>[S]</code>： SYN（开始连接）</li><li><code>[.]</code>: 没有 Flag</li><li><code>[P]</code>: PSH（推送数据）</li><li><code>[F]</code>: FIN （结束连接）</li><li><code>[R]</code>: RST（重置连接）</li></ul><h2 id="06-总结"><a href="#06-总结" class="headerlink" title="06 总结"></a>06 总结</h2><p>本文可以当字典查阅，记住一些常用的 tcpdump 抓包案例，其他的用到再通过 <code>man tcpdump</code> 辅助编写。和 wireshark 等图形化工具配合使用，能更加深理解。</p><p><strong>参考：</strong></p><p><a href="https://blog.csdn.net/Jmilk/article/details/86618205?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">https://blog.csdn.net/Jmilk/article/details/86618205?tdsourcetag=s_pctim_aiomsg</a></p><p><a href="https://www.cnblogs.com/f-ck-need-u/p/7064286.html?tdsourcetag=s_pctim_aiomsg" target="_blank" rel="noopener">https://www.cnblogs.com/f-ck-need-u/p/7064286.html?tdsourcetag=s_pctim_aiomsg</a></p><p><a href="https://danielmiessler.com/study/tcpdump/" target="_blank" rel="noopener">https://danielmiessler.com/study/tcpdump/</a></p><p><a href="http://bencane.com/2014/10/13/quick-and-practical-reference-for-tcpdump/" target="_blank" rel="noopener">http://bencane.com/2014/10/13/quick-and-practical-reference-for-tcpdump/</a></p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> tcpdump </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux云网络基础之网卡虚拟化技术 macvlan 详解</title>
      <link href="/2019/04/01/tech/net/vnet/linux-macvlan/"/>
      <url>/2019/04/01/tech/net/vnet/linux-macvlan/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><h2 id="01-macvlan-简介"><a href="#01-macvlan-简介" class="headerlink" title="01 macvlan 简介"></a>01 macvlan 简介</h2><p>前面的文章讲过了几种 Linux 虚拟网络设备：tap/tun、veth-pair、bridge，它们本质上是 Linux 系统 提供的网络虚拟化解决方案，今天要讲的 macvlan 也是其中的一种，准确说这是一种网卡虚拟化的解决方案。因为 macvlan 这种技术能将 <strong>一块物理网卡虚拟成多块虚拟网卡</strong> ，相当于物理网卡施展了 多重影分身之术 ，由一个变多个。</p><p><img src="/images/virt/macvlanmulti.jpeg" alt="macvlanmulti"></p><a id="more"></a><h2 id="02-macvlan-的工作原理"><a href="#02-macvlan-的工作原理" class="headerlink" title="02 macvlan 的工作原理"></a>02 macvlan 的工作原理</h2><p>macvlan 是 Linux kernel 支持的新特性，支持的版本有 v3.9-3.19 和 4.0+，比较稳定的版本推荐 4.0+。它一般是以内核模块的形式存在，我们可以通过以下方法判断当前系统是否支持：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># modprobe macvlan</span></span><br><span class="line"><span class="comment"># lsmod | grep macvlan</span></span><br><span class="line">macvlan                24576  0</span><br></pre></td></tr></table></figure><p>如果第一个命令报错，或者第二个命令没有返回，说明当前系统不支持 macvlan，需要升级内核。</p><p>macvlan 这种技术听起来有点像 VLAN，但它们的实现机制是完全不一样的。macvlan 子接口和原来的主接口是完全独立的，可以单独配置 MAC 地址和 IP 地址，而 VLAN 子接口和主接口共用相同的 MAC 地址。VLAN 用来划分广播域，而 macvlan 共享同一个广播域。</p><p>通过不同的子接口，macvlan 也能做到流量的隔离。macvlan 会根据收到包的目的 MAC 地址判断这个包需要交给哪个虚拟网卡，虚拟网卡再把包交给上层的协议栈处理。</p><p><img src="/images/virt/macvlanflow.png" alt="macvlanflow"></p><h2 id="03-四种模式"><a href="#03-四种模式" class="headerlink" title="03 四种模式"></a>03 四种模式</h2><p>根据 macvlan 子接口之间的通信模式，macvlan 有四种网络模式：</p><ul><li>private 模式</li><li>vepa(virtual ethernet port aggregator) 模式</li><li>bridge 模式</li><li>passthru 模式</li></ul><p>默认使用的是 vepa 模式。</p><h3 id="3-1-private"><a href="#3-1-private" class="headerlink" title="3.1 private"></a>3.1 private</h3><p>这种模式下，同一主接口下的子接口之间彼此隔离，不能通信。即使从外部的物理交换机导流，也会被无情地丢掉。</p><p><img src="/images/virt/private.png" alt="linux-macvlan-private-mode"></p><h3 id="3-2-vepa"><a href="#3-2-vepa" class="headerlink" title="3.2 vepa"></a>3.2 vepa</h3><p>这种模式下，子接口之间的通信流量需要导到外部支持 <code>802.1Qbg/VPEA</code> 功能的交换机上（可以是物理的或者虚拟的），经由外部交换机转发，再绕回来。</p><p>注：<code>802.1Qbg/VPEA</code> 功能简单说就是交换机要支持 <code>发夹（hairpin）</code> 功能，也就是数据包从一个接口上收上来之后还能再扔回去。</p><p><img src="/images/virt/vepa.png" alt="linux-macvlan-802.1qbg-vepa-mode"></p><h3 id="3-3-bridge"><a href="#3-3-bridge" class="headerlink" title="3.3 bridge"></a>3.3 bridge</h3><p>这种模式下，模拟的是 Linux bridge 的功能，但比 bridge 要好的一点是每个接口的 MAC 地址是已知的，不用学习。所以，这种模式下，子接口之间就是直接可以通信的。</p><p><img src="/images/virt/bridge.png" alt="linux-macvlan-bridge-mode"></p><h3 id="3-4-passthru"><a href="#3-4-passthru" class="headerlink" title="3.4 passthru"></a>3.4 passthru</h3><p>这种模式，只允许单个子接口连接主接口，且必须设置成混杂模式，一般用于子接口桥接和创建 VLAN 子接口的场景。</p><p><img src="/images/virt/passthru.png" alt="linux-macvlan-passthru-mode"></p><h3 id="3-5-mactap"><a href="#3-5-mactap" class="headerlink" title="3.5 mactap"></a>3.5 mactap</h3><p>和 macvlan 相似的技术还有一种是 mactap。和 macvlan 不同的是，mactap 收到包之后不是交给协议栈，而是交给一个 tapX 文件，然后通过这个文件，完成和用户态的直接通信。</p><p><img src="/images/virt/mactap.png" alt="mactap"></p><h2 id="04-实践"><a href="#04-实践" class="headerlink" title="04 实践"></a>04 实践</h2><p>在 Linux 系统下，创建 macvlan 的命令形式如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip link add link DEVICE name NAME <span class="built_in">type</span> &#123; macvlan | macvtap &#125; mode &#123; private | vepa | bridge |</span><br><span class="line">              passthru  [ nopromisc ] &#125;</span><br></pre></td></tr></table></figure><p>通常，单独使用 macvlan 毫无意义，一般都是结合 VM 和容器来构建网络。下面我们就简单使用 namespace 来看看 Linux 是怎么使用 macvlan 的。</p><p>实验拓扑如下：</p><p><img src="/images/virt/macvlan.png" alt="macvlan.png"></p><p>在我的系统中，以接口 <code>enp0s8</code> 为例创建两个 macvlan 子接口（使用 bridge 模式），配置 IP 并将其挂到两个 namespace 中，测试连通性。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建两个 macvlan 子接口</span></span><br><span class="line">ip link add link enp0s8 dev mac1 <span class="built_in">type</span> macvlan mode bridge</span><br><span class="line">ip link add link enp0s8 dev mac2 <span class="built_in">type</span> macvlan mode bridge</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建两个 namespace</span></span><br><span class="line">ip netns add ns1</span><br><span class="line">ip netns add ns2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将两个子接口分别挂到两个 namespace 中</span></span><br><span class="line">ip link <span class="built_in">set</span> mac1 netns ns1</span><br><span class="line">ip link <span class="built_in">set</span> mac2 netns ns2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 IP 并启用</span></span><br><span class="line">ip netns <span class="built_in">exec</span> ns1 ip a a 192.168.56.122/24 dev mac1</span><br><span class="line">ip netns <span class="built_in">exec</span> ns1 ip l s mac1 up</span><br><span class="line"></span><br><span class="line">ip netns <span class="built_in">exec</span> ns1 ip a a 192.168.56.123/24 dev mac2</span><br><span class="line">ip netns <span class="built_in">exec</span> ns2 ip l s mac2 up</span><br></pre></td></tr></table></figure><p>注：<code>enp0s8</code> 的 IP 是 <code>192.168.56.110/24</code>，配置的子接口 IP 也必须是同一网段的。</p><p>完了两个子接口 ping 一下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~<span class="comment"># ip netns exec ns1 ip a show mac1</span></span><br><span class="line">9: mac1@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/ether 2e:6e:d9:08:c5:05 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 192.168.56.122/24 scope global mac1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::2c6e:d9ff:fe08:c505/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">root@ubuntu:~<span class="comment"># ip netns exec ns1 ping 192.168.56.123</span></span><br><span class="line">PING 192.168.56.123 (192.168.56.123) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.56.123: icmp_seq=1 ttl=64 time=0.052 ms</span><br><span class="line">64 bytes from 192.168.56.123: icmp_seq=2 ttl=64 time=0.028 ms</span><br><span class="line">^C</span><br><span class="line">--- 192.168.56.123 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.028/0.040/0.052/0.012 ms</span><br></pre></td></tr></table></figure><p>可以看到，能够 ping 通，如果把上面的 mode 换成其他模式就行不通了，这个就留给大家去实验了（默认是 vepa 模式）。</p><p>另外，在 docker 中，macvlan 是一种较为重要的跨主机网络模型，这块的内容就留作下篇文章再做讲解了。</p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> macvlan </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文带你全面了解虚拟机的四种网络模型</title>
      <link href="/2019/03/25/tech/net/vnet/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%9B%9B%E7%A7%8D%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
      <url>/2019/03/25/tech/net/vnet/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%9B%9B%E7%A7%8D%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><h2 id="01-从物理网络到虚拟网络"><a href="#01-从物理网络到虚拟网络" class="headerlink" title="01 从物理网络到虚拟网络"></a>01 从物理网络到虚拟网络</h2><p>著名的「六度分隔定理」说到，世界上任何两个互不相识的人，只需要最多六个人就能够建立起联系。这个定理成立的前提就是依托于庞大的网络结构。</p><p>在虚拟化技术没出现之前，构成网络的元素都是实体的物理设备，比如交换机、路由器、网线等等，人们想要构建一个小型的局域网自己玩玩，都要买各种设备，成本高还不灵活。虚拟化技术普及之后，云计算开始大行其道，我们在自己的单机上就可以建各种虚拟机，想怎么玩就怎么玩。</p><a id="more"></a><p>随之而来的就是网络变得更复杂了，由以前看得见摸得着的物理网络一下变成玄乎的虚拟网络了，好不容易建了几台虚拟机，发现网络不通，或者网络通了，但并不知道是怎么通的，这难言的苦水哽在喉咙实在令人不适。</p><p>这篇文章就来说说虚拟机世界里的几种网络模型，我们主要以 <code>VirtualBox</code> 和 <code>VMware Workstation</code> 这两款目前最主流的桌面虚拟化软件作为例子。</p><p>总的来说，目前有四种常见的网络模型：</p><ul><li>桥接（Bridge Adapter）</li><li>NAT</li><li>主机（Host-only Adapter）</li><li>内部网络（Internal）</li></ul><p>这也是 <code>VirtualBox</code> 支持的四种模型，对于 <code>VMware</code>，则只有前三种。</p><p>下图显示了 <code>VirtualBox</code> 支持的几种网络模型：</p><p><img src="/images/virt/virtualboxnet.png" alt="virtualboxnet"></p><h2 id="02-桥接（Bridge-Adapter）"><a href="#02-桥接（Bridge-Adapter）" class="headerlink" title="02 桥接（Bridge Adapter）"></a>02 桥接（Bridge Adapter）</h2><p>虚拟机桥接网络模型就是使用虚拟交换机（Linux Bridge），将虚拟机和物理机连接起来，它们处于同一个网段，IP 地址是一样的。如下图所示：</p><p><img src="/images/virt/bridgenet.png" alt=""></p><p>在这种网络模型下，虚拟机和物理机都处在一个二层网络里面，所以有：</p><ul><li>虚拟机之间彼此互通</li><li>虚拟机与主机彼此可以互通</li><li>只要物理机可以上网，那么虚拟机也可以。我们来验证下：</li></ul><p><img src="/images/virt/bridgetointernet.jpeg" alt="bridgetointernet"></p><p>桥接网络的好处是简单方便，但也有一个很明显的问题，就是一旦虚拟机太多，广播就会很严重。所以，桥接网络一般也只适用于桌面虚拟机或者小规模网络这种简单的形式。</p><h2 id="03-NAT"><a href="#03-NAT" class="headerlink" title="03 NAT"></a>03 NAT</h2><p>另一种模型是 NAT，即网络地址转换（Network Address Translatation）。这种模型严格来讲，又可以分为 <code>NAT</code> 和 <code>NAT 网络</code>两种，我们看上面的图 1 也可以看到。</p><p>根据 NAT 的原理，虚拟机所在的网络和物理机所在的网络不在同一个网段，虚拟机要访问物理所在网络必须经过一个地址转换的过程，也就是说在虚拟机网络内部需要内置一个虚拟的 NAT 设备来做这件事。</p><p>但其中 <code>NAT</code> 和 <code>NAT 网络</code> 两者还有些许的不同：</p><ul><li><code>NAT</code>：主机上的虚拟机之间是互相隔离的，彼此不能通信（它们有独立的网络栈，独立的虚拟 NAT 设备）</li><li><code>NAT 网络</code>：虚拟机之间共享虚拟 NAT 设备，彼此互通。</li></ul><p>如下图，展示了两者细微的差别：</p><p><img src="/images/virt/natnet.jpeg" alt="natnet"></p><p>PS：NAT 网络模式中一般还会内置一个虚拟的 DHCP 服务器来进行 IP 地址的管理。</p><p>下面我们通过实验来验证一下两种模式的区别，首先是 <code>NAT</code> 模式：</p><p>访问外网没问题：</p><p><img src="/images/virt/nattointernet.png" alt="nattointernet"></p><p>访问其他虚拟机：</p><p><img src="/images/virt/nattovm.png" alt="nattovm"></p><p>可以看到，两个虚拟机由于有隔离的网络栈，所以它们的 IP 地址并不在一个网段，所以 ping 不通。</p><p>再来看 <code>NAT 网络</code>，访问外网同样没问题，我们来看下 VM 之间的互通：</p><p><img src="/images/virt/natnettovm.jpeg" alt="natnettovm"></p><p>可以看到，差别体现出来了，<code>NAT 网络</code> 虚拟机之间共享网络栈，它们的 IP 地址处于同一个网段，所以彼此是互通的。</p><p>总结一下，以上两种 NAT 模式，如果不做其他配置，那么有：</p><ul><li>虚拟机可以访问主机，反之不行</li><li>如果主机可以上外网，那么虚拟机也可以</li><li>对于 <code>NAT</code>，同主机上的虚拟机之间不能互通</li><li>对于 <code>NAT 网络</code>，虚拟机之间可以互通</li></ul><p>PS：如果做了 <strong>端口映射</strong> 配置，那么主机也可以访问虚拟机。</p><h2 id="04-主机网络（Host-only-Adapter）"><a href="#04-主机网络（Host-only-Adapter）" class="headerlink" title="04 主机网络（Host-only Adapter）"></a>04 主机网络（Host-only Adapter）</h2><p>主机网络顾名思义，就是只限于主机内部访问的网络，虚拟机之间彼此互通，虚拟机与主机之间彼此互通。但是默认情况下虚拟机不能访问外网（注意：这里说的是默认情况下，如果稍作配置，也是可以的）。</p><p>主机网络看似简单，其实它的网络模型是相对比较复杂的，可以说前面几种模式实现的功能，在这种模式下，都可以通过虚拟机和网卡的配置来实现，这得益于它特殊的网络模型。</p><p>主机网络模型会在主机中模拟出一块虚拟网卡供虚拟机使用，所有虚拟机都连接到这块网卡上，这块网卡默认会使用网段 <code>192.168.56.x</code>（在主机的网络配置界面可以看到这块网卡），如下是基本的拓扑图示：</p><p><img src="/images/virt/host-onlynet.png" alt="host-onlynet"></p><p>默认情况下，虚拟机之间可以互通，虚拟机只能和主机上的虚拟网卡互通，不能和不同网段的网卡互通，更不能访问外网，如果想做到这样，那么需要如图中 <strong>红虚线</strong> 所示，将物理网卡和虚拟网卡桥接或共享。在主机上做如下设置即可：</p><p><img src="/images/virt/hostonlyshare.png" alt="hostonlyshare"></p><p>通过以上配置，我们来验证一下，虚拟机可以访问主机物理网卡和外网了：</p><p><img src="/images/virt/hostonlytointernet.jpeg" alt="hostonlytointernet"></p><h2 id="05-内部网络（internal）"><a href="#05-内部网络（internal）" class="headerlink" title="05 内部网络（internal）"></a>05 内部网络（internal）</h2><p>最后一种网络模型是内部网络，这种模型是相对最简单的一种，虚拟机与外部环境完全断开，只允许虚拟机之间互相访问，这种模型一般不怎么用，所以在 <code>VMware</code> 虚拟机中是没有这种网络模式的。这里我们就不多说了。</p><h2 id="06-总结"><a href="#06-总结" class="headerlink" title="06 总结"></a>06 总结</h2><p>虚拟机的四种网络模型：桥接、NAT、主机和内网模型。</p><p>下面以一张表来描述它们之间的通信行为：</p><table><thead><tr><th style="text-align:center">Model</th><th style="text-align:center">VM -&gt; host</th><th style="text-align:center">host -&gt; VM</th><th style="text-align:center">VM <-> VM</-></th><th style="text-align:center">VM -&gt; Internet</th><th style="text-align:center">Internet -&gt; VM</th></tr></thead><tbody><tr><td style="text-align:center">Bridged</td><td style="text-align:center">+</td><td style="text-align:center">+</td><td style="text-align:center">+</td><td style="text-align:center">+</td><td style="text-align:center">+</td></tr><tr><td style="text-align:center">NAT</td><td style="text-align:center">+</td><td style="text-align:center">Port Forwarding</td><td style="text-align:center">-</td><td style="text-align:center">+</td><td style="text-align:center">Port Forwarding</td></tr><tr><td style="text-align:center">NAT Network</td><td style="text-align:center">+</td><td style="text-align:center">Port Forwarding</td><td style="text-align:center">+</td><td style="text-align:center">+</td><td style="text-align:center">Port Forwarding</td></tr><tr><td style="text-align:center">Host-only</td><td style="text-align:center">+</td><td style="text-align:center">+</td><td style="text-align:center">+</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">Internal</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">+</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr></tbody></table><p><strong>参考：</strong> </p><p><a href="https://technology.amis.nl/2018/07/27/virtualbox-networking-explained/#prettyPhoto" target="_blank" rel="noopener">https://technology.amis.nl/2018/07/27/virtualbox-networking-explained/#prettyPhoto</a></p><p><a href="https://blog.csdn.net/niqinwen/article/details/11761487" target="_blank" rel="noopener">https://blog.csdn.net/niqinwen/article/details/11761487</a></p><p><a href="https://www.jianshu.com/p/5b8da7a1ad63" target="_blank" rel="noopener">https://www.jianshu.com/p/5b8da7a1ad63</a></p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> Bridge </tag>
            
            <tag> NAT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux云网络基础之 IP 隧道详解</title>
      <link href="/2019/03/18/tech/net/vnet/ip%E9%9A%A7%E9%81%93/"/>
      <url>/2019/03/18/tech/net/vnet/ip%E9%9A%A7%E9%81%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>通过之前的文章<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247484961&amp;idx=1&amp;sn=f26d7994f57abbf5de2007a2f451d9f5&amp;chksm=ea743299dd03bb8f6ac063c1cb00d5a592094c7778ab0a5baf37b7469fa3eb018101ef34551f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">tap/tun 详解</a>，我们知道 tun 是一个网络层的设备，也被叫做点对点设备，之所以叫这个名字，是因为 tun 常常被用来做隧道通信（tunnel）。</p><a id="more"></a><h2 id="IP-隧道"><a href="#IP-隧道" class="headerlink" title="IP 隧道"></a>IP 隧道</h2><p>Linux 原生支持多种三层隧道，其底层实现原理都是基于 tun 设备。我们可以通过命令 <code>ip tunnel help</code> 查看 IP 隧道的相关操作。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># ip tunnel help</span></span><br><span class="line">Usage: ip tunnel &#123; add | change | del | show | prl | 6rd &#125; [ NAME ]</span><br><span class="line">          [ mode &#123; ipip | gre | sit | isatap | vti &#125; ] [ remote ADDR ] [ <span class="built_in">local</span> ADDR ]</span><br><span class="line">          [ [i|o]seq ] [ [i|o]key KEY ] [ [i|o]csum ]</span><br><span class="line">          [ prl-default ADDR ] [ prl-nodefault ADDR ] [ prl-delete ADDR ]</span><br><span class="line">          [ 6rd-prefix ADDR ] [ 6rd-relay_prefix ADDR ] [ 6rd-reset ]</span><br><span class="line">          [ ttl TTL ] [ tos TOS ] [ [no]pmtudisc ] [ dev PHYS_DEV ]</span><br><span class="line"></span><br><span class="line">Where: NAME := STRING</span><br><span class="line">       ADDR := &#123; IP_ADDRESS | any &#125;</span><br><span class="line">       TOS  := &#123; STRING | 00..ff | inherit | inherit/STRING | inherit/00..ff &#125;</span><br><span class="line">       TTL  := &#123; 1..255 | inherit &#125;</span><br><span class="line">       KEY  := &#123; DOTTED_QUAD | NUMBER &#125;</span><br></pre></td></tr></table></figure><p>可以看到，Linux 原生一共支持 5 种 IP 隧道。</p><ul><li><code>ipip</code>：即 <code>IPv4 in IPv4</code>，在 IPv4 报文的基础上再封装一个 IPv4 报文。</li><li><code>gre</code>：即通用路由封装（<code>Generic Routing Encapsulation</code>），定义了在任意一种网络层协议上封装其他任意一种网络层协议的机制，IPv4 和 IPv6 都适用。</li><li><code>sit</code>：和 <code>ipip</code> 类似，不同的是 <code>sit</code> 是用 IPv4 报文封装 IPv6 报文，即 <code>IPv6 over IPv4</code>。</li><li><code>isatap</code>：即站内自动隧道寻址协议（<code>Intra-Site Automatic Tunnel Addressing Protocol</code>），和 <code>sit</code> 类似，也是用于 IPv6 的隧道封装。</li><li><code>vti</code>：即虚拟隧道接口（<code>Virtual Tunnel Interface</code>），是 cisco 提出的一种 <code>IPsec</code> 隧道技术。</li></ul><h2 id="实践-IPIP-隧道"><a href="#实践-IPIP-隧道" class="headerlink" title="实践 IPIP 隧道"></a>实践 IPIP 隧道</h2><p>我们下面以 <code>ipip</code> 作为例子，来实践下 Linux 的隧道通信。本文以前文的 Linux 路由机制作为基础，不清楚 Linux 路由的可以先翻看下那篇文章再来看。</p><p>实践之前，需要知道的是，<code>ipip</code> 需要内核模块 <code>ipip.ko</code> 的支持，通过 <code>lsmod | grep ipip</code> 查看内核是否加载，若没有则用 <code>modprobe ipip</code> 先加载，正常加载应该显示：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]<span class="comment"># modprobe ipip</span></span><br><span class="line">[root@by ~]<span class="comment"># lsmod | grep ipip</span></span><br><span class="line">ipip                   13465  0</span><br><span class="line">tunnel4                13252  1 ipip</span><br><span class="line">ip_tunnel              25163  1 ipip</span><br></pre></td></tr></table></figure><p>加载 <code>ipip</code> 模块后，就可以创建隧道了，方法是先创建一个 tun 设备，然后将该 tun 设备绑定为一个 <code>ipip</code> 隧道即可。</p><p>我们的实验拓扑如下：</p><p><img src="/images/virt/ipiptunnel.jpeg" alt="ipiptunnel.png"></p><p>首先参照路由那篇文章，保证 v1 和 v2 能够通信，这里就不再赘述了。</p><p>然后创建 tun 设备，并设置为 <code>ipip</code> 隧道。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1) 在 ns1 上创建 tun1 和 ipip tunnel</span></span><br><span class="line">ip netns <span class="built_in">exec</span> ns1 ip tunnel add tun1 mode ipip remote 10.10.20.2 <span class="built_in">local</span> 10.10.10.2</span><br><span class="line"></span><br><span class="line">ip netns <span class="built_in">exec</span> ns1 ip l s tun1 up</span><br><span class="line">ip netns <span class="built_in">exec</span> ns1 ip a a 10.10.100.10 peer 10.10.200.10 dev tun1</span><br></pre></td></tr></table></figure><p>上面的命令是在 NS1 上创建 tun 设备 tun1，并设置隧道模式为 <code>ipip</code>，然后还需要设置隧道端点，用 <code>remote</code> 和 <code>local</code> 表示，这是 <strong>隧道外层 IP</strong>，对应的还有 <strong>隧道内层 IP</strong>，用 <code>ip addr xx peer xx</code> 配置。大体的示意图如下所示。</p><p><img src="/images/virt/ipipformat.jpeg" alt="ipipformat.png"></p><p>同理，我们也在 NS2 上做如上配置。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1) 在 ns2 上创建 tun2 和 ipip tunnel</span></span><br><span class="line">ip netns <span class="built_in">exec</span> ns2 ip tunnel add tun2 mode ipip remote 10.10.10.2 <span class="built_in">local</span> 10.10.20.2</span><br><span class="line"></span><br><span class="line">ip netns <span class="built_in">exec</span> ns2 ip l s tun2 up</span><br><span class="line">ip netns <span class="built_in">exec</span> ns2 ip a a 10.10.200.10 peer 10.10.100.10 dev tun2</span><br></pre></td></tr></table></figure><p>当做完上述配置，两个 tun 设备端点就可以互通了，如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]<span class="comment"># ip netns exec ns1 ping 10.10.200.10 -c 4</span></span><br><span class="line">PING 10.10.200.10 (10.10.200.10) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.10.200.10: icmp_seq=1 ttl=64 time=0.090 ms</span><br><span class="line">64 bytes from 10.10.200.10: icmp_seq=2 ttl=64 time=0.148 ms</span><br><span class="line">64 bytes from 10.10.200.10: icmp_seq=3 ttl=64 time=0.112 ms</span><br><span class="line">64 bytes from 10.10.200.10: icmp_seq=4 ttl=64 time=0.110 ms</span><br><span class="line"></span><br><span class="line">--- 10.10.200.10 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.090/0.115/0.148/0.020 ms</span><br></pre></td></tr></table></figure><p>我们试着来分析下上述这个过程。</p><p>1、首先 ping 命令构建一个 ICMP 请求包，ICMP 包封装在 IP 包中，源目的 IP 地址分别为 <code>tun1(10.10.100.10)</code> 和 <code>tun2(10.10.200.10)</code> 的地址。</p><p>2、由于 tun1 和 tun2 不在同一网段，所以会查路由表，当通过 <code>ip tunnel</code> 命令建立 <code>ipip</code> 隧道之后，会自动生成一条路由，如下，表明去往目的地 <code>10.10.200.10</code> 的路由直接从 tun1 出去。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]<span class="comment"># ip netns exec ns1 route -n</span></span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">10.10.10.0      0.0.0.0         255.255.255.0   U     0      0        0 v1</span><br><span class="line">10.10.20.0      10.10.10.1      255.255.255.0   UG    0      0        0 v1</span><br><span class="line">10.10.200.10    0.0.0.0         255.255.255.255 UH    0      0        0 tun1</span><br></pre></td></tr></table></figure><p>3、由于配置了隧道端点，数据包出了 tun1，到达 v1，根据 <code>ipip</code> 隧道的配置，会封装上一层新的 IP 报头，源目的 IP 地址分别为 <code>v1(10.10.10.2)</code> 和 <code>v2(10.10.20.2)</code>。</p><p>4、v1 和 v2 同样不在一个网段，同样查路由表，发现去往 <code>10.10.20.0</code> 网段可以从 <code>10.10.10.1</code> 网关发出。</p><p>5、Linux 打开了 <code>ip_forward</code>，相当于一台路由器，<code>10.10.10.0</code> 和 <code>10.10.20.0</code> 是两条直连路由，所以直接查表转发，从 NS1 过渡到 NS2。</p><p>6、数据包到达 NS2 的 v2，解封装数据包，发现内层 IP 报文的目的 IP 地址是 <code>10.10.200.10</code>，这正是自己配置的 <code>ipip</code> 隧道的 tun2 地址，于是就将报文交给 tun2 设备。至此，tun1 的 ping 请求包就成功到达 tun2。</p><p>7、由于 ICMP 报文的传输特性，有去必有回，所以 NS2 上会构造 ICMP 响应包，并根据以上相同步骤封装和解封装数据包，直至到达 tun1，整个 ping 过程完成。</p><p>以上便是大体的 <code>ipip</code> 隧道通信过程，下面我们可以再抓包进一步验证。</p><p>如下是通过 wireshark 抓取的 v1 口的包：</p><p><img src="/images/virt/ipippcap.jpeg" alt="ipippcap.png"></p><p>可以看到，有两层 IP 报文头，外层使用的 <code>ipip</code> 协议构成隧道的端点，内层是正常的通信报文，封装了 ICMP 报文作为 payload。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现在的 Linux 内核原生支持 5 种隧道协议，它们底层实现都是采用 tun 虚拟设备。</p><p>我们熟知的各种 VPN 软件，其底层实现都离不开这 5 种隧道协议。</p><p>我们可以把上面的 <code>ipip</code> 改成其他隧道模式，其他不变，同样可以完成不同隧道的实验。</p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> tunnel </tag>
            
            <tag> ipip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux云网络基础之 Linux 虚拟路由</title>
      <link href="/2019/03/14/tech/net/vnet/Linux%E8%B7%AF%E7%94%B1%E5%8A%9F%E8%83%BD/"/>
      <url>/2019/03/14/tech/net/vnet/Linux%E8%B7%AF%E7%94%B1%E5%8A%9F%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>之前文章有读者留言：</p><p><img src="/images/virt/vrouterqa.jpeg" alt="vrouterqa.png"></p><p>我的回答基本上是一句废话，因为只要你知道点网络的基础知识，肯定知道这种情况要走三层路由。</p><p>但知道归知道，不实践永远不知道自己是不是真的知道（有点绕），我相信那位读者也是希望我能讲讲这其中具体是怎么路由的，今天这篇文章就来说说这个。</p><a id="more"></a><h2 id="Linux-本身就是一台路由器"><a href="#Linux-本身就是一台路由器" class="headerlink" title="Linux 本身就是一台路由器"></a>Linux 本身就是一台路由器</h2><p>前面的文章我们学习了多种虚拟的网络设备，包括网卡、交换机等，也了解了怎么用工具来操作这些设备，那么，回到今天的主题，路由器有没有对应的虚拟设备，能不能也用相关工具来操作呢，这个答案如果要深究的话，也是有的，比如 OpenStack 的 DVR、一些开源的虚拟路由器实现等等。</p><p>不过我们不做那么深究的讨论，简化问题，Linux 系统实际上没有实现相关的虚拟路由器设备，自然也没有工具可以操作路由器，因为 <strong>Linux 本身就是一台路由器</strong>。</p><p>Linux 提供一个开关来操作路由功能，就是 <code>/proc/sys/net/ipv4/ip_forward</code>，默认这个开关是关的，打开只需：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure><p>但这种打开方式只是临时的，如果要一劳永逸，可以修改配置文件 <code>/etc/sysctl.conf</code>，添加或修改项 <code>net.ipv4.ip_forward</code> 为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure><p>即可。</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>为了降低大家实践的难度，我们就不创建虚拟机了，直接使用 namespace，一条 <code>ip</code> 命令就可以搞定所有的操作。</p><p>我们按照下面的图示进行操作（NS1 和 NS2 分布在不同网段）：</p><p><img src="/images/virt/vrouter.jpeg" alt=""></p><p>创建两个 namespace：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip netns add ns1</span><br><span class="line">ip netns add ns2</span><br></pre></td></tr></table></figure><p>创建两对 veth-pair，一端分别挂在两个 namespace 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ip link add v1 type veth peer name v1_r</span><br><span class="line">ip link add v2 type veth peer name v2_r</span><br><span class="line"></span><br><span class="line">ip link set v1 netns ns1</span><br><span class="line">ip link set v2 netns ns2</span><br></pre></td></tr></table></figure><p>分别给两对 veth-pair 端点配上 IP 并启用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ip a a 10.10.10.1/24 dev v1_r</span><br><span class="line">ip l s v1_r up</span><br><span class="line">ip a a 10.10.20.1/24 dev v2_r</span><br><span class="line">ip l s v2_r up</span><br><span class="line"></span><br><span class="line">ip netns exec ns1 ip a a 10.10.10.2/24 dev v1</span><br><span class="line">ip netns exec ns1 ip l s v1 up</span><br><span class="line">ip netns exec ns2 ip a a 10.10.20.2/24 dev v2</span><br><span class="line">ip netns exec ns2 ip l s v2 up</span><br></pre></td></tr></table></figure><p>验证一下： v1 ping v2，结果不通。</p><p>看下 <code>ip_forward</code> 的值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]# cat /proc/sys/net/ipv4/ip_forward</span><br><span class="line">0</span><br></pre></td></tr></table></figure><p>没开路由怎么通，改为 1 再试，还是不通。</p><p>看下 ns1 的路由表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]# ip netns exec ns1 route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">10.10.10.0      0.0.0.0         255.255.255.0   U     0      0        0 v1</span><br></pre></td></tr></table></figure><p>只有一条直连路由，没有去往 <code>10.10.20.0/24</code> 网段的路由，怎么通？那就给它配一条：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]# ip netns exec ns1 route add -net 10.10.20.0 netmask 255.255.255.0 gw 10.10.10.1</span><br><span class="line"></span><br><span class="line">[root@by ~]# ip netns exec ns1 route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">10.10.10.0      0.0.0.0         255.255.255.0   U     0      0        0 v1</span><br><span class="line">10.10.20.0      10.10.10.1      255.255.255.0   UG    0      0        0 v1</span><br></pre></td></tr></table></figure><p>同理也给 ns2 配上去往 <code>10.10.10.0/24</code> 网段的路由。</p><p>最后再 ping，成功了！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]# ip netns exec ns1 ping 10.10.20.2</span><br><span class="line">PING 10.10.20.2 (10.10.20.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.10.20.2: icmp_seq=1 ttl=63 time=0.071 ms</span><br><span class="line">64 bytes from 10.10.20.2: icmp_seq=2 ttl=63 time=0.070 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.10.20.2 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.070/0.070/0.071/0.008 ms</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Linux 本身是一台路由器。</p><p>上面的实验使用 namespace 效果和使用虚拟机是一样的，关键是知道有这个功能，知道怎么用就差不多了。</p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> vrouter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux云网络基础之虚拟网络设备 Bridge 详解</title>
      <link href="/2019/03/10/tech/net/vnet/bridge%E8%AF%A6%E8%A7%A3/"/>
      <url>/2019/03/10/tech/net/vnet/bridge%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>前面几篇文章介绍了 tap/tun、veth-pair，今天这篇来看看 Bridge。</p><h2 id="Bridge-是什么"><a href="#Bridge-是什么" class="headerlink" title="Bridge 是什么"></a>Bridge 是什么</h2><p>同 tap/tun、veth-pair 一样，Bridge 也是一种虚拟网络设备，所以具备虚拟网络设备的所有特性，比如可以配置 IP、MAC 等。</p><p>除此之外，Bridge 还是一个交换机，具有交换机所有的功能。</p><a id="more"></a><p>对于普通的网络设备，就像一个管道，只有两端，数据从一端进，从另一端出。而 Bridge 有多个端口，数据可以从多个端口进，从多个端口出。</p><p>Bridge 的这个特性让它可以接入其他的网络设备，比如物理设备、虚拟设备、VLAN 设备等。Bridge 通常充当主设备，其他设备为从设备，这样的效果就等同于物理交换机的端口连接了一根网线。比如下面这幅图通过 Bridge 连接两个 VM 的 tap 虚拟网卡和物理网卡 eth0。</p><p><img src="/images/virt/vmbrtap.png" alt="vmbrtap"></p><h2 id="VM-同主机通信"><a href="#VM-同主机通信" class="headerlink" title="VM 同主机通信"></a>VM 同主机通信</h2><p>以这个图来简单说明下，借助 Bridge 来完成同主机两台 VM 的之间的通信流程。</p><p>首先准备一个 centos 或 ubuntu 虚拟机，然后创建一个 bridge：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip link add br0 type bridge</span><br><span class="line">ip link set br0 up</span><br></pre></td></tr></table></figure><p>然后通过 <code>virt-manager</code> 创建两个 kvm 虚拟机：kvm1 和 kvm2（前提得支持嵌套虚拟化），将它们的 vNIC 挂到 br0 上，如下图：</p><p><img src="/images/virt/kvm1br0.jpeg" alt="kvm1br0.png"></p><p>kvm 虚机会使用 tap 设备作为它的虚拟网卡，我们验证下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ps -ef | grep kvm1</span><br><span class="line">libvirt+      3549     1  87 ?        00:22:09 qemu-system-x86_64 -enable-kvm -name kvm1 ... -netdev tap,fd=26,id=hostnet0,vhost=on,vhostfd=28 ...</span><br></pre></td></tr></table></figure><p>可以看到，其中网络部分参数，<code>-netdev tap,fd=26</code> 表示的就是连接主机上的 tap 设备。</p><p>创建的 fd=26 为读写 <code>/dev/net/tun</code> 的文件描述符。</p><p>使用 <code>lsof -p 3549</code> 验证下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># lsof -p 3549</span><br><span class="line">COMMAND    PID USER   FD      TYPE             DEVICE    SIZE/OFF     NODE NAME</span><br><span class="line">...</span><br><span class="line">qemu-system 3549  libvirt-qemu   26u      CHR             10,200         0t107    135 /dev/net/tun</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到，PID 为 3549 的进程打开了文件 <code>/dev/net/tun</code>，分配的文件描述符 fd 为 26。</p><p>因此，我们可以得出以下结论：在 kvm 虚机启动时，会向内核注册 tap 虚拟网卡，同时打开设备文件 <code>/dev/net/tun</code>，拿到文件描述符 fd，然后将 fd 和 tap 关联，tap 就成了一端连接着用户空间的 qemu-kvm，一端连着主机上的 bridge 的端口，促使两者完成通信。</p><p>下面分别给两虚机配上 IP：<code>10.1.1.2/24</code> 和 <code>10.1.1.3/24</code>，ping 一下：</p><p><img src="/images/virt/kvmping.png" alt="kvmping.png"></p><p>在 bridge 上抓个包看看：</p><p><img src="/images/virt/tcpdumpbro.jpeg" alt="tcpdumpbro.png"></p><p>可以看到，br0 上抓到 ping 的 ICMP echo 包和 ARP 包。</p><h2 id="Bridge-常用使用场景"><a href="#Bridge-常用使用场景" class="headerlink" title="Bridge 常用使用场景"></a>Bridge 常用使用场景</h2><p>Bridge 设备通常就是结合 tap/tun、veth-pair 设备用于虚拟机、容器网络里面。这两种网络，在数据传输流程上还有些许不同，我们简单来看下：</p><p>首先是虚拟机网络，虚拟机一般通过 tap/tun 设备将虚拟机网卡同宿主机里的 Bridge 连接起来，完成同主机和跨主机的通信。如下图所示：</p><p><img src="/images/virt/vmnet.jpeg" alt="vmnet.png"></p><p>【图片来源于网络，侵权必删】</p><p>虚拟机发出的数据包通过 tap 设备先到达 br0，然后经过 eth0 发送到物理网络中，数据包不需要经过主机的的协议栈，效率是比较高的。</p><p>其次是容器网络（容器网络有多种引申的形式，这里我们只说 Bridge 网络），容器网络和虚拟机网络类似，不过一般是使用 veth-pair 来连接容器和主机，因为在主机看来，容器就是一个个被隔离的 namespace，用 veth-pair 更有优势。如下图所示：</p><p><img src="/images/virt/dockernet.jpeg" alt="dockernet.png"></p><p>【图片来源于网络，侵权必删】</p><p>容器的 Bridge 网络通常配置成内网形式，要出外网需要走 NAT，所以它的数据传输不像虚拟机的桥接形式可以直接跨过协议栈，而是必须经过协议栈，通过 NAT 和 ip_forward 功能从物理网卡转发出去，因此，从性能上看，Bridge 网络虚拟机要优于容器。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Linux Bridge 是虚拟交换机，功能和物理交换机一样，用于连接虚拟机和容器。</p><p>虚拟机网络和容器网络的区别。</p><p>Bridge 是偏低级的工具，更高级的工具是 Open vSwitch，这个工具后面再详说。</p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> Bridge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文总结虚拟网络设备 eth, tap/tun, veth-pair</title>
      <link href="/2019/03/08/tech/net/vnet/eth-taptun-vethpair%E6%80%BB%E7%BB%93/"/>
      <url>/2019/03/08/tech/net/vnet/eth-taptun-vethpair%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><blockquote><p>本文翻译自：<a href="http://t.cn/EIdjMTc" target="_blank" rel="noopener">http://t.cn/EIdjMTc</a></p></blockquote><p>Linux 虚拟网络的背后都是由一个个的虚拟设备构成的。虚拟化技术没出现之前，计算机网络系统都只包含物理的网卡设备，通过网卡适配器，线缆介质，连接外部网络，构成庞大的 Internet。</p><p><img src="/images/net/virtual-device-physical-2.png" alt="virtual-device-physical-2.png"></p><a id="more"></a><p>然而，随着虚拟化技术的出现，网络也随之被虚拟化，相较于单一的物理网络，虚拟网络变得非常复杂，在一个主机系统里面，需要实现诸如交换、路由、隧道、隔离、聚合等多种网络功能。</p><p>而实现这些功能的基本元素就是虚拟的网络设备，比如 tap、tun 和 veth-pair。</p><h2 id="tap-tun"><a href="#tap-tun" class="headerlink" title="tap/tun"></a>tap/tun</h2><p>tap/tun 提供了一台主机内用户空间的数据传输机制。它虚拟了一套网络接口，这套接口和物理的接口无任何区别，可以配置 IP，可以路由流量，不同的是，它的流量只在主机内流通。</p><p>tap/tun 有些许的不同，tun 只操作三层的 IP 包，而 tap 操作二层的以太网帧。</p><p><img src="/images/net/virtual-device-tuntap-4.png" alt="virtual-device-tuntap-4.png"></p><h2 id="veth-pair"><a href="#veth-pair" class="headerlink" title="veth-pair"></a>veth-pair</h2><p>veth-pair 是成对出现的一种虚拟网络设备，一端连接着协议栈，一端连接着彼此，数据从一端出，从另一端进。</p><p>它的这个特性常常用来连接不同的虚拟网络组件，构建大规模的虚拟网络拓扑，比如连接 Linux Bridge、OVS、LXC 容器等。</p><p>一个很常见的案例就是它被用于 OpenStack Neutron，构建非常复杂的网络形态。</p><p><img src="/images/net/virtual-device-veth-1.png" alt="virtual-device-veth-1.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后，总结一下，我们提到几种网络设备，eth0、tap、tun、veth-pair，这些都构成了如今云网络必不可少的元素。</p><p><img src="/images/net/virtual-devices-all-4.png" alt="virtual-devices-all-4.png"></p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> tap </tag>
            
            <tag> tun </tag>
            
            <tag> veth-pair </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux云网络基础之虚拟网络设备 veth-pair 详解</title>
      <link href="/2019/03/03/tech/net/vnet/veth-pair%E8%AF%A6%E8%A7%A3/"/>
      <url>/2019/03/03/tech/net/vnet/veth-pair%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>前面这篇文章介绍了 tap/tun 设备之后，大家应该对虚拟网络设备有了一定的了解，本文来看另外一种虚拟网络设备 veth-pair。</p><h2 id="01-veth-pair-是什么"><a href="#01-veth-pair-是什么" class="headerlink" title="01 veth-pair 是什么"></a>01 veth-pair 是什么</h2><p>顾名思义，veth-pair 就是一对的虚拟设备接口，和 tap/tun 设备不同的是，它都是成对出现的。一端连着协议栈，一端彼此相连着。如下图所示：</p><p><img src="/images/net/veth.png" alt="veth"></p><a id="more"></a><p>正因为有这个特性，它常常充当着一个桥梁，连接着各种虚拟网络设备，典型的例子像“两个 namespace 之间的连接”，“Bridge、OVS 之间的连接”，“Docker 容器之间的连接” 等等，以此构建出非常复杂的虚拟网络结构，比如 OpenStack Neutron。</p><h2 id="02-veth-pair-的连通性"><a href="#02-veth-pair-的连通性" class="headerlink" title="02 veth-pair 的连通性"></a>02 veth-pair 的连通性</h2><p>我们给上图中的 veth0 和 veth1 分别配上 IP：10.1.1.2 和 10.1.1.3，然后从 veth0 ping 一下 veth1。理论上它们处于同网段，是能 ping 通的，但结果却是 ping 不通。</p><p>抓个包看看，<code>tcpdump -nnt -i veth0</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# tcpdump -nnt -i veth0</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on veth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">ARP, Request who-has 10.1.1.3 tell 10.1.1.2, length 28</span><br><span class="line">ARP, Request who-has 10.1.1.3 tell 10.1.1.2, length 28</span><br></pre></td></tr></table></figure><p>可以看到，由于 veth0 和 veth1 处于同一个网段，且是第一次连接，所以会事先发 ARP 包，但 veth1 并没有响应 ARP 包。</p><p>经查阅，这是由于我使用的 Ubuntu 系统内核中一些 ARP 相关的默认配置限制所导致的，需要修改一下配置项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/conf/veth1/accept_local</span><br><span class="line">echo 1 &gt; /proc/sys/net/ipv4/conf/veth0/accept_local</span><br><span class="line">echo 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter</span><br><span class="line">echo 0 &gt; /proc/sys/net/ipv4/conf/veth0/rp_filter</span><br><span class="line">echo 0 &gt; /proc/sys/net/ipv4/conf/veth1/rp_filter</span><br></pre></td></tr></table></figure><p>完了再 ping 就行了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# ping -I veth0 10.1.1.3 -c 2</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) from 10.1.1.2 veth0: 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.047 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.064 ms</span><br><span class="line"></span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 3008ms</span><br><span class="line">rtt min/avg/max/mdev = 0.047/0.072/0.113/0.025 ms</span><br></pre></td></tr></table></figure><p>我们对这个通信过程比较感兴趣，可以抓包看看。</p><p>对于 veth0 口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# tcpdump -nnt -i veth0</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on veth0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">ARP, Request who-has 10.1.1.3 tell 10.1.1.2, length 28</span><br><span class="line">ARP, Reply 10.1.1.3 is-at 5a:07:76:8e:fb:cd, length 28</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 1, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 2, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 3, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2244, seq 1, length 64</span><br></pre></td></tr></table></figure><p>对于 veth1 口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# tcpdump -nnt -i veth1</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on veth1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">ARP, Request who-has 10.1.1.3 tell 10.1.1.2, length 28</span><br><span class="line">ARP, Reply 10.1.1.3 is-at 5a:07:76:8e:fb:cd, length 28</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 1, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 2, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2189, seq 3, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 2244, seq 1, length 64</span><br></pre></td></tr></table></figure><p>奇怪，我们并没有看到 ICMP 的 <code>echo reply</code> 包，那它是怎么 ping 通的？</p><p>其实这里 <code>echo reply</code> 走的是 localback 口，不信抓个包看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# tcpdump -nnt -i lo</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on lo, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 2244, seq 1, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 2244, seq 2, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 2244, seq 3, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 2244, seq 4, length 64</span><br></pre></td></tr></table></figure><p>为什么？</p><p>我们看下整个通信流程就明白了。</p><ol><li>首先 ping 程序构造 ICMP <code>echo request</code>，通过 socket 发给协议栈。</li><li>由于 ping 指定了走 veth0 口，如果是第一次，则需要发 ARP 请求，否则协议栈直接将数据包交给 veth0。</li><li>由于 veth0 连着 veth1，所以 ICMP request 直接发给 veth1。</li><li>veth1 收到请求后，交给另一端的协议栈。</li><li>协议栈看本地有 10.1.1.3 这个 IP，于是构造 ICMP reply 包，查看路由表，发现回给 10.1.1.0 网段的数据包应该走 localback 口，于是将 reply 包交给 lo 口（会优先查看路由表的 0 号表，<code>ip route show table 0</code> 查看）。</li><li>lo 收到协议栈的 reply 包后，啥都没干，转手又回给协议栈。</li><li>协议栈收到 reply 包之后，发现有 socket 在等待包，于是将包给 socket。</li><li>等待在用户态的 ping 程序发现 socket 返回，于是就收到 ICMP 的 reply 包。</li></ol><p>整个过程如下图所示：</p><p><img src="/images/virt/pingveth.jpeg" alt="pingveth"></p><h2 id="03-两个-namespace-之间的连通性"><a href="#03-两个-namespace-之间的连通性" class="headerlink" title="03 两个 namespace 之间的连通性"></a>03 两个 namespace 之间的连通性</h2><p>namespace 是 Linux 2.6.x 内核版本之后支持的特性，主要用于资源的隔离。有了 namespace，一个 Linux 系统就可以抽象出多个网络子系统，各子系统间都有自己的网络设备，协议栈等，彼此之间互不影响。</p><p>如果各个 namespace 之间需要通信，怎么办呢，答案就是用 veth-pair 来做桥梁。</p><p>根据连接的方式和规模，可以分为“直接相连”，“通过 Bridge 相连” 和 “通过 OVS 相连”。</p><h3 id="3-1-直接相连"><a href="#3-1-直接相连" class="headerlink" title="3.1 直接相连"></a>3.1 直接相连</h3><p>直接相连是最简单的方式，如下图，一对 veth-pair 直接将两个 namespace 连接在一起。</p><p><img src="/images/virt/linuxswitch-veth.png" alt="linuxswitch-veth"></p><p>给 veth-pair 配置 IP，测试连通性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 创建 namespace</span><br><span class="line">ip netns a ns1</span><br><span class="line">ip netns a ns2</span><br><span class="line"></span><br><span class="line"># 创建一对 veth-pair veth0 veth1</span><br><span class="line">ip l a veth0 type veth peer name veth1</span><br><span class="line"></span><br><span class="line"># 将 veth0 veth1 分别加入两个 ns</span><br><span class="line">ip l s veth0 netns ns1</span><br><span class="line">ip l s veth1 netns ns2</span><br><span class="line"></span><br><span class="line"># 给两个 veth0 veth1 配上 IP 并启用</span><br><span class="line">ip netns exec ns1 ip a a 10.1.1.2/24 dev veth0</span><br><span class="line">ip netns exec ns1 ip l s veth0 up</span><br><span class="line">ip netns exec ns2 ip a a 10.1.1.3/24 dev veth1</span><br><span class="line">ip netns exec ns2 ip l s veth1 up</span><br><span class="line"></span><br><span class="line"># 从 veth0 ping veth1</span><br><span class="line">[root@localhost ~]# ip netns exec ns1 ping 10.1.1.3</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.073 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.068 ms</span><br><span class="line"></span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">15 packets transmitted, 15 received, 0% packet loss, time 14000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.068/0.084/0.201/0.032 ms</span><br></pre></td></tr></table></figure><h3 id="3-2-通过-Bridge-相连"><a href="#3-2-通过-Bridge-相连" class="headerlink" title="3.2 通过 Bridge 相连"></a>3.2 通过 Bridge 相连</h3><p>Linux Bridge 相当于一台交换机，可以中转两个 namespace 的流量，我们看看 veth-pair 在其中扮演什么角色。</p><p>如下图，两对 veth-pair 分别将两个 namespace 连到 Bridge 上。</p><p><img src="/images/virt/linuxswitch-ovs-veth.png" alt="linuxswitch-ovs-veth"></p><p>同样给 veth-pair 配置 IP，测试其连通性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># 首先创建 bridge br0</span><br><span class="line">ip l a br0 type bridge</span><br><span class="line">ip l s br0 up </span><br><span class="line"></span><br><span class="line"># 然后创建两对 veth-pair</span><br><span class="line">ip l a veth0 type veth peer name br-veth0</span><br><span class="line">ip l a veth1 type veth peer name br-veth1</span><br><span class="line"></span><br><span class="line"># 分别将两对 veth-pair 加入两个 ns 和 br0</span><br><span class="line">ip l s veth0 netns ns1</span><br><span class="line">ip l s br-veth0 master br0</span><br><span class="line">ip l s br-veth0 up</span><br><span class="line"></span><br><span class="line">ip l s veth1 netns ns2</span><br><span class="line">ip l s br-veth1 master br0</span><br><span class="line">ip l s br-veth1 up</span><br><span class="line"></span><br><span class="line"># 给两个 ns 中的 veth 配置 IP 并启用</span><br><span class="line">ip netns exec ns1 ip a a 10.1.1.2/24 dev veth0</span><br><span class="line">ip netns exec ns1 ip l s veth0 up</span><br><span class="line"></span><br><span class="line">ip netns exec ns2 ip a a 10.1.1.3/24 dev veth1</span><br><span class="line">ip netns exec ns2 ip l s veth1 up</span><br><span class="line"></span><br><span class="line"># veth0 ping veth1</span><br><span class="line">[root@localhost ~]# ip netns exec ns1 ping 10.1.1.3</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.060 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.105 ms</span><br><span class="line"></span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.060/0.082/0.105/0.024 ms</span><br></pre></td></tr></table></figure><h3 id="3-3-通过-OVS-相连"><a href="#3-3-通过-OVS-相连" class="headerlink" title="3.3 通过 OVS 相连"></a>3.3 通过 OVS 相连</h3><p>OVS 是第三方开源的 Bridge，功能比 Linux Bridge 要更强大，对于同样的实验，我们用 OVS 来看看是什么效果。</p><p>如下图所示：</p><p><img src="/images/virt/linuxswitch-ovs.png" alt="linuxswitch-ovs"></p><p>同样测试两个 namespace 之间的连通性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 用 ovs 提供的命令创建一个 ovs bridge</span><br><span class="line">ovs-vsctl add-br ovs-br</span><br><span class="line"></span><br><span class="line"># 创建两对 veth-pair</span><br><span class="line">ip l a veth0 type veth peer name ovs-veth0</span><br><span class="line">ip l a veth1 type veth peer name ovs-veth1</span><br><span class="line"></span><br><span class="line"># 将 veth-pair 两端分别加入到 ns 和 ovs bridge 中</span><br><span class="line">ip l s veth0 netns ns1</span><br><span class="line">ovs-vsctl add-port ovs-br ovs-veth0</span><br><span class="line">ip l s ovs-veth0 up</span><br><span class="line"></span><br><span class="line">ip l s veth1 netns ns2</span><br><span class="line">ovs-vsctl add-port ovs-br ovs-veth1</span><br><span class="line">ip l s ovs-veth1 up</span><br><span class="line"></span><br><span class="line"># 给 ns 中的 veth 配置 IP 并启用</span><br><span class="line">ip netns exec ns1 ip a a 10.1.1.2/24 dev veth0</span><br><span class="line">ip netns exec ns1 ip l s veth0 up</span><br><span class="line"></span><br><span class="line">ip netns exec ns2 ip a a 10.1.1.3/24 dev veth1</span><br><span class="line">ip netns exec ns2 ip l s veth1 up</span><br><span class="line"></span><br><span class="line"># veth0 ping veth1</span><br><span class="line">[root@localhost ~]# ip netns exec ns1 ping 10.1.1.3</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.311 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.087 ms</span><br><span class="line">^C</span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.087/0.199/0.311/0.112 ms</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>veth-pair 在虚拟网络中充当着桥梁的角色，连接多种网络设备构成复杂的网络。</p><p>veth-pair 的三个经典实验，直接相连、通过 Bridge 相连和通过 OVS 相连。</p><p><strong>参考：</strong></p><p><a href="http://www.opencloudblog.com/?p=66" target="_blank" rel="noopener">http://www.opencloudblog.com/?p=66</a></p><p><a href="https://segmentfault.com/a/1190000009251098" target="_blank" rel="noopener">https://segmentfault.com/a/1190000009251098</a></p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> veth-pair </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何用 tap/tun 设备编写一个 ICMP 程序</title>
      <link href="/2019/03/01/tech/net/vnet/%E5%9F%BA%E4%BA%8Etaptun%E5%86%99%E4%B8%80%E4%B8%AAICMP%E7%A8%8B%E5%BA%8F/"/>
      <url>/2019/03/01/tech/net/vnet/%E5%9F%BA%E4%BA%8Etaptun%E5%86%99%E4%B8%80%E4%B8%AAICMP%E7%A8%8B%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>前面两篇文章已经介绍过 tap/tun 的<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247484961&amp;idx=1&amp;sn=f26d7994f57abbf5de2007a2f451d9f5&amp;chksm=ea743299dd03bb8f6ac063c1cb00d5a592094c7778ab0a5baf37b7469fa3eb018101ef34551f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">原理</a>和<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247484976&amp;idx=1&amp;sn=a3c5112bea36c8a543660c7ac1497b36&amp;chksm=ea743288dd03bb9e15e7971894c80151e504ad5912d660204d6a3ea140cb3e6f55af245d50f2&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">配置工具</a>。这篇文章通过一个编程示例来深入了解 tap/tun 的程序结构。</p><a id="more"></a><h2 id="01-准备工作"><a href="#01-准备工作" class="headerlink" title="01 准备工作"></a>01 准备工作</h2><p>首先通过 <code>modinfo tun</code> 查看系统内核是否支持 tap/tun 设备驱动。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@by ~]# modinfo tun</span><br><span class="line">filename:       /lib/modules/3.10.0-862.14.4.el7.x86_64/kernel/drivers/net/tun.ko.xz</span><br><span class="line">alias:          devname:net/tun</span><br><span class="line">alias:          char-major-10-200</span><br><span class="line">license:        GPL</span><br><span class="line">author:         (C) 1999-2004 Max Krasnyansky &lt;maxk@qualcomm.com&gt;</span><br><span class="line">description:    Universal TUN/TAP device driver</span><br><span class="line">retpoline:      Y</span><br><span class="line">rhelversion:    7.5</span><br><span class="line">srcversion:     50878D5D5A0138445B25AA8</span><br><span class="line">depends:</span><br><span class="line">intree:         Y</span><br><span class="line">vermagic:       3.10.0-862.14.4.el7.x86_64 SMP mod_unload modversions</span><br><span class="line">signer:         CentOS Linux kernel signing key</span><br><span class="line">sig_key:        E4:A1:B6:8F:46:8A:CA:5C:22:84:50:53:18:FD:9D:AD:72:4B:13:03</span><br><span class="line">sig_hashalgo:   sha256</span><br></pre></td></tr></table></figure><p>在 linux 2.4 及之后的内核版本中，tun/tap 驱动是默认编译进内核中的。</p><p>如果你的系统不支持，请先选择手动编译内核或者升级内核。编译时开启下面的选项即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Device Drivers =&gt; Network device support =&gt; Universal TUN/TAP device driver support</span><br></pre></td></tr></table></figure><p>tap/tun 也支持编译成模块，如果编译成模块，需要手动加载它：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# modprobe tun</span><br><span class="line">[root@localhost ~]# lsmod | grep tun</span><br><span class="line">tun                    31665  0</span><br></pre></td></tr></table></figure><p>关于以上的详细步骤，网上有很多教程，这里就不再赘述了。</p><p><a href="https://blog.csdn.net/lishuhuakai/article/details/70305543" target="_blank" rel="noopener">https://blog.csdn.net/lishuhuakai/article/details/70305543</a></p><p>上面只是加载了 tap/tun 模块，要完成 tap/tun 的编码，还需要有设备文件，运行命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mknod /dev/net/tun c 10 200 # c表示为字符设备，10和200分别是主设备号和次设备号</span><br></pre></td></tr></table></figure><p>这样在 <code>/dev/net</code> 下就创建了一个名为 tun 的文件。</p><h2 id="02-编程示例"><a href="#02-编程示例" class="headerlink" title="02 编程示例"></a>02 编程示例</h2><h3 id="2-1-启动设备"><a href="#2-1-启动设备" class="headerlink" title="2.1 启动设备"></a>2.1 启动设备</h3><p>使用 tap/tun 设备，需要先进行一些初始化工作，如下代码所示：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tun_alloc</span><span class="params">(<span class="keyword">char</span> *dev, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    assert(dev != <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ifreq</span> <span class="title">ifr</span>;</span></span><br><span class="line">    <span class="keyword">int</span> fd, err;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">char</span> *clonedev = <span class="string">"/dev/net/tun"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((fd = open(clonedev, O_RDWR)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> fd;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(&amp;ifr, <span class="number">0</span>, <span class="keyword">sizeof</span>(ifr));</span><br><span class="line">    ifr.ifr_flags = flags;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (*dev != <span class="string">'\0'</span>) &#123;</span><br><span class="line">        <span class="built_in">strncpy</span>(ifr.ifr_name, dev, IFNAMSIZ);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> ((err = ioctl(fd, TUNSETIFF, (<span class="keyword">void</span> *) &amp;ifr)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span> err;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一旦设备开启成功，系统会给设备分配一个名称，对于tun设备，一般为tunX，X为从0开始的编号；</span></span><br><span class="line">    <span class="comment">// 对于tap设备，一般为tapX</span></span><br><span class="line">    <span class="built_in">strcpy</span>(dev, ifr.ifr_name);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fd;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先打开字符设备文件 <code>/dev/net/tun</code>，然后用 <code>ioctl</code> 注册设备的工作模式，是 tap 还是 tun。这个模式由结构体 <code>struct ifreq</code> 的属性 <code>ifr_flags</code> 来定义，它有以下表示：</p><ul><li>IFF_TUN: 表示创建一个 tun 设备。</li><li>IFF_TAP: 表示创建一个 tap 设备。</li><li>IFF_NO_PI: 表示不包含包头信息，默认的，每个数据包传到用户空间时，都会包含一个附加的包头来保存包信息，这个表示不加包头。</li><li>IFF_ONE_QUEUE：表示采用单一队列模式。</li></ul><p>还是有一个属性是 <code>ifr_name</code>，表示设备的名字，它可以由用户自己指定，也可以由系统自动分配，比如 <code>tapX</code>、<code>tunX</code>，X 从 0 开始编号。</p><p><code>ioctl</code> 完了之后，文件描述符 fd 就和设备建立起了关联，之后就可以根据 fd 进行 read 和 write 操作了。</p><h3 id="2-2-写一个-ICMP-的调用函数"><a href="#2-2-写一个-ICMP-的调用函数" class="headerlink" title="2.2 写一个 ICMP 的调用函数"></a>2.2 写一个 ICMP 的调用函数</h3><p>为了测试上面的程序，我们写一个简单的 ICMP echo 程序。我们会使用 tun 设备，然后给 <code>tunX</code> 接口发送一个 ping 包，程序简单响应这个包，完成 ICMP 的 request 和 reply 的功能。</p><p>如下代码所示：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tun_fd, nread;</span><br><span class="line">    <span class="keyword">char</span> buffer[<span class="number">4096</span>];</span><br><span class="line">    <span class="keyword">char</span> tun_name[IFNAMSIZ];</span><br><span class="line"></span><br><span class="line">    tun_name[<span class="number">0</span>] = <span class="string">'\0'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Flags: IFF_TUN   - TUN device (no Ethernet headers)</span></span><br><span class="line"><span class="comment">     *        IFF_TAP   - TAP device</span></span><br><span class="line"><span class="comment">     *        IFF_NO_PI - Do not provide packet information</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    tun_fd = tun_alloc(tun_name, IFF_TUN | IFF_NO_PI);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tun_fd &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        perror(<span class="string">"Allocating interface"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Open tun/tap device: %s for reading...\n"</span>, tun_name);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">char</span> ip[<span class="number">4</span>];</span><br><span class="line">        <span class="comment">// 收包</span></span><br><span class="line">        nread = read(tun_fd, buffer, <span class="keyword">sizeof</span>(buffer));</span><br><span class="line">        <span class="keyword">if</span> (nread &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            perror(<span class="string">"Reading from interface"</span>);</span><br><span class="line">            close(tun_fd);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Read %d bytes from tun/tap device\n"</span>, nread);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 简单对收到的包调换一下顺序</span></span><br><span class="line">        <span class="built_in">memcpy</span>(ip, &amp;buffer[<span class="number">12</span>], <span class="number">4</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;buffer[<span class="number">12</span>], &amp;buffer[<span class="number">16</span>], <span class="number">4</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;buffer[<span class="number">16</span>], ip, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        buffer[<span class="number">20</span>] = <span class="number">0</span>;</span><br><span class="line">        *((<span class="keyword">unsigned</span> <span class="keyword">short</span> *)&amp;buffer[<span class="number">22</span>]) += <span class="number">8</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 发包</span></span><br><span class="line">        nread = write(tun_fd, buffer, nread);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Write %d bytes to tun/tap device, that's %s\n"</span>, nread, buffer);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面测试一下。</p><h3 id="2-3-给-tap-tun-设备配置-IP-地址"><a href="#2-3-给-tap-tun-设备配置-IP-地址" class="headerlink" title="2.3 给 tap/tun 设备配置 IP 地址"></a>2.3 给 tap/tun 设备配置 IP 地址</h3><p>编译：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost coding]# gcc -o taptun taptun.c</span><br><span class="line">[root@localhost coding]# ./taptun</span><br><span class="line">Open tun/tap device: tun0 for reading...</span><br></pre></td></tr></table></figure><p>开另一个终端，查看生成了 <code>tun0</code> 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost coding]# ip a</span><br><span class="line">6: tun0: &lt;POINTOPOINT,MULTICAST,NOARP&gt; mtu 1500 qdisc noop state DOWN qlen 500</span><br><span class="line">    link/none</span><br></pre></td></tr></table></figure><p>给 <code>tun0</code> 接口配置 IP 并启用，比如 <code>10.1.1.2/24</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip a a 10.1.1.2/24 dev tun0</span><br><span class="line">[root@localhost ~]# ip l s tun0 up</span><br></pre></td></tr></table></figure><p>再开一个终端，用 <code>tcpdump</code> 抓 <code>tun0</code> 的包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# tcpdump -nnt -i tun0</span><br></pre></td></tr></table></figure><p>然后在第二个终端 <code>ping</code> 一下 <code>10.1.1.0/24</code> 网段的 IP，比如 <code>10.1.1.3</code>，看到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ping -c 4 10.1.1.3</span><br><span class="line">PING 10.1.1.3 (10.1.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=1 ttl=64 time=0.133 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=2 ttl=64 time=0.188 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=3 ttl=64 time=0.092 ms</span><br><span class="line">64 bytes from 10.1.1.3: icmp_seq=4 ttl=64 time=0.110 ms</span><br><span class="line"></span><br><span class="line">--- 10.1.1.3 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3290ms</span><br><span class="line">rtt min/avg/max/mdev = 0.092/0.130/0.188/0.038 ms</span><br></pre></td></tr></table></figure><p>由于 <code>tun0</code> 接口建好之后，会生成一条到本网段 <code>10.1.1.0/24</code> 的默认路由，根据默认路由，数据包会走 <code>tun0</code> 口，所以能 ping 通，可以用 <code>route -n</code> 查看。</p><p>再看 tcpdump 抓包终端，成功显示 ICMP 的 request 包和 reply 包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# tcpdump -nnt -i tun0</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on tun0, link-type RAW (Raw IP), capture size 262144 bytes</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 3250, seq 1, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 3250, seq 1, length 64</span><br><span class="line">IP 10.1.1.2 &gt; 10.1.1.3: ICMP echo request, id 3250, seq 2, length 64</span><br><span class="line">IP 10.1.1.3 &gt; 10.1.1.2: ICMP echo reply, id 3250, seq 2, length 64</span><br></pre></td></tr></table></figure><p>再看程序 <code>taptun.c</code> 的输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost coding]# ./taptun</span><br><span class="line">Open tun/tap device: tun0 for reading...</span><br><span class="line">Read 48 bytes from tun/tap device</span><br><span class="line">Write 48 bytes to tun/tap device</span><br><span class="line">Read 48 bytes from tun/tap device</span><br><span class="line">Write 48 bytes to tun/tap device</span><br></pre></td></tr></table></figure><p>ok，以上便验证了程序的正确性。</p><h2 id="03-总结"><a href="#03-总结" class="headerlink" title="03 总结"></a>03 总结</h2><p>通过这个小例子，让我们知道了基于 tap/tun 编程的流程，对 tap/tun 又加深了一层理解。</p><p>使用 tap/tun 设备需要包含头文件 <code>#include &lt;linux/if_tun.h&gt;</code>，以下是完整代码。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/******************************************************************************</span></span><br><span class="line"><span class="comment"> *File Name: taptun.c</span></span><br><span class="line"><span class="comment"> *Author: 公众号: Linux云计算网络</span></span><br><span class="line"><span class="comment"> *Created Time: 2019年02月23日 星期六 21时28分24秒</span></span><br><span class="line"><span class="comment"> *****************************************************************************/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;net/if.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/ioctl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;linux/if_tun.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tun_alloc</span><span class="params">(<span class="keyword">char</span> *dev, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    assert(dev != <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ifreq</span> <span class="title">ifr</span>;</span></span><br><span class="line">    <span class="keyword">int</span> fd, err;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">char</span> *clonedev = <span class="string">"/dev/net/tun"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ((fd = open(clonedev, O_RDWR)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> fd;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(&amp;ifr, <span class="number">0</span>, <span class="keyword">sizeof</span>(ifr));</span><br><span class="line">    ifr.ifr_flags = flags;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (*dev != <span class="string">'\0'</span>) &#123;</span><br><span class="line">        <span class="built_in">strncpy</span>(ifr.ifr_name, dev, IFNAMSIZ);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> ((err = ioctl(fd, TUNSETIFF, (<span class="keyword">void</span> *) &amp;ifr)) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        close(fd);</span><br><span class="line">        <span class="keyword">return</span> err;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一旦设备开启成功，系统会给设备分配一个名称，对于tun设备，一般为tunX，X为从0开始的编号；</span></span><br><span class="line">    <span class="comment">// 对于tap设备，一般为tapX</span></span><br><span class="line">    <span class="built_in">strcpy</span>(dev, ifr.ifr_name);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fd;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> tun_fd, nread;</span><br><span class="line">    <span class="keyword">char</span> buffer[<span class="number">4096</span>];</span><br><span class="line">    <span class="keyword">char</span> tun_name[IFNAMSIZ];</span><br><span class="line"></span><br><span class="line">    tun_name[<span class="number">0</span>] = <span class="string">'\0'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* Flags: IFF_TUN   - TUN device (no Ethernet headers)</span></span><br><span class="line"><span class="comment">     *        IFF_TAP   - TAP device</span></span><br><span class="line"><span class="comment">     *        IFF_NO_PI - Do not provide packet information</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    tun_fd = tun_alloc(tun_name, IFF_TUN | IFF_NO_PI);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tun_fd &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        perror(<span class="string">"Allocating interface"</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Open tun/tap device: %s for reading...\n"</span>, tun_name);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">unsigned</span> <span class="keyword">char</span> ip[<span class="number">4</span>];</span><br><span class="line">        <span class="comment">// 收包</span></span><br><span class="line">        nread = read(tun_fd, buffer, <span class="keyword">sizeof</span>(buffer));</span><br><span class="line">        <span class="keyword">if</span> (nread &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            perror(<span class="string">"Reading from interface"</span>);</span><br><span class="line">            close(tun_fd);</span><br><span class="line">            <span class="built_in">exit</span>(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Read %d bytes from tun/tap device\n"</span>, nread);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 简单对收到的包调换一下顺序</span></span><br><span class="line">        <span class="built_in">memcpy</span>(ip, &amp;buffer[<span class="number">12</span>], <span class="number">4</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;buffer[<span class="number">12</span>], &amp;buffer[<span class="number">16</span>], <span class="number">4</span>);</span><br><span class="line">        <span class="built_in">memcpy</span>(&amp;buffer[<span class="number">16</span>], ip, <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        buffer[<span class="number">20</span>] = <span class="number">0</span>;</span><br><span class="line">        *((<span class="keyword">unsigned</span> <span class="keyword">short</span> *)&amp;buffer[<span class="number">22</span>]) += <span class="number">8</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 发包</span></span><br><span class="line">        nread = write(tun_fd, buffer, nread);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Write %d bytes to tun/tap device, that's %s\n"</span>, nread, buffer);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> tap </tag>
            
            <tag> tun </tag>
            
            <tag> ICMP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux网络命令必知必会之创建 tap/tun 设备</title>
      <link href="/2019/02/28/tech/net/vnet/%E5%88%9B%E5%BB%BAtaptun%E8%AE%BE%E5%A4%87/"/>
      <url>/2019/02/28/tech/net/vnet/%E5%88%9B%E5%BB%BAtaptun%E8%AE%BE%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>在<a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247484961&amp;idx=1&amp;sn=f26d7994f57abbf5de2007a2f451d9f5&amp;chksm=ea743299dd03bb8f6ac063c1cb00d5a592094c7778ab0a5baf37b7469fa3eb018101ef34551f&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">这篇文章</a>中，我们已经介绍了 tap/tun 的基本原理，本文将介绍如何使用工具 <code>tunctl</code>和 <code>ip tuntap</code> 来创建并使用 tap/tun 设备。</p><a id="more"></a><h2 id="tunctl"><a href="#tunctl" class="headerlink" title="tunctl"></a>tunctl</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>首先在 <code>centos</code> 的环境中安装 <code>tunctl</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# vim /etc/yum.repos.d/nux-misc.repo</span><br><span class="line"></span><br><span class="line">[nux-misc]</span><br><span class="line">name=Nux Misc</span><br><span class="line">baseurl=http://li.nux.ro/download/nux/misc/el7/x86_64/</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.ro</span><br></pre></td></tr></table></figure><p><code>ubuntu</code> 是 <code>apt-get install uml-utilities</code>。</p><p><code>man tunctl</code> 查看 <code>tunctl</code> 手册，用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Synopsis</span><br><span class="line">tunctl [ OPTIONS ] [ -u owner ] [-g group] [ -t device-name ]</span><br></pre></td></tr></table></figure><ul><li>-u 参数指定用户名，表明这个接口只受该用户控制，这个接口发生的事不会影响到系统的接口。</li><li>-g 指定一组用户</li><li>-t 指定要创建的 tap/tun 设备名。</li></ul><p><code>[OPTIONS]</code> 部分：</p><ul><li>-b 简单打印创建的接口名字</li><li>-n 创建 tun 设备</li><li>-p 创建 tap 设备，默认创建该设备</li><li>-f tun-clone-device 指定 tun 设备对应的文件名，默认是 <code>/dev/net/tun</code>，有些系统是 <code>/dev/misc/net/tun</code>。</li><li>-d interfacename 删除指定接口</li></ul><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p><strong>常见用法：</strong></p><p>默认创建 tap 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tunctl</span><br></pre></td></tr></table></figure><p>以上等价于 <code>tunctl -p</code></p><p>为用户 <code>user</code> 创建一个 tap 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tunctl -u user</span><br></pre></td></tr></table></figure><p>创建 tun 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tunctl -n</span><br></pre></td></tr></table></figure><p>为接口配置 IP 并启用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ifconfig tap0 192.168.0.254 up</span><br></pre></td></tr></table></figure><p>为接口添加路由：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># route add -host 192.168.0.1 dev tap0</span><br></pre></td></tr></table></figure><p>删除接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tunctl -d tap0</span><br></pre></td></tr></table></figure><h2 id="ip-tuntap"><a href="#ip-tuntap" class="headerlink" title="ip tuntap"></a>ip tuntap</h2><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>命令行输入 <code>ip help</code> 查看 <code>ip</code> 命令是否支持 <code>tuntap</code> 工具，支持的话就会显示 <code>tuntap</code> 选项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip help</span><br><span class="line">Usage: ip [ OPTIONS ] OBJECT &#123; COMMAND | help &#125;</span><br><span class="line">       ip [ -force ] -batch filename</span><br><span class="line">where  OBJECT := &#123; link | addr | addrlabel | route | rule | neigh | ntable |</span><br><span class="line">                   tunnel | tuntap | maddr | mroute | mrule | monitor | xfrm |</span><br><span class="line">                   netns | l2tp | tcp_metrics | token &#125;</span><br></pre></td></tr></table></figure><p>不支持就请升级或下载最新的 <code>iproute2</code> 工具包，或者使用上面介绍的 <code>tunctl</code> 工具。</p><h3 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h3><p>输入 <code>ip tuntap help</code> 查看详细使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip tuntap help</span><br><span class="line">Usage: ip tuntap &#123; add | del &#125; [ dev PHYS_DEV ]</span><br><span class="line">          [ mode &#123; tun | tap &#125; ] [ user USER ] [ group GROUP ]</span><br><span class="line">          [ one_queue ] [ pi ] [ vnet_hdr ] [ multi_queue ]</span><br><span class="line"></span><br><span class="line">Where: USER  := &#123; STRING | NUMBER &#125;</span><br><span class="line">       GROUP := &#123; STRING | NUMBER &#125;</span><br></pre></td></tr></table></figure><p><strong>常见用法：</strong></p><p>创建 tap/tun 设备：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip tuntap add dev tap0 mod tap # 创建 tap </span><br><span class="line">ip tuntap add dev tun0 mod tun # 创建 tun</span><br></pre></td></tr></table></figure><p>删除 tap/tun 设备：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip tuntap del dev tap0 mod tap # 删除 tap </span><br><span class="line">ip tuntap del dev tun0 mod tun # 删除 tun</span><br></pre></td></tr></table></figure><p>PS: <code>user</code> 和 <code>group</code> 参数和 <code>tunctl</code> 的 -u、 -g 参数是一样的。</p><p>以上两个工具，我们更推荐使用 <code>ip tuntap</code>，一个是因为 <code>iproute2</code> 更全更新，已经逐步在替代老旧的一些工具，另一个是因为 <code>tunctl</code> 在某些 <code>Debian</code> 类的系统上支持不全。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>tunctl</code> 和 <code>ip tuntap</code> 的常见使用方式。</p><p>更推荐使用 <code>ip tuntap</code> 工具。</p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> tap </tag>
            
            <tag> tun </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux云网络基础之虚拟网络设备 tap/tun 详解</title>
      <link href="/2019/02/26/tech/net/vnet/taptun%E8%AF%A6%E8%A7%A3/"/>
      <url>/2019/02/26/tech/net/vnet/taptun%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>在云计算时代，虚拟机和容器已经成为标配。它们背后的网络管理都离不开一样东西，就是虚拟网络设备，或者叫虚拟网卡，tap/tun 就是在云计算时代非常重要的虚拟网络网卡。</p><h2 id="tap-tun-是什么"><a href="#tap-tun-是什么" class="headerlink" title="tap/tun 是什么"></a>tap/tun 是什么</h2><p>tap/tun 是 Linux 内核 2.4.x 版本之后实现的虚拟网络设备，不同于物理网卡靠硬件网路板卡实现，tap/tun 虚拟网卡完全由软件来实现，功能和硬件实现完全没有差别，它们都属于网络设备，都可以配置 IP，都归 Linux 网络设备管理模块统一管理。</p><a id="more"></a><p>作为网络设备，tap/tun 也需要配套相应的驱动程序才能工作。tap/tun 驱动程序包括两个部分，一个是字符设备驱动，一个是网卡驱动。这两部分驱动程序分工不太一样，字符驱动负责数据包在内核空间和用户空间的传送，网卡驱动负责数据包在 TCP/IP 网络协议栈上的传输和处理。</p><h2 id="用户空间与内核空间的数据传输"><a href="#用户空间与内核空间的数据传输" class="headerlink" title="用户空间与内核空间的数据传输"></a>用户空间与内核空间的数据传输</h2><p>在 Linux 中，用户空间和内核空间的数据传输有多种方式，字符设备就是其中的一种。tap/tun 通过驱动程序和一个与之关联的字符设备，来实现用户空间和内核空间的通信接口。</p><p>在 Linux 内核 2.6.x 之后的版本中，tap/tun 对应的字符设备文件分别为：</p><ul><li>tap：/dev/tap0</li><li>tun：/dev/net/tun</li></ul><p>设备文件即充当了用户空间和内核空间通信的接口。当应用程序打开设备文件时，驱动程序就会创建并注册相应的虚拟设备接口，一般以 <code>tunX</code> 或 <code>tapX</code> 命名。当应用程序关闭文件时，驱动也会自动删除 <code>tunX</code> 和 <code>tapX</code> 设备，还会删除已经建立起来的路由等信息。</p><p>tap/tun 设备文件就像一个管道，一端连接着用户空间，一端连接着内核空间。当用户程序向文件 <code>/dev/net/tun</code> 或 <code>/dev/tap0</code> 写数据时，内核就可以从对应的 <code>tunX</code> 或 <code>tapX</code> 接口读到数据，反之，内核可以通过相反的方式向用户程序发送数据。</p><p><img src="/images/virt/tapwriteread.png" alt="tapwriteread.png"></p><h2 id="tap-tun-和网络协议栈的数据传输"><a href="#tap-tun-和网络协议栈的数据传输" class="headerlink" title="tap/tun 和网络协议栈的数据传输"></a>tap/tun 和网络协议栈的数据传输</h2><p>tap/tun 通过实现相应的网卡驱动程序来和网络协议栈通信。一般的流程和物理网卡和协议栈的交互流程是一样的，不同的是物理网卡一端是连接物理网络，而 tap/tun 虚拟网卡一般连接到用户空间。</p><p>如下图的示意图，我们有两个应用程序 A、B，物理网卡 <code>eth0</code> 和虚拟网卡 <code>tun0</code> 分别配置 IP：<code>10.1.1.11</code> 和 <code>192.168.1.11</code>，程序 A 希望构造数据包发往 <code>192.168.1.0/24</code> 网段的主机 <code>192.168.1.1</code>。</p><p><img src="/images/virt/taptun.png" alt="taptun"></p><p>基于上图，我们看看数据包的流程：</p><ol><li>应用程序 A 构造数据包，目的 IP 是 <code>192.168.1.1</code>，通过 <code>socket A</code> 将这个数据包发给协议栈。</li><li>协议栈根据数据包的目的 IP 地址，匹配路由规则，发现要从 <code>tun0</code> 出去。</li><li><code>tun0</code> 发现自己的另一端被应用程序 B 打开了，于是将数据发给程序 B.</li><li>程序 B 收到数据后，做一些跟业务相关的操作，然后构造一个新的数据包，源 IP 是 <code>eth0</code> 的 IP，目的 IP 是 <code>10.1.1.0/24</code> 的网关 <code>10.1.1.1</code>，封装原来的数据的数据包，重新发给协议栈。</li><li>协议栈再根据本地路由，将这个数据包从 <code>eth0</code> 发出。</li></ol><p>后续步骤，当 <code>10.1.1.1</code> 收到数据包后，会进行解封装，读取里面的原始数据包，继而转发给本地的主机 <code>192.168.1.1</code>。当接收回包时，也遵循同样的流程。</p><p>在这个流程中，应用程序 B 的作用其实是利用 <code>tun0</code> 对数据包做了一层隧道封装。其实 <code>tun</code> 设备的最大用途就是用于隧道通信的。</p><h2 id="tap-tun-的区别"><a href="#tap-tun-的区别" class="headerlink" title="tap/tun 的区别"></a>tap/tun 的区别</h2><p>看到这里，你可能还不大明白 tap/tun 的区别。<br>tap 和 tun 虽然都是虚拟网络设备，但它们的工作层次还不太一样。</p><ul><li>tap 是一个二层设备（或者以太网设备），只能处理二层的以太网帧；</li><li>tun 是一个点对点的三层设备（或网络层设备），只能处理三层的 IP 数据包。</li></ul><h2 id="tap-tun-的应用"><a href="#tap-tun-的应用" class="headerlink" title="tap/tun 的应用"></a>tap/tun 的应用</h2><p>从上面的数据流程中可以看到，<code>tun</code> 设备充当了一层隧道，所以，tap/tun 最常见的应用也就是用于隧道通信，比如 VPN，包括 tunnel 和应用层的 IPsec 等，其中比较有名的两个开源项目是 <a href="http://openvpn.sourceforge.net" target="_blank" rel="noopener">openvpn</a> 和 <a href="http://vtun.sourceforge.net" target="_blank" rel="noopener">VTun</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>tun/tap 虚拟网卡，对应于物理网卡，如 eth0。</p><p>tun/tap 驱动包括字符设备驱动和网卡驱动。</p><p>tun/tap 常用于隧道通信。</p><p><strong>参考：</strong></p><p><a href="https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/" target="_blank" rel="noopener">https://opengers.github.io/openstack/openstack-base-virtual-network-devices-tuntap-veth/</a></p><p><a href="https://segmentfault.com/a/1190000009249039" target="_blank" rel="noopener">https://segmentfault.com/a/1190000009249039</a></p><p><a href="https://mirrors.edge.kernel.org/pub/linux/kernel/people/marcelo/linux-2.4/Documentation/networking/tuntap.txt" target="_blank" rel="noopener">https://mirrors.edge.kernel.org/pub/linux/kernel/people/marcelo/linux-2.4/Documentation/networking/tuntap.txt</a></p><p><a href="https://zh.wikipedia.org/wiki/TUN%E4%B8%8ETAP" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/TUN%E4%B8%8ETAP</a></p><p><a href="http://blog.chinaunix.net/uid-317451-id-92474.html" target="_blank" rel="noopener">http://blog.chinaunix.net/uid-317451-id-92474.html</a></p><p><a href="https://blog.csdn.net/bytxl/article/details/26586109" target="_blank" rel="noopener">https://blog.csdn.net/bytxl/article/details/26586109</a></p><p><a href="https://blog.csdn.net/u013982161/article/details/51816162" target="_blank" rel="noopener">https://blog.csdn.net/u013982161/article/details/51816162</a></p><p><a href="https://www.cnblogs.com/yml435/p/5917628.html" target="_blank" rel="noopener">https://www.cnblogs.com/yml435/p/5917628.html</a></p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> tap </tag>
            
            <tag> tun </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文搞懂 network namespace</title>
      <link href="/2019/01/10/tech/%E6%A8%A1%E6%9D%BF/"/>
      <url>/2019/01/10/tech/%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有 <strong>10T</strong> 书籍和视频资源，后台回复<strong>「1024」</strong>即可免费领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可免费领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> Namespace </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文搞懂 network namespace</title>
      <link href="/2019/01/10/tech/net/vnet/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82network_namespace/"/>
      <url>/2019/01/10/tech/net/vnet/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82network_namespace/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>本文通过 IP 命令操作来简单介绍 network namespace 的基本概念和用法。</p><p>深入了解可以看看我之前写的两篇文章 <a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247483982&amp;idx=1&amp;sn=35e2aac1f4c164c8afa79aa91707c90d&amp;chksm=ea7436f6dd03bfe036ccc8293aaf25d0c042f21afac5277bf04a32af36643a2780c980e53d09&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Docker 基础技术之 Linux namespace 详解</a> 和 <a href="http://mp.weixin.qq.com/s?__biz=MzI1OTY2MzMxOQ==&amp;mid=2247483993&amp;idx=1&amp;sn=906551e374c0d8d40db00cbde934e624&amp;chksm=ea7436e1dd03bff724d7ee03267115b2ecf54bd146fa3f826b69e918b5a2455d1562d085ab62&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Docker 基础技术之 Linux namespace 源码分析</a>。</p><p>和 network namespace 相关的操作的子命令是 <code>ip netns</code> 。</p><a id="more"></a><h2 id="1-ip-netns-add-xx-创建一个-namespace"><a href="#1-ip-netns-add-xx-创建一个-namespace" class="headerlink" title="1. ip netns add  xx 创建一个 namespace"></a>1. ip netns add  xx 创建一个 namespace</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ip netns add net1</span><br><span class="line"># ip netns ls</span><br><span class="line">net1</span><br></pre></td></tr></table></figure><h2 id="2-ip-netns-exec-xx-yy-在新-namespace-xx-中执行-yy-命令"><a href="#2-ip-netns-exec-xx-yy-在新-namespace-xx-中执行-yy-命令" class="headerlink" title="2. ip netns exec xx yy 在新 namespace xx 中执行 yy 命令"></a>2. ip netns exec xx yy 在新 namespace xx 中执行 yy 命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net1 ip addr </span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line"># ip netns exec net1 bash // 在 net1 中打开一个shell终端</span><br><span class="line"># ip addr // 在net1中的shell终端</span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line"># exit // 退出net1</span><br></pre></td></tr></table></figure><p>上面 bash 不好区分是当前是在哪个 shell，可以采用下面的方法解决：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net1 /bin/bash --rcfile &lt;(echo &quot;PS1=\&quot;namespace net1&gt; \&quot;&quot;)</span><br><span class="line">namespace net1&gt; ping www.baidu.com</span><br></pre></td></tr></table></figure><p>每个 namespace 在创建的时候会自动创建一个回环接口 <code>lo</code> ，默认不启用，可以通过 <code>ip link set lo up</code> 启用。</p><h2 id="3-network-namespace-之间的通信"><a href="#3-network-namespace-之间的通信" class="headerlink" title="3. network namespace 之间的通信"></a>3. network namespace 之间的通信</h2><p>新创建的 namespace 默认不能和主机网络，以及其他 namespace 通信。</p><p>可以使用 Linux 提供的 <code>veth pair</code> 来完成通信。下面显示两个 namespace 之间通信的网络拓扑：</p><p><img src="/images/virt/netns.png" alt=""></p><h3 id="3-1-ip-link-add-type-veth-创建-veth-pair"><a href="#3-1-ip-link-add-type-veth-创建-veth-pair" class="headerlink" title="3.1 ip link add type veth 创建 veth pair"></a>3.1 ip link add type veth 创建 veth pair</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># ip link add type veth</span><br><span class="line"># ip link</span><br><span class="line">3: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 1a:53:39:5a:26:12 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">4: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 46:df:46:1f:bf:d6 brd ff:ff:ff:ff:ff:ff</span><br></pre></td></tr></table></figure><p>使用命令 <code>ip link add xxx type veth peer name yyy</code> 指定 veth pair 的名字。</p><h3 id="3-2-ip-link-set-xx-netns-yy-将-veth-xx-加入到-namespace-yy-中"><a href="#3-2-ip-link-set-xx-netns-yy-将-veth-xx-加入到-namespace-yy-中" class="headerlink" title="3.2 ip link set xx netns yy 将 veth xx 加入到 namespace yy 中"></a>3.2 ip link set xx netns yy 将 veth xx 加入到 namespace yy 中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># ip link set veth0 netns net0</span><br><span class="line"># ip link set veth1 netns net1</span><br><span class="line">#</span><br><span class="line"># ip netns exec net0 ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">10: veth0@if11: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000</span><br><span class="line">    link/ether 1a:53:39:5a:26:12 brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br></pre></td></tr></table></figure><h3 id="3-3-给-veth-pair-配上-ip-地址"><a href="#3-3-给-veth-pair-配上-ip-地址" class="headerlink" title="3.3 给 veth pair 配上 ip 地址"></a>3.3 给 veth pair 配上 ip 地址</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net0 ip link set veth0 up</span><br><span class="line"># ip netns exec net0 ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">10: veth0@if11: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000</span><br><span class="line">    link/ether 1a:53:39:5a:26:12 brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br><span class="line"># ip netns exec net0 ip addr add 10.1.1.1/24 dev veth0</span><br><span class="line"># ip netns exec net0 ip route</span><br><span class="line">10.1.1.0/24 dev veth0  proto kernel  scope link  src 10.1.1.1 linkdown</span><br><span class="line">#</span><br><span class="line"># ip netns exec net1 ip link set veth1 up</span><br><span class="line"># ip netns exec net1 ip addr add 10.1.1.2/24 dev veth1</span><br></pre></td></tr></table></figure><p>可以看到，在配完 ip 之后，还自动生成了对应的路由表信息。</p><h3 id="3-4-ping-测试两个-namespace-的连通性"><a href="#3-4-ping-测试两个-namespace-的连通性" class="headerlink" title="3.4. ping 测试两个 namespace 的连通性"></a>3.4. ping 测试两个 namespace 的连通性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net0 ping 10.1.1.2</span><br><span class="line">PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.069 ms</span><br><span class="line">64 bytes from 10.1.1.2: icmp_seq=2 ttl=64 time=0.054 ms</span><br><span class="line">64 bytes from 10.1.1.2: icmp_seq=3 ttl=64 time=0.053 ms</span><br><span class="line">64 bytes from 10.1.1.2: icmp_seq=4 ttl=64 time=0.053 ms</span><br></pre></td></tr></table></figure><p>Done!</p><h2 id="4-多个不同-namespace-之间的通信"><a href="#4-多个不同-namespace-之间的通信" class="headerlink" title="4. 多个不同 namespace 之间的通信"></a>4. 多个不同 namespace 之间的通信</h2><p>2 个 namespace 之间通信可以借助 <code>veth pair</code> ，多个 namespace 之间的通信则可以使用 bridge 来转接，不然每两个 namespace 都去配 <code>veth pair</code> 将会是一件麻烦的事。下面就看看如何使用 bridge 来转接。</p><p>拓扑图如下：</p><p><img src="/images/virt/bridgens.png" alt=""></p><h3 id="4-1-使用-ip-link-和-brctl-创建-bridge"><a href="#4-1-使用-ip-link-和-brctl-创建-bridge" class="headerlink" title="4.1 使用 ip link 和 brctl 创建 bridge"></a>4.1 使用 ip link 和 brctl 创建 bridge</h3><p>通常 Linux 中和 bridge 有关的操作是使用命令 <code>brctl</code> (<code>yum install -y bridge-utils</code> ) 。但为了前后照应，这里都用 ip 相关的命令来操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 建立一个 bridge</span><br><span class="line"># ip link add br0 type bridge</span><br><span class="line"># ip link set dev br0 up</span><br><span class="line">9: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether 42:55:ed:eb:a0:07 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fe80::4055:edff:feeb:a007/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><h3 id="4-2-创建-veth-pair"><a href="#4-2-创建-veth-pair" class="headerlink" title="4.2 创建 veth pair"></a>4.2 创建 veth pair</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//（1）创建 3 个 veth pair</span><br><span class="line"># ip link add type veth</span><br><span class="line"># ip link add type veth</span><br><span class="line"># ip link add type veth</span><br></pre></td></tr></table></figure><h3 id="4-3-将-veth-pair-的一头挂到-namespace-中，一头挂到-bridge-上，并设-IP-地址"><a href="#4-3-将-veth-pair-的一头挂到-namespace-中，一头挂到-bridge-上，并设-IP-地址" class="headerlink" title="4.3 将 veth pair 的一头挂到 namespace 中，一头挂到 bridge 上，并设 IP 地址"></a>4.3 将 veth pair 的一头挂到 namespace 中，一头挂到 bridge 上，并设 IP 地址</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// （1）配置第 1 个 net0</span><br><span class="line"># ip link set dev veth1 netns net0</span><br><span class="line"># ip netns exec net0 ip link set dev veth1 name eth0</span><br><span class="line"># ip netns exec net0 ip addr add 10.0.1.1/24 dev eth0</span><br><span class="line"># ip netns exec net0 ip link set dev eth0 up</span><br><span class="line">#</span><br><span class="line"># ip link set dev veth0 master br0</span><br><span class="line"># ip link set dev veth0 up</span><br><span class="line"></span><br><span class="line">// （2）配置第 2 个 net1</span><br><span class="line"># ip link set dev veth3 netns net1</span><br><span class="line"># ip netns exec net1 ip link set dev veth3 name eth0</span><br><span class="line"># ip netns exec net1 ip addr add 10.0.1.2/24 dev eth0</span><br><span class="line"># ip netns exec net1 ip link set dev eth0 up</span><br><span class="line">#</span><br><span class="line"># ip link set dev veth2 master br0</span><br><span class="line"># ip link set dev veth2 up</span><br><span class="line"></span><br><span class="line">// （3）配置第 3 个 net2</span><br><span class="line"># ip link set dev veth5 netns net2</span><br><span class="line"># ip netns exec net2 ip link set dev veth5 name eth0</span><br><span class="line"># ip netns exec net2 ip addr add 10.0.1.3/24 dev eth0</span><br><span class="line"># ip netns exec net2 ip link set dev eth0 up</span><br><span class="line"># </span><br><span class="line"># ip link set dev veth4 master br0</span><br><span class="line"># ip link set dev veth4 up</span><br></pre></td></tr></table></figure><p>这样之后，竟然通不了，经查阅 <a href="https://segmentfault.com/q/1010000010011053/a-1020000010025650" target="_blank" rel="noopener">参见</a> ，是因为</p><blockquote><p>原因是因为系统为bridge开启了iptables功能，导致所有经过br0的数据包都要受iptables里面规则的限制，而docker为了安全性，将iptables里面filter表的FORWARD链的默认策略设置成了drop，于是所有不符合docker规则的数据包都不会被forward，导致你这种情况ping不通。</p><p>解决办法有两个，二选一：</p><ol><li>关闭系统bridge的iptables功能，这样数据包转发就不受iptables影响了：echo 0 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</li><li>为br0添加一条iptables规则，让经过br0的包能被forward：iptables -A FORWARD -i br0 -j ACCEPT</li></ol><p>第一种方法不确定会不会影响docker，建议用第二种方法。</p></blockquote><p>我采用以下方法解决：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -A FORWARD -i br0 -j ACCEPT</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># ip netns exec net0 ping -c 2 10.0.1.2</span><br><span class="line">PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.0.1.2: icmp_seq=1 ttl=64 time=0.071 ms</span><br><span class="line">64 bytes from 10.0.1.2: icmp_seq=2 ttl=64 time=0.072 ms</span><br><span class="line"></span><br><span class="line">--- 10.0.1.2 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.071/0.071/0.072/0.008 ms</span><br><span class="line"></span><br><span class="line"># ip netns exec net0 ping -c 2 10.0.1.3</span><br><span class="line">PING 10.0.1.3 (10.0.1.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.0.1.3: icmp_seq=1 ttl=64 time=0.071 ms</span><br><span class="line">64 bytes from 10.0.1.3: icmp_seq=2 ttl=64 time=0.087 ms</span><br><span class="line"></span><br><span class="line">--- 10.0.1.3 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.071/0.079/0.087/0.008 ms</span><br></pre></td></tr></table></figure><p>Done!</p><h2 id="5-Bridge-之间的同住机通信"><a href="#5-Bridge-之间的同住机通信" class="headerlink" title="5. Bridge 之间的同住机通信"></a>5. Bridge 之间的同住机通信</h2><p>以上所说的是一个 bridge 同网段的通信，现在看看不同 bridge 跨网段的通信，如下拓扑：</p><h2 id="6-Bridge-之间的跨住机通信"><a href="#6-Bridge-之间的跨住机通信" class="headerlink" title="6. Bridge 之间的跨住机通信"></a>6. Bridge 之间的跨住机通信</h2><p><a href="https://www.cnblogs.com/iiiiher/p/8057922.html" target="_blank" rel="noopener">https://www.cnblogs.com/iiiiher/p/8057922.html</a></p><p><strong>参考资料：</strong>  </p><p><a href="http://cizixs.com/2017/02/10/network-virtualization-network-namespace" target="_blank" rel="noopener">linux 网络虚拟化： network namespace 简介</a></p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> Namespace </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 系统下实践 VLAN</title>
      <link href="/2019/01/07/tech/net/vnet/Linux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E5%AE%9E%E8%B7%B5VLAN/"/>
      <url>/2019/01/07/tech/net/vnet/Linux%E7%B3%BB%E7%BB%9F%E4%B8%8B%E5%AE%9E%E8%B7%B5VLAN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><h2 id="01-准备环境"><a href="#01-准备环境" class="headerlink" title="01 准备环境"></a>01 准备环境</h2><p>环境：ubuntu 16.04 环境（物理 or 虚拟）</p><p>确认 CPU 是否支持虚拟化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># egrep -o &apos;(vmx|svm)&apos; /proc/cpuinfo</span><br><span class="line"># vmx</span><br></pre></td></tr></table></figure><p>如果不支持，开启 KVM 嵌套虚拟化之后再重启。</p><a id="more"></a><h3 id="1-1-安装-KVM-环境"><a href="#1-1-安装-KVM-环境" class="headerlink" title="1.1 安装 KVM 环境"></a>1.1 安装 KVM 环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y qemu-kvm qemu-system libvirt-bin virt-manager bridge-utils vlan</span><br></pre></td></tr></table></figure><h3 id="1-2-安装-Ubuntu-图形化界面"><a href="#1-2-安装-Ubuntu-图形化界面" class="headerlink" title="1.2 安装 Ubuntu 图形化界面"></a>1.2 安装 Ubuntu 图形化界面</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y xinit gdm kubuntu-desktop</span><br></pre></td></tr></table></figure><h2 id="02-创建-KVM-虚拟机"><a href="#02-创建-KVM-虚拟机" class="headerlink" title="02 创建 KVM 虚拟机"></a>02 创建 KVM 虚拟机</h2><p>使用 virt-manager 创建 KVM 虚拟机，方法比较简单，由于篇幅有限，大家可以查阅相关资料自行了解。</p><p>创建完之后用 <code>virsh list --all</code> 查看创建的 VM：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Id    Name                           State</span><br><span class="line">----------------------------------------------------</span><br><span class="line"> -     kvm1                           shut off</span><br><span class="line"> -     kvm2                           shut off</span><br><span class="line"> -     kvm3                           shut off</span><br></pre></td></tr></table></figure><p>我们的实验拓扑如下：</p><p><img src="/images/net/brvlan.png" alt=""></p><p>图中创建了 2 个 Linux Bridge：brvlan1 和 brvlan2，宿主机的物理网卡 eth0 抽象出两个虚拟设备 eth0.1 和 eth0.2，也就是两个 VLAN 设备，它们分别定义了两个 VLAN：VLAN1 和 VLAN2。挂接到两个 Bridge 上的网络设备自动加入到相应的 VLAN 中。VLAN1 接两个 VM，VLAN 接一个 VM。</p><p>实验的目的是要验证属于同一个 VLAN1 中 VM1 和 VM2 能 ping 通，而属于不同 VLAN 中的 VM ping 不通。</p><h2 id="03-实验开始"><a href="#03-实验开始" class="headerlink" title="03 实验开始"></a>03 实验开始</h2><h3 id="3-1-配置-VLAN"><a href="#3-1-配置-VLAN" class="headerlink" title="3.1 配置 VLAN"></a>3.1 配置 VLAN</h3><p>编辑 <code>/etc/network/interfaces</code>，加入两个 Bridge 和两个 VLAN 设备的配置，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># The primary network interface</span><br><span class="line">auto ens33</span><br><span class="line">iface ens33 inet dhcp</span><br><span class="line"></span><br><span class="line">auto ens33.1</span><br><span class="line">iface ens33.1 inet manual</span><br><span class="line">        vlan-raw-device ens33</span><br><span class="line"></span><br><span class="line">auto brvlan1</span><br><span class="line">iface brvlan1 inet manual</span><br><span class="line">        bridge_stp off</span><br><span class="line">        bridge_waitport 0</span><br><span class="line">        bridge_fd 0</span><br><span class="line">        bridge_ports ens33.1</span><br><span class="line"></span><br><span class="line">auto ens33.2</span><br><span class="line">iface ens33.2 inet manual</span><br><span class="line">        vlan-raw-device ens33</span><br><span class="line"></span><br><span class="line">auto brvlan2</span><br><span class="line">iface brvlan2 inet manual</span><br><span class="line">        bridge_stp off</span><br><span class="line">        bridge_waitport 0</span><br><span class="line">        bridge_fd 0</span><br><span class="line">        bridge_ports ens33.2</span><br></pre></td></tr></table></figure><p><strong>注意</strong>，这里务必和自己电脑的接口名称统一，比如我这里叫 ens33，就配 ens33.1 和 ens33.2 的 VLAN 设备，当然你也可以改成 eth0 的形式。</p><p>重启宿主机，<code>ifconfig</code> 查看网络接口：</p><p><img src="/images/net/vlanif.png" alt="vlanif.png"></p><p>用 <code>brctl show</code> 查看当前 Linux Bridge 的配置，ens33.1 和 ens33.2 分别挂载 brvlan1 和 brvlan2 上了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># brctl show</span><br><span class="line">bridge namebridge idSTP enabledinterfaces</span><br><span class="line">brvlan18000.000c298c57e8noens33.1</span><br><span class="line">brvlan28000.000c298c57e8noens33.2</span><br><span class="line">virbr08000.000000000000yes</span><br></pre></td></tr></table></figure><h3 id="3-2-配置-VM"><a href="#3-2-配置-VM" class="headerlink" title="3.2 配置 VM"></a>3.2 配置 VM</h3><p>我们先配置 VM1，启动 <code>virt-manager</code>，在图形界面中将 VM1 的虚拟网卡挂到 brvlan1 上：</p><p><img src="/images/net/vm1brvlan1.png" alt="vm1brvlan1.png"></p><p>同样的方式配置 VM2 和 VM3，VM2 也配到 brvlan1 上，VM3 配到 brvlan2 上。</p><h3 id="3-3-查看-VM-配置"><a href="#3-3-查看-VM-配置" class="headerlink" title="3.3 查看 VM 配置"></a>3.3 查看 VM 配置</h3><p>用 <code>virsh start xxx</code> 启动 3 个 VM：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># virsh start kvm1</span><br><span class="line"># virsh start kvm2</span><br><span class="line"># virsh start kvm3</span><br></pre></td></tr></table></figure><p>再通过 <code>brctl show</code> 查看 Bridge，这时发现 brvlan1 下接了 vnet0 和 vnet1，brvlan2 下接了 vnet2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># brctl show</span><br><span class="line">bridge namebridge idSTP enabledinterfaces</span><br><span class="line">brvlan18000.000c298c57e8noens33.1</span><br><span class="line">vnet0</span><br><span class="line">vnet1</span><br><span class="line">brvlan28000.000c298c57e8noens33.2</span><br><span class="line">vnet2</span><br><span class="line">virbr08000.000000000000yes</span><br></pre></td></tr></table></figure><p>通过 <code>virsh domiflist xxx</code> 确认这就是 VM 的虚拟网卡：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># virsh domiflist kvm1</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet0      bridge     brvlan1    rtl8139     52:54:00:b3:dd:3a</span><br><span class="line"></span><br><span class="line"># virsh domiflist kvm2</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet1      bridge     brvlan1    rtl8139     52:54:00:b7:4f:ef</span><br><span class="line"></span><br><span class="line"># virsh domiflist kvm3</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet2      bridge     brvlan2    rtl8139     52:54:00:d8:b8:2a</span><br></pre></td></tr></table></figure><h3 id="04-验证"><a href="#04-验证" class="headerlink" title="04 验证"></a>04 验证</h3><p>为了验证相同 VLAN 之间的连通性和不同 VLAN 之间的隔离性，我们为 3 个 VM 都配置同一网段的 IP。</p><p>使用 <code>virt-manager</code> 进入 VM console 控制面。</p><p>配置 VM1 的 IP：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 192.168.100.10 netmask 255.255.255.0</span><br></pre></td></tr></table></figure><p>配置 VM2 的 IP：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 192.168.100.20 netmask 255.255.255.0</span><br></pre></td></tr></table></figure><p>配置 VM3 的 IP：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 192.168.100.30 netmask 255.255.255.0</span><br></pre></td></tr></table></figure><p>使用 VM1 ping VM2 能 ping 通，VM2 ping VM3 不能 ping 通。</p><p><img src="/images/net/vlanping.png" alt="vlanping.png"></p><p>验证完毕。</p><p>大家如果有兴趣，可以抓个包看看，在发送 ping 包之前，需要知道对方的 MAC 地址，所以会先在网络中广播 ARP 包。ARP 是二层协议，VLAN 的作用就是隔离二层的广播域，ARP 包自然就不能在不同 VLAN 中流通，所以在相同 VLAN 中，通信双方能够拿到对方的 MAC 地址，也就能 ping 通，不同 VLAN 反之。</p><center>–END–</center><hr><p>后台回复「<font color="red">加群</font>」，带你进入高手如云交流群。</p><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」(id: cloud_dev)</strong> ，号内有 <strong>10T</strong> 书籍和视频资源，后台回复 <strong>「1024」</strong> 即可领取，分享的内容包括但不限于 Linux、网络、云计算虚拟化、容器Docker、OpenStack、Kubernetes、工具、SDN、OVS、DPDK、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 06 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> VLAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 012 Pod 的自动扩容与缩容</title>
      <link href="/2018/09/30/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_012_Pod_%E7%9A%84%E8%87%AA%E5%8A%A8%E6%89%A9%E5%AE%B9%E4%B8%8E%E7%BC%A9%E5%AE%B9/"/>
      <url>/2018/09/30/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_012_Pod_%E7%9A%84%E8%87%AA%E5%8A%A8%E6%89%A9%E5%AE%B9%E4%B8%8E%E7%BC%A9%E5%AE%B9/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 CloudDeveloper，欢迎大家和我一起学 K8S，这是系列第 12 篇。</p><p>上一篇我们了解了 Pod 的手动扩容和缩容，本篇来看看自动的方式。</p><a id="more"></a><p>K8S 作为一个集群式的管理软件，自动化、智能化是免不了的功能。Google 在 K8S v1.1 版本中就加入了这个 Pod 横向自动扩容的功能（Horizontal Pod Autoscaling，简称 HPA）。</p><p>HPA 与之前的 Deployment、Service 一样，也属于一种 K8S 资源对象。</p><p>HPA 的目标是希望通过追踪集群中所有 Pod 的负载变化情况，来自动化地调整 Pod 的副本数，以此来满足应用的需求和减少资源的浪费。</p><p>HAP 度量 Pod 负载变化情况的指标有两种：  </p><ul><li>CPU 利用率（CPUUtilizationPercentage）</li><li>自定义的度量指标，比如服务在每秒之内的请求数（TPS 或 QPS）</li></ul><p>如何统计和查询这些指标，要依托于一个组件——Heapster。Heapster 会监控一段时间内集群内所有 Pod 的 CPU 利用率的平均值或者其他自定义的值，在满足条件时（比如 CPU 使用率超过 80% 或 降低到 10%）会将这些信息反馈给 HPA 控制器，HPA 控制器就根据 RC 或者 Deployment 的定义调整 Pod 的数量。</p><p>HPA 实现的方式有两种：配置文件和命令行</p><ol><li>配置文件</li></ol><p>这种方式是通过定义 yaml 配置文件来创建 HPA，如下是基本定义：   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: php-apache</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:               # (1)</span><br><span class="line">    kind: Deployment   </span><br><span class="line">    name: php-apache</span><br><span class="line">  minReplicas: 1                # (2)</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  targetAverageUtilization: 50  # (3)</span><br></pre></td></tr></table></figure><p>文件 kind 类型是 <code>HorizontalPodAutoscaler</code>，其中有 3 个地方需要额外注意下：  </p><p>（1）<code>scaleTargetRef</code> 字段指定需要管理的 Deployment/RC 的名字，也就是提前需要存在一个 Deployment/RC 对象。</p><p>（2） <code>minReplicas</code> 和 <code>maxReplicas</code> 字段定义 Pod 可伸缩的数量范围。这个例子中扩容最高不能超过 10 个，缩容最低不能少于 1 个。</p><p>（3）<code>targetAverageUtilization</code> 指定 CPU 使用率，也就是自动扩容和缩容的触发条件，当 CPU 使用率超过 50% 时会触发自动动态扩容的行为，当回落到 50% 以下时，又会触发自动动态缩容的行为。</p><ol start="2"><li>命令行</li></ol><p>这种方式就是通过 <code>kubectl autoscale</code> 命令来实现创建 HPA 对象，实现自动扩容和缩容行为。比如和上面的例子等价的命令如下：   </p><p><code>kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10</code></p><p>通过参数来引入各个字段。</p><p>OK，本文就到这里，更多实践的例子大家可以参考 K8S 官网。下文我们将会探索 K8S 的容错机制。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 11 Pod 扩容与缩容 双十一前后的忙碌</title>
      <link href="/2018/09/29/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_11_Pod_%E6%89%A9%E5%AE%B9%E4%B8%8E%E7%BC%A9%E5%AE%B9_%E5%8F%8C%E5%8D%81%E4%B8%80%E5%89%8D%E5%90%8E%E7%9A%84%E5%BF%99%E7%A2%8C/"/>
      <url>/2018/09/29/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_11_Pod_%E6%89%A9%E5%AE%B9%E4%B8%8E%E7%BC%A9%E5%AE%B9_%E5%8F%8C%E5%8D%81%E4%B8%80%E5%89%8D%E5%90%8E%E7%9A%84%E5%BF%99%E7%A2%8C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 11 篇。</p><p>资源的伸缩在云计算环境中是至关重要的，云计算的动机就是企图提高资源的利用率，在用户请求高峰期的时候能够对资源进行横向扩容，反之，当用户请求回落低谷的时候，能够及时缩减资源，避免资源的浪费。</p><a id="more"></a><p>这就像双十一的时候，随着用户不断地涌入，阿里后台需要不断调配更多的资源来支撑用户大量的请求，当过了双十一当天，再慢慢缩减资源的使用。</p><p>Kubernetes 作为一个集群管理系统，提供了两种资源伸缩的方式：手动和自动。本文先来看手动方式。</p><p>Kubernetes 的资源伸缩本质上指的是 Pod 的扩容和缩容（scale up/down），也就是增加或减少 Pod 的副本数。</p><p>手动的方式是使用 <code>kubectl scale</code> 命令手动进行，或者基于 YAML 配置来实现。</p><p>首先，定义一个 <code>nginx-deployment.yaml</code> 配置文件：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web_server</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        images: nginx:1.12.1</span><br></pre></td></tr></table></figure><p>其中定义了 3 个副本，执行 <code>kubectl create -f nginx-deployment.yaml</code> 创建 Pod。</p><center><img src="/images/k8s/pod_scale.png" alt=""></center><p>如果现在遇到高峰请求，我们急需进行扩容，执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl scale deployment nginx-deployment --replicas 5</span><br></pre></td></tr></table></figure></p><p>将 Pod 扩增到 5 个。</p><center><img src="/images/k8s/pod_scale_up.png" alt=""></center><p>其中，用 <code>--replicas</code> 来指示增缩的数量，对于缩容，将 <code>--replicas</code> 设置为比当前 Pod 副本数量更小的数字即可，比如缩容到 2 个如下：</p><center><img src="/images/k8s/pod_scale_down.png" alt=""></center><p>可以看到，Pod 销毁会经历一个 <code>Terminating</code> 的过程，最终 3 个副本被删除，只保留了 2 个副本。</p><p>以上是通过命令的形式来实现手动的扩容和缩容，我们也可以修改 YAML 配置文件中的 <code>replicas</code> 来实现，只要修改完之后执行 <code>kubectl apply</code> 即可。</p><p>OK，本文到此为止，下文我们再来 Pod 伸缩的另一种方式——自动扩容和缩容。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 10 Job 机器人加工厂</title>
      <link href="/2018/09/26/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_10_Job_%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A0%E5%B7%A5%E5%8E%82/"/>
      <url>/2018/09/26/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_10_Job_%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%8A%A0%E5%B7%A5%E5%8E%82/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 10 篇。</p><p>通常，我们在执行任务时，会启用多个服务，有些任务需要长时间运行，全天 24 小时不中断，所以一般会启用 Daemon 类的 服务；而有些任务则只需要短暂执行，任务执行完，服务就没有存在的必要了。</p><a id="more"></a><p>容器提供服务运行的环境，根据任务持续运行的时间，将容器分为两类：服务类容器和工作类容器。</p><p>服务类容器需要一直运行来提供持续性的服务，而工作类容器则是运行一次性的任务，任务完成后便会退出。</p><p>前面学习的 Deployment、ReplicaSet 和 DaemonSet 都用于管理服务类容器，而工作类容器则由本文要讲得 Job 来管理。</p><p>Job 多用于执行一次性的任务，批处理任务等，Job 就像是现代化机械加工厂的机器人，当有任务来的时候，便会启动，按照预先设定好的程序执行任务，直至任务执行完，便会进入休眠状态。</p><p>进一步，Job 根据任务的类型和执行的动作又分为以下几类：   </p><ul><li>单 Job 单任务：只启动一个 Job 来完成任务，同时 Job 只启用一个 Pod ，适用于简单的任务。</li><li>多 Job 多任务：启动多个 Job 来处理批量任务，每个任务对应一个 Job，Pod 的数量可以自定义。</li><li>单 Job 多任务：采用一个任务队列来存放任务，启动一个 Job 作为消费者来处理这些任务，Job 会启动多个 Pod，Pod 的数量可以自定义。</li><li>定时 Job：也叫 CronJob，启动一个 Job 来定时执行任务，类似 Linux 的 Crontab 程序。</li></ul><p>上述 Job 的分类需要注意两点：</p><p>1）Job 执行失败的重启策略；Job 执行的是一次性的任务，但也不保证一定能执行成功，如果执行失败，应该怎么处理？这个是由前面所讲的 Pod 重启策略来决定的。在 Job Controller 中，只允许定义两种策略：  </p><ul><li>Never：Pod 执行失败，不会重启该 Pod，但会根据 Job 定义的期望数重新创建 Pod。</li><li>OnFailure：Pod 执行失败，则会尝试重启该 Pod。</li></ul><p>两种策略尝试的次数由 <code>spec.backoffLimits</code> 来限制，默认是 6 次（K8S 1.8.0 新加的特性）。</p><p>2）批量任务的多次并行处理的限制；对于批量任务，通常是一个 Pod 对应一个任务，但有时为了加快任务的处理，会启动多个 Pod 来并行处理单个任务。可以通过下面两个参数来设置并行度：</p><ul><li><code>spec.completions</code>：总的启动 Pod 数，只有当所有 Pod 执行成功结束，任务才结束。</li><li><code>spec.parallelism</code>：每个任务对应的 Pod 的并行数，当有一个 Pod 执行成功结束，该任务就执行结束。</li></ul><p>下面通过几个例子来实践一下上面的几种 Job 类别。</p><h3 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h3><h4 id="单-Job-单-Pod-执行一次性任务"><a href="#单-Job-单-Pod-执行一次性任务" class="headerlink" title="单 Job 单 Pod 执行一次性任务"></a>单 Job 单 Pod 执行一次性任务</h4><p>首先，定义 Job 的 yaml 配置文件 myjob.yaml：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: myjob</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: myjob</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: hello</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;echo&quot;, &quot;hello, I&apos;m Linux云计算网络, Welcome&quot;]</span><br><span class="line">      restartPolicy: Never</span><br></pre></td></tr></table></figure><p>执行 <code>kubectl create -f myjob.yaml</code> 创建 job 对象：</p><center><img src="/images/k8s/job-yaml.png" alt=""></center><p>可以看到期望创建的 Job 数为 1，成功执行的 Job 数也为 1，这表明该 Job 已经执行完任务退出了。这个 Job 执行的任务就是创建一个 Pod，Pod 中创建一个 busybox 容器，并进入容器输出一段字符串：<strong>“hello, I’m Linux云计算网络, Welcome”</strong>。</p><p>查看一下 Pod 的状态：   </p><center><img src="/images/k8s/job-pod.png" alt=""></center><p>可以看到，该 Pod 的状态为 <code>Completed</code>，表示它已经执行完任务并成功退出了。那怎么看该任务的执行结果呢？可以执行 <code>kubectl logs myjob</code> 调出该 Pod 的历史执行信息进行查看：</p><center><img src="/images/k8s/job-logs.png" alt=""></center><p>看到历史输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hello, I&apos;m Linux云计算网络, Welcome</span><br></pre></td></tr></table></figure><p>以上是执行成功的情况，如果执行失败，会根据 <code>restartPolicy</code> 进行重启，重启的方式上面也说了。大家可以自己实践下。</p><h4 id="多-Job-多-Pod-执行批量任务"><a href="#多-Job-多-Pod-执行批量任务" class="headerlink" title="多 Job 多 Pod 执行批量任务"></a>多 Job 多 Pod 执行批量任务</h4><p>首先，定义 Job 的 yaml 模板文件 job.yaml.txt，然后再根据这个模板文件创建多个 Job yaml 文件。模板文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: work-item-$ITEM</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: job</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: c</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo work item $ITEM &amp;&amp; sleep 2&quot;]</span><br><span class="line">      restartPolicy: Never</span><br></pre></td></tr></table></figure><p>其中，<code>$ITEM</code> 作为各个 Job 项的标识。接着，使用以下脚本，根据 Job 模板创建三个 Job 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">for i in app book phone</span><br><span class="line">do</span><br><span class="line">  cat myjob_tmp.yaml | sed &quot;s/\$ITEM/$i/g&quot; &gt; ./jobs/job-$i.yaml</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>最后，创建三个 Job 对象，如下：</p><center><img src="/images/k8s/job-multi.png" alt=""></center><h4 id="单-Job-多-Pod-执行批量任务"><a href="#单-Job-多-Pod-执行批量任务" class="headerlink" title="单 Job 多 Pod 执行批量任务"></a>单 Job 多 Pod 执行批量任务</h4><p>这种方式是用一个队列来存放任务，然后启动一个 Job 来执行任务，Job 可以根据需求启动多个 Pod 来承载任务的执行。定义下面的配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: myjob</span><br><span class="line">spec:</span><br><span class="line">  completions: 6</span><br><span class="line">  parallelism: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: myjob</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: hello</span><br><span class="line">        images: busybox</span><br><span class="line">        command: [&quot;echo&quot;, &quot;hello Linux云计算网络&quot;]</span><br><span class="line">      restartPolicy: OnFailure</span><br></pre></td></tr></table></figure><p>这里用到了上面说的两个参数：<code>completions</code> 和 <code>parallelism</code>，表示每次并行运行两个 Pod，直到总共 6 个 Pod 成功运行完成。如下：</p><center><img src="/images/k8s/job-para.png" alt=""></center><p>可以看到 DESIRED 和 SUCCESSFUL 最终均为 6，符合预期，实际上也有 6 个 Pod 成功运行并退出，呈 <code>Completed</code> 状态。</p><p>随便查看其中一个 Pod 的历史执行情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl logs myjob-5lfnp</span><br><span class="line">hello Linux云计算网络</span><br></pre></td></tr></table></figure><h4 id="定时任务-CronJob"><a href="#定时任务-CronJob" class="headerlink" title="定时任务 CronJob"></a>定时任务 CronJob</h4><p>定义一个 CronJob 配置文件，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  schedule: &quot;*/1 * * * *&quot;</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: hello</span><br><span class="line">            images: busybox</span><br><span class="line">            command: [&quot;echo&quot;, &quot;Hello Linux云计算网络&quot;]</span><br><span class="line">          restartPolicy: OnFailure</span><br></pre></td></tr></table></figure><p>kind 类型为 CronJob，<code>spec.schedule</code> 表示定时调度，指定什么时候运行 Job，格式与 Linux 的 Crontab 命令是一样的，这里 <code>*/1 * * * *</code> 的含义是每一分钟启动一次。</p><p>创建 CronJob 对象，通过 <code>kubectl get cronjob</code> 查看 CronJob 的状态：  </p><center><img src="/images/k8s/cronjob-get.png" alt=""></center><p>过一段时间再查看 Pod 的状态：</p><center><img src="/images/k8s/cronjob.png" alt=""></center><p>可以看到，此时产生了 3 个 Pod，3 个 Jobs，这是每隔一分钟就会启动一个 Job。执行 <code>kubectl logs</code> 查看其中一个的历史执行情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kubectl logs hello-1536764760-lm5kt</span><br><span class="line">Hello Linux云计算网络</span><br></pre></td></tr></table></figure><p>到此，本文就结束了。我们从理论结合实践，梳理了 Job 的几种类型，下文我们开始看一种有状态的 Controller——StatefulSet。</p><p>同样，需要学习资料的后台回复“K8S” 和 “K8S2”，想加群学习回复“加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 09 DaemonSet 我是一只看门狗</title>
      <link href="/2018/09/19/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_09_DaemonSet_%E6%88%91%E6%98%AF%E4%B8%80%E5%8F%AA%E7%9C%8B%E9%97%A8%E7%8B%97/"/>
      <url>/2018/09/19/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_09_DaemonSet_%E6%88%91%E6%98%AF%E4%B8%80%E5%8F%AA%E7%9C%8B%E9%97%A8%E7%8B%97/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 9 篇。</p><p>和上文中的 Deployment 一样，DaemonSet 也是一种副本管理机制，和 Deployment 可以在每个 Node 上运行好几个 Pod 副本不同的是，DaemonSet 始终保证每个 Node 最多只会运行一个副本，就像它的名称一样，作为一只看门狗（Daemon）守护在主人家里。</p><a id="more"></a><p>那么，哪些应用适合用 DaemonSet 的方式来部署呢？</p><p>主要有以下几类：</p><ul><li>监控类的，比如 Prometheus，collectd，New Relic agent，Ganglia gmond 等。</li><li>系统管理类的，比如 kube-proxy, kube-flannel 等。</li><li>日志收集类的，比如 fluentd，logstash 等。</li><li>数据存储类的，比如 glusterd, ceph 等。</li><li>……</li></ul><p>其中，系统管理类的应用主要是 K8S 自身的一些系统组件，我们可以通过 <code>kubectl get daemonset --namespace=kube-system</code> 查看到：  </p><center><img src="/images/k8s/daemon-sys.png" alt=""></center><p>DaemonSet <code>kube-proxy</code> 和 <code>kube-flannel-ds</code> 有 3 个副本，分别负责在每个节点上运行 kube-proxy 和 flannel 组件。</p><p>kube-proxy 前面的文章讲过，它有负载均衡的功能，主要将外部对 Service 的访问导向后端的 Pod 上。显然，一个 Node 运行一个负载均衡器足矣。</p><p>我们可以通过 <code>kubectl edit daemonset kube-proxy --namespace=kube-system</code> 来查看 kube-proxy 的 yaml 配置文件。</p><center><img src="/images/k8s/kube-proxy-dae.png" alt=""></center><p>可以看到它的 kind 是 DaemonSet。</p><p>接着再来看 kube-flannel-ds，这是一个网络插件组件，主要用于构建 K8S 的集群网络，这里大家不懂可以跳过，不影响本文的理解，后面在讲到 K8S 网络的时候会重点讲这个网络方案。</p><p>这里我们只需要知道，各个 Pod 间的网络连通就是 flannel 来实现的。</p><p>这是一个第三方的插件，我们可以直接下载它的 yaml 文件进行安装，执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><p>得到 kube-flannel.yml 文件：</p><center><img src="/images/k8s/kube-flannel-dae.png" alt=""></center><p>这里只列出了一部分内容，kind 类型是 DaemonSet。</p><p>其实 DaemonSet 配置文件的语法和结构和 Deployment 几乎完全一样，不同就在于将 kind 设为 DaemonSet。</p><p>OK，DaemonSet 的探讨就到这里，下文我们继续讨论另外一种 Controller：Job。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 08 Deployment 副本管理 重新招一个员工来填坑</title>
      <link href="/2018/09/16/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_08_Deployment_%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86_%E9%87%8D%E6%96%B0%E6%8B%9B%E4%B8%80%E4%B8%AA%E5%91%98%E5%B7%A5%E6%9D%A5%E5%A1%AB%E5%9D%91/"/>
      <url>/2018/09/16/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_08_Deployment_%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86_%E9%87%8D%E6%96%B0%E6%8B%9B%E4%B8%80%E4%B8%AA%E5%91%98%E5%B7%A5%E6%9D%A5%E5%A1%AB%E5%9D%91/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文首发于我的公众号 <strong>「Linux云计算网络」</strong> ，专注于干货分享，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，欢迎大家关注，二维码文末可以扫。</p></blockquote><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 8 篇。</p><p>Deployment 是 K8S v1.2 引入的概念，与之一起引入还有 ReplicaSet。这两个概念是等同的，准确说是 Deployment 内部调用 ReplicaSet 来实现。</p><a id="more"></a><p>之前这个概念是由 Replication Controller 来实现的，但由于和 K8S 代码中的模块重名，所以就改成 Deployment + ReplicaSet 的组合。</p><p>Deployment 实现了 Pod 的副本管理，使得应用的表现形态和用户期望的状态保持一致。比如用户期望应用部署为 3 副本，如果在运行过程中有一个副本挂了，那么 Deployment 会自动拉起一个副本。</p><p>Deployment 对于应用的编排、自动扩容和缩容、升级回滚等功能都是至关重要的。</p><p>下面我们通过一个例子来看看 Deployment 是如何工作的。</p><p>定义一个 <code>nginx.yaml</code> 文件（对 yaml 文件不熟悉的可以查阅这篇文章）：   </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1   </span><br><span class="line">kind: Deployment </span><br><span class="line">metadata:  </span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2  </span><br><span class="line">  template:  </span><br><span class="line">    metadata:</span><br><span class="line">      labels:  </span><br><span class="line">        app: web-server</span><br><span class="line">    spec: </span><br><span class="line">      containers:  </span><br><span class="line">      - name: nginx  </span><br><span class="line">        images: nginx:1.12.1     </span><br><span class="line">        ports:  </span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure><p>这个文件定义了一个 nginx 容器应用，两个 Pod 副本。也就是每个 Pod 中会跑一个 nginx 应用。</p><p>执行<code>kubectl create -f nginx.yaml</code>创建 Deployment 对象，在执行 <code>kubectl get deploy</code> 查看创建的 Deployment。</p><center><img src="/images/k8s/deploy-yaml.png" alt=""></center><p>可以看到，其中两个参数 <code>desired</code>（期待副本数）和 <code>current</code>（当前副本数）都为 2，保持一致，我们再执行 <code>kubectl get pod -o wide</code> 查看当前 Pod 的情况：</p><center><img src="/images/k8s/deploy-pod.png" alt=""></center><p>可以看到，创建了两个 Pod 自动调度到了 Node1 和 Node2 上。这说明每个 Pod 副本是由 Deployment 统一创建并维护的。</p><p>为了一探究竟，我们继续深挖 Deployment。</p><p>执行 <code>kubectl describe deployment nginx-deployment</code> 查看该 Deployment 的详细信息。</p><center><img src="/images/k8s/deploy-replica.png" alt=""></center><p>图中圈住的地方告诉我们，这里创建了一个 ReplicaSet，也就是说 Deployment 内部是调用 ReplicaSet 来完成 Pod 副本的创建的。是否是这样，我们继续验证。</p><p>执行 <code>kubeclt get replicaset</code> 显示创建的 ReplicaSet 对象：</p><center><img src="/images/k8s/deploy-get-replica.png" alt=""></center><p>可以看到这里的 ReplicaSet 名称和上面 Deployment 信息里显示的是一样的，同样，执行 <code>kubectl describe replicaset xxx</code> 显示该 ReplicaSet 的详细信息。</p><center><img src="/images/k8s/deploy-replica-events.png" alt=""></center><p>图中，有两处地方值得注意。一处是 <code>Controlled By</code>，表明 ReplicaSet 是由谁创建并控制的，显然这里显示是 Deployment。第二处是 <code>Events</code>，Events 记录了 K8S 中每一种对象的日志信息，这里的信息有助于排错查问题。我们可以看到这里记录了两个 Pod 副本的创建，Pod 的名称和我们在上面执行 <code>kubectl get pod</code> 看到的结果是一样的。</p><p>继续执行 <code>kubectl describe pod xxx</code> 查看其中一个 Pod 的详细信息：</p><center><img src="/images/k8s/deploy-pod-des.png" alt=""></center><p>可以看到这个 Pod 是由 ReplicaSet 创建的。</p><p>到此，我们不难得出下面这幅图：</p><center><img src="/images/k8s/deploy-pod1.png" alt=""></center><p>用户通过 kubeclt 创建 Deployment，Deployment 又创建 ReplicaSet，最终由 ReplicaSet 创建 Pod。</p><p>从命名上我们也可以看出，子对象的名字 = 父对象的名字 + 随机字符串。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文我们从实践上剖析了 Deployment 创建 Pod，实际上经过 ReplicaSet 进行创建。Deployment 最主要是对 Pod 进行副本管理，这样可以进行很多自动化管理的复杂操作，后面我们逐步从实践上去剖析 Pod 的各种操作。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>我的公众号 <strong>「Linux云计算网络」</strong> ，号内有大量书籍和视频资源，后台回复<strong>「1024」</strong>即可领取，分享的内容包括但不限于云计算虚拟化、容器、OpenStack、K8S、雾计算、网络、工具、SDN、OVS、DPDK、Linux、Go、Python、C/C++编程技术等内容，欢迎大家关注。</p><p><img src="/images/weichat.png" alt=""></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>历史上那些具有跨时代意义的云计算创业公司</title>
      <link href="/2018/09/13/tech/cloud/%E5%8E%86%E5%8F%B2%E4%B8%8A%E9%82%A3%E4%BA%9B%E5%85%B7%E6%9C%89%E8%B7%A8%E6%97%B6%E4%BB%A3%E6%84%8F%E4%B9%89%E7%9A%84%E4%BA%91%E8%AE%A1%E7%AE%97%E5%88%9B%E4%B8%9A%E5%85%AC%E5%8F%B8/"/>
      <url>/2018/09/13/tech/cloud/%E5%8E%86%E5%8F%B2%E4%B8%8A%E9%82%A3%E4%BA%9B%E5%85%B7%E6%9C%89%E8%B7%A8%E6%97%B6%E4%BB%A3%E6%84%8F%E4%B9%89%E7%9A%84%E4%BA%91%E8%AE%A1%E7%AE%97%E5%88%9B%E4%B8%9A%E5%85%AC%E5%8F%B8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/cloud/8.jpg" alt=""></center><p>这里所说的不是像 Google、VMware、Cisco、Intel 等等这些巨头公司，而是在巨头的夹缝中冉冉升起的那些新星。由于知识面有限，列举不尽完全，大家可以留言说说你心目中还有哪些值得铭记的公司。</p><a id="more"></a><h3 id="Rackspace"><a href="#Rackspace" class="headerlink" title="Rackspace"></a>Rackspace</h3><hr><p>Rackspace 理应不算新星了，但说到“跨时代意义”，它是绝对的名副其实。原因就在于它和 NASA（美国国家航空航天局）合作开发了 OpenStack——一个让云计算产业进入井喷时代的项目。OpenStack 一经开源，短短两年时间就力压群雄成为开源社区仅次于 Linux 的全球第二大开源项目。</p><center><img src="/images/cloud/rackspace.jpg" alt=""></center><p>可以说是 OpenStack 拯救了 Rackspace。Rackspace 于 1998 成立，是最早期的一批云计算提供商，OpenStack 出来之前，一直不温不火，2010 年 OpenStack 出来之后，名声大震，一步跃居为全球仅次于 Amazon 和 VMware 的三大云计算中心之一。</p><p>如今 OpenStack 已经更新到 Rocky 版本，在众多竞争者和效仿者的围追堵截之下，依然很强势，这和它优秀的架构是分不开的。我很好奇它更新到 Z 版本之后会发生什么？</p><h3 id="dotCloud"><a href="#dotCloud" class="headerlink" title="dotCloud"></a>dotCloud</h3><hr><p>OpenStack 带动了 IaaS 的发展，在这个节骨眼上，人们开始意识到，只有 IaaS 已经无法满足用户变态的需求了。</p><p>时间也落在 2010 年，这个时候几个大胡子年轻人在旧金山成立了一家做 PaaS 的公司，起名 dotCloud，开始杀入 PaaS 领域。</p><center><img src="/images/cloud/dotcloud.png" alt=""></center><p>年轻一辈都意识到了商机，大佬们（Google、Microsoft、Amazon 等）能不意识到吗？于是大佬们也纷纷涉足 PaaS 领域，最终 dotCloud 寡不敌众，不得不缴械投降。</p><p>dotCloud 的工程师于心不甘啊，辛辛苦苦画下的大饼就这样被割分完了。但也没办法，谁让人家是大佬呢。</p><p>鉴于工程师们血液里都流淌着一股热爱分享的劲儿，他们决定将 dotCloud 的核心技术开源给世人。</p><p>谁能想到，无心插柳柳成荫。这门技术瞬间风靡全球，开启了又一个新的时代。</p><p>这门技术就是 Docker。</p><center><img src="/images/cloud/docker.jpg" alt=""></center><p>dotCloud 又火了，但为了纪念这个神圣的时刻，dotCloud 改名为 Docker Inc，全身心投入 Docker 的研发中。至于原来的 PaaS 业务，Docker 将其卖给了德国人的 cloudControl。但好景不长，cloudControl 于 2016 年就关闭了。</p><p>如今，Docker 技术依然可圈可点，虽然很多人在 Kubernetes 出来之后，叫衰 Docker，但我却不以为然。</p><h3 id="Nicira"><a href="#Nicira" class="headerlink" title="Nicira"></a>Nicira</h3><hr><p>2007 年，斯坦福大学的 Nick McKeown 教授和他的天才学生 Martin Casado 博士根据他们的研究成果创办了 Nicira。这是 SDN 网络的鼻祖公司，Nick 教授和 Martin 博士也因此被人们称为 SDN 之父。</p><p>Nicira 公司做了很多牛逼的事：发明了世界上第一个 SDN 控制器 NOX，第一个分布式交换机 Open vSwitch 及其配套协议 OpenFlow 协议，领导研发 OpenStack 网络驱动模块 Quantum/Neutron，开源了业界第一个与硬件无关，支持多种 X86 虚拟化平台的分布式网络虚拟化架构（DVNI），等等等等。</p><p>Nicira 当时那套 SDN 的解决方案，放在今天来说都是很超前的东西。很快，Nicira 就被 VMware 和 Cisco 这些巨头盯上了。巨头们看上的不仅是 Nicira 的技术，更是 Nicira 那帮工程师天才般的智慧和对技术敏锐的洞察力。</p><p>巨头们开始了疯狂的收购战，最终 VMware 以 12.6 亿美元拔得头筹。Martin 博士也带领他的一般众将归入了 VMware 的麾下。</p><center><img src="/images/cloud/nicira.jpg" alt=""></center><p>至此，Nicira 的舞台也算退出了。Open vSwitch 如今仍然占据 SDN 数据面的头把交椅。</p><h3 id="Palo-Alto-Networks"><a href="#Palo-Alto-Networks" class="headerlink" title="Palo Alto Networks"></a>Palo Alto Networks</h3><hr><p>2005 年，以色列的一个天才少年 Nir Zuk 在一间简陋的办公室里，一手创办了 PAN，这就是当下极富盛名的下一代防火墙的初创者（很多公司都宣称自己的防火墙是下一代防火墙，众说纷坛，其实争论这个没多大意义，人家 PAN 都还没说话呢）。</p><center><img src="/images/cloud/palo.jpg" alt=""></center><p>回看 Nir 的一生，也是颇具传奇，经历像极了乔布斯，但没乔布斯那么惨是被老东家赶出的，Nir 是因为老东家小气不肯给他资源做项目而无奈出走。谁能想到，PAN 能在短时间之内就超越了老东家。</p><blockquote><p>小公司要在大公司的夹缝中生长，需要天时地利人和，以及运气，就像 Docker，上帝为你关上一扇门，就会为你打开一扇窗。</p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 01 云计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> OpenStack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 07 豌豆荚之旅（二）</title>
      <link href="/2018/09/11/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_07_%E8%B1%8C%E8%B1%86%E8%8D%9A%E4%B9%8B%E6%97%85%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/09/11/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_07_%E8%B1%8C%E8%B1%86%E8%8D%9A%E4%B9%8B%E6%97%85%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/9.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学习 K8S，这是系列第 7 篇。</p><p>上篇我们简单学习了 Pod 的基础知识，本篇开始讲述一些 Pod 的高阶知识（本文只做理论的简单阐述，后面会针对每个点进行实践）。</p><a id="more"></a><h3 id="Pod-的生命周期管理"><a href="#Pod-的生命周期管理" class="headerlink" title="Pod 的生命周期管理"></a>Pod 的生命周期管理</h3><p>豌豆荚自诞生之日起，便注定要经历生老病死的一生。Pod 是由容器组成的，Pod 生命周期实际上是由容器的生命周期决定的。</p><p>在整个生命周期过程中，Pod 会被定义为各种状态，如下：</p><center><img src="/images/k8s/pod_lifetime.png" alt=""></center><p>这些状态包括正常状态和异常状态，当出现异常状态时，K8S 的监控机制会检测到这种异常，并执行相应的异常处理。</p><h3 id="Pod-的监控机制"><a href="#Pod-的监控机制" class="headerlink" title="Pod 的监控机制"></a>Pod 的监控机制</h3><p>Pod 的监控主要是监控 Pod 内容器的健康状况，并进行相关的异常处理和容错管理。</p><p>当监控到某个容器异常退出或健康检查失败时，Pod 会执行重启策略，使得 Pod 内的容器健康运转。</p><p>如下记录了 Pod 的重启策略和健康检查机制：</p><center><img src="/images/k8s/pod_jk.png" alt=""></center><h3 id="Pod-的调度管理"><a href="#Pod-的调度管理" class="headerlink" title="Pod 的调度管理"></a>Pod 的调度管理</h3><p>K8S Master 上的 Scheduler 服务负责实现 Pod 的调度管理，Pod  是静态的，只有真正被调度到具体的节点上才能发挥它的作用。K8S 根据不同的应用场景，定义了多种不同的调度策略。这些策略可以是根据算法自动完成的，也可以是人为指定的。具体可以看下面这张导图：</p><center><img src="/images/k8s/pod_scheduler.png" alt=""></center><p>笼统来看，有时候为了权衡应用场景和集群资源的需求，需要对 Pod 进行扩容和缩容，这同样属于 Pod 调度管理的范畴。</p><h3 id="Pod-的存储管理"><a href="#Pod-的存储管理" class="headerlink" title="Pod 的存储管理"></a>Pod 的存储管理</h3><p>Pod 和容器的数据存储使用 Volume，K8S Volume 和 Docker 的 Volume 是一样的原理，都是文件系统上的一个目录，只不过在 K8S 中实现了更多的 backend driver。包括 emptyDir、hostPath、GCE Persistent Disk、NFS、Ceph 等。</p><center><img src="/images/k8s/pod_volume.png" alt=""></center><p>Volume 提供了对各种 driver 的抽象，容器在使用 Volume 读写数据的时候不需要关心底层具体存储方式的实现，对它来说，所有类型的 Volume 都是一个目录。</p><p>当 Volume 被挂载到 Pod 中时，这个 Pod 中的容器都会共享这个 Volume，当其中的容器销毁时，Volume 中的数据也不会丢失，当 Pod 销毁时，根据不同的 driver 实现，数据也可以保存下来。</p><p>Volume 提高了 Pod 内数据的持久化管理，延长了 Pod 和容器的生命周期。</p><h3 id="Pod-的网络管理"><a href="#Pod-的网络管理" class="headerlink" title="Pod 的网络管理"></a>Pod 的网络管理</h3><p>在 K8S 中，定义了多种资源对象，很多对象本身就是一个通信的实体，比如容器、Pod、Service、Node。</p><p>K8S 维护这多种对象之间的通信关系，比如：Pod 内容器之间的通信、Pod 与容器之间的通信、Pod 之间的通信、Pod 与 Service 之间的通信，以及外部的访问。</p><p>这些通信机制的建立离不开 K8S 建立的完善的网络模型。K8S 使用了 CNI（容器网络规范）来标准化、归一化网络模型。</p><p>第三方的厂商或开发者可以根据自身网络需求，遵从 CNI 的规范，实现各种网络方案，并以插件的形式提供给 K8S 使用。目前比较知名的网络方案有：flannel、calico、weave、canal 等。</p><center><img src="/images/k8s/pod_net.png" alt=""></center><p>这些网络方案各有千秋、虽然实现方式各有区别，但殊途同归，最终都是满足 K8S 中各种实体间的通信需求。</p><p>OK，本文就到这里，我们通过两篇文章大致梳理了豌豆荚从出生到死亡要面临的多种人生的关卡。跨过去了，就成熟了，希望我们都能跨过自己人生的关卡。</p><p>下文我们开始进入实践的部分。</p><p>为了给大家更多的福利，这个系列的每一篇文章我都会送一些电子书，可能有重的，也有一些新书，之前送了《K8S 指南》和《容器与容器云》，这次送一本新书《》，大家有需要的后台回复“K8S2”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 06 豌豆荚之旅（一）</title>
      <link href="/2018/09/09/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_06_%E8%B1%8C%E8%B1%86%E8%8D%9A%E4%B9%8B%E6%97%85%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2018/09/09/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_06_%E8%B1%8C%E8%B1%86%E8%8D%9A%E4%B9%8B%E6%97%85%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/8.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学习 K8S，这是系列第 6 篇。</p><p>Pod 中文译为豌豆荚，很形象，豌豆荚里面包裹的多颗小豌豆就是容器，小豌豆和亲密无间的老伙计壳荚子自出生之日起就得面对各种各样的人生大事：  </p><a id="more"></a><ul><li>容器、Pod、Node 之间的关系</li><li>Pod 的生命周期管理</li><li>Pod 的调度管理</li><li>Pod 的监控</li><li>Pod 的升级与回滚</li><li>Pod 的扩容与缩容</li><li>Pod 的存储管理</li><li>Pod 的网络管理</li><li>……</li></ul><h3 id="为什么需要-Pod？"><a href="#为什么需要-Pod？" class="headerlink" title="为什么需要 Pod？"></a>为什么需要 Pod？</h3><p>我们假设没有 Pod，应用部署的最小单元就是容器，会有什么问题？首先，应用调度粒度太细，不便于管理。想象一下淘宝网站运行着海量应用，每个应用又拆分成多个服务，每个服务部署在一个容器里，一个集群管理系统要管理庞大的容器集群，既要顾忌不同应用之间的隔离性，又要考虑相同应用之间的关联性，这在管理上将会是灾难性的难题。</p><p>其次，资源利用率低。有很多应用之间存在某种强关联关系，它们需要彼此能共享对方的资源，双方的交互需要快捷有效，如果把它们部署到单独的容器中，资源利用和通信将成为最主要的系统瓶颈。</p><p>Pod 的提出改变了这种局面，它将强关联的应用整合在一起，作为一个整体对外提供服务，既简化了管理的难度，又提高了资源的利用率。</p><p>那哪些应用是强关联，适合放到一个 Pod 中呢？举个例子，比如下面这个 Pod 包含两个容器，一个 File Puller，一个是 Web Server。</p><center><img src="/images/k8s/pod.png" alt=""></center><p>File Puller 会定期从外部的 Content Manager 中拉取最新的文件，将其存放在 Volume 中。然后 Web Server 从 Volume 中读取文件，来响应 Consumer 的请求。</p><p>这两个容器通过 Volume 来共享实时的数据，协作处理一个 Consumer 的请求，把它们放到同一个 Pod 中就是合适的。</p><p>如果有应用和任何应用之间都不存在联系，那么它们就单独部署在一个 Pod 中，称为<code>one-container-per-pod</code>。即便只有一个容器，K8S 管理的也是 Pod 而不是直接管理容器。</p><p>综上，Pod 在设计的时候，主要动机有以下两点：</p><ol><li>方便管理</li></ol><p>Pod 提供了比容器更高一层的抽象，K8S以 Pod 为最小单元进行应用的部署、调度、扩展、共享资源和管理周期。</p><ol start="2"><li>资源共享和通信</li></ol><p>Pod 内的所有容器共享同一个网络空间，它们之间可以通过 <code>localhost</code> 相互通信。同样，所有容器共享 Volume，一个容器挂载一个 Volume，其余容器都可以访问这个 Volume。</p><h3 id="容器、Pod、Node-之间的关系"><a href="#容器、Pod、Node-之间的关系" class="headerlink" title="容器、Pod、Node 之间的关系"></a>容器、Pod、Node 之间的关系</h3><p>容器是 Pod 的一个属性，定义了应用的类型及共享的资源。每个容器会分配一个 Port，Pod 内的容器通过 localhost:Port 的形式来通信。</p><p>一个 Pod 包含一个或多个容器，每个 Pod 会分配一个唯一的 IP 地址，Pod 内的多个容器共享这个 IP 地址，每个容器的 Port 加上 Pod IP 共同组成一个 <code>Endpoint</code>，共同对外提供服务。</p><p>在部署应用的时候，Pod 会被 Master 作为一个整体调度到一个 Node 上。如果开启多副本管理，则多个 Pod 会根据调度策略调度到不同的 Node 上。如果 Node 宕机，则该 Node 上的所有 Pod 会被自动调度到其他 Node 上。</p><p>下面是容器、Pod、Node 三者之间的关系图：  </p><center><img src="/images/k8s/pod_docker_node.png" alt=""></center><h3 id="Pod-根容器"><a href="#Pod-根容器" class="headerlink" title="Pod 根容器"></a>Pod 根容器</h3><p>Pod 中有一个特殊的容器，叫 Pod 的根容器——Pause 容器，这是一个很小的容器，镜像大小大概为 200KB。</p><center><img src="/images/k8s/pod_pause.png" alt=""></center><p>Pause 容器存在的意义是: <strong>维护 Pod 的状态信息</strong>。</p><p>由于 Pod 是作为一个整体进行调度，我们难以对这个整体的信息进行简单的判断和有效地进行行动。</p><p>想象一下，假如 Pod 内一个容器死亡了，是算整体死亡呢还是 N/M 死亡率，如果 Pod 内所有容器都死亡了，那是不是该 Pod 也就死亡了，如果加入新的容器或原有容器故障恢复呢，如何让新成员快速融入环境？</p><p>理论上，虽然 Pod 是由一组容器组成的，但 Pod 和容器是彼此独立的，也就是容器的故障不应该影响 Pod 的存在，Pod 有相应的手段来保证容器的健康状况。</p><p>引入与业务无关的，并且不易死亡的 Pause 容器就可以很好的解决这个问题，Pause 容器的状态就代表了 Pod 的状态，只要 Pause 不死，那么不管应用容器发生什么变化，Pod 的状态信息都不会改变。</p><p>这样，Pod 内的多个应用容器共享 Pause 容器的 IP 和 Volume，当加入新的容器或者原有的容器因故障重启后就可以根据 Pause 保存的状态快速学习到当前 Pod 的状态。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文简单学习了 Pod 的初级知识，包括 Pod 的设计动机，容器、Pod 和 Node 之间的关系，以及 Pod 的守护者——Pause 容器。</p><p>容器的 Port + Pod IP = Endpoint，构成一个 Pod 的通信实体，Pod 中的容器共享网络和存储，这些共享信息是由 Pause 容器来维护的。</p><p>下文继续豌豆荚之旅的第二个部分，学习 Pod 的管理哲学。</p><p>为了给大家更多的福利，这个系列的每一篇文章我都会送一些电子书，可能有重的，也有一些新书，之前送了《K8S 指南》和《容器与容器云》，这次送一本由 K8S 中文社区主编的《K8S 中文手册》，大家有需要的后台回复“K8S2”</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 05 使用产品前请先阅读说明书</title>
      <link href="/2018/09/07/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_05_%E4%BD%BF%E7%94%A8%E4%BA%A7%E5%93%81%E5%89%8D%E8%AF%B7%E5%85%88%E9%98%85%E8%AF%BB%E8%AF%B4%E6%98%8E%E4%B9%A6/"/>
      <url>/2018/09/07/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_05_%E4%BD%BF%E7%94%A8%E4%BA%A7%E5%93%81%E5%89%8D%E8%AF%B7%E5%85%88%E9%98%85%E8%AF%BB%E8%AF%B4%E6%98%8E%E4%B9%A6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/7.png" alt=""></center><hr><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 5 篇。</p><p>生活中，随处可见，几乎每一款产品都会附带一份说明书，说明书可以记录产品的使用方法，也可以记录产品的配方。有了说明书，我们完全可以窥探一款产品的全貌。</p><a id="more"></a><p>在 K8S 中，<code>yaml</code> 配置文件就是 K8S 资源对象的说明书，定义了对象包含的元素及采取的动作，每种对象都可以通过 yaml 配置文件来创建。</p><center><img src="/images/k8s/yaml.jpg" alt=""></center><h3 id="yaml-是什么"><a href="#yaml-是什么" class="headerlink" title="yaml 是什么"></a>yaml 是什么</h3><p>yaml 是一种用来写配置文件的语言，没错，它是一门语言。如果你用过 json，那么对它就不会陌生，yaml 又被称为是 json 的超集，使用起来比 json 更方便。</p><p>结构上它有两种可选的类型：Lists 和 Maps。</p><p>List 用 -（破折号） 来定义每一项，Map 则是一个 key:value 的键值对来表示。如下是一个 json 文件到 yaml 文件的转换：  </p><p>json:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">    &quot;kind&quot;: &quot;Pod&quot;,</span><br><span class="line">    &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;xx&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;containers&quot;: [&#123;</span><br><span class="line">            &quot;name&quot;: &quot;front-end&quot;,</span><br><span class="line">            &quot;images&quot;: &quot;nginx&quot;,</span><br><span class="line">            &quot;ports&quot;: [&#123;</span><br><span class="line">                &quot;containerPort&quot;: &quot;80&quot;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;, &#123;</span><br><span class="line">            &quot;name&quot;: &quot;flaskapp-demo&quot;,</span><br><span class="line">            &quot;images&quot;: &quot;jcdemo/flaskapp&quot;,</span><br><span class="line">            &quot;ports&quot;: [&#123;</span><br><span class="line">                &quot;containerPort&quot;: &quot;5000&quot;</span><br><span class="line">            &#125;]</span><br><span class="line">        &#125;]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>yaml:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">    name: xx</span><br><span class="line">spec:</span><br><span class="line">    containers:</span><br><span class="line">        - name: front-end</span><br><span class="line">        images: nginx</span><br><span class="line">        ports:</span><br><span class="line">            - containerPort: 80</span><br><span class="line">    - name: flaskapp-demo</span><br><span class="line">        images: jcdemo/flaskapp</span><br><span class="line">        ports: 8080</span><br></pre></td></tr></table></figure></p><p>这个文件简单地定义了一个 Pod 对象，包含两个容器，我们可以很清晰地看到两者是如何转换的。</p><h3 id="K8S-创建资源的两种方式"><a href="#K8S-创建资源的两种方式" class="headerlink" title="K8S 创建资源的两种方式"></a>K8S 创建资源的两种方式</h3><p>在 K8S 中，有两种创建资源的方式：kubectl 命令和 yaml 配置文件。</p><p>两种方式各有各的好处。命令行的方式最为简单，一条命令就万事大吉，但缺点也很明显，你并不知道这条命令背后到底做了哪些事，配置文件就提供了一种让你知其然更知其所以然的方式。总的来说，它有以下好处：   </p><ul><li>完整性：配置文件描述了一个资源的完整状态，可以很清楚地知道一个资源的创建背后究竟做了哪些事；</li><li>灵活性：配置文件可以创建比命令行更复杂的结构；</li><li>可维护性：配置文件提供了创建资源对象的模板，能够重复使用；</li><li>可扩展性：适合跨环境、规模化的部署。</li><li>……</li></ul><p>当然，复杂的东西对用户就难以做到友好，我们需要熟悉它的配置文件的语法，有一定难度。下面举几个例子，让你对 yaml 配置文件有一个基本的认识。</p><h3 id="几个例子"><a href="#几个例子" class="headerlink" title="几个例子"></a>几个例子</h3><p>下面，我们分别来看看 <code>deployment</code>、<code>pod</code>、<code>service</code> 这三种资源的说明书都长啥样。</p><p>由于 K8S 对每种资源的定义非常庞杂，限于篇幅，我们只看一些必选的参数，目的是通过这几个例子，读懂 yaml 配置文件。</p><h4 id="deployment"><a href="#deployment" class="headerlink" title="deployment"></a>deployment</h4><p>定义 deployment 配置文件，命名为：nginx-deployment.yaml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1   # 1.9.0 之前的版本使用 apps/v1beta2，可通过命令 kubectl api-versions 查看</span><br><span class="line">kind: Deployment     #指定创建资源的角色/类型</span><br><span class="line">metadata:      #资源的元数据/属性</span><br><span class="line">    name: nginx-deployment    #资源的名字，在同一个namespace中必须唯一</span><br><span class="line">spec:</span><br><span class="line">    replicas: 2      #副本数量2</span><br><span class="line">    selector:      #定义标签选择器</span><br><span class="line">        matchLabels:</span><br><span class="line">            app: web-server</span><br><span class="line">    template:      #这里Pod的定义</span><br><span class="line">        metadata:</span><br><span class="line">            labels:      #Pod的label</span><br><span class="line">                app: web-server</span><br><span class="line">    spec:         # 指定该资源的内容  </span><br><span class="line">        containers:  </span><br><span class="line">        - name: nginx      #容器的名字  </span><br><span class="line">          images: nginx:1.12.1  #容器的镜像地址    </span><br><span class="line">          ports:  </span><br><span class="line">          - containerPort: 80  #容器对外的端口</span><br></pre></td></tr></table></figure></p><p>执行<code>kubectl create -f nginx.yaml</code>创建 deployment 资源：</p><center><img src="/images/k8s/deploy-yaml.png" alt=""></center><h4 id="pod"><a href="#pod" class="headerlink" title="pod"></a>pod</h4><p>定义 pod 配置文件，命名为 redis-pod.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod  </span><br><span class="line">metadata:  </span><br><span class="line">    name: pod-redis</span><br><span class="line">    labels:</span><br><span class="line">        name: redis</span><br><span class="line">spec: </span><br><span class="line">    containers:</span><br><span class="line">    - name: pod-redis</span><br><span class="line">        images: docker.io/redis  </span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80    #容器对外的端口</span><br></pre></td></tr></table></figure><p>执行<code>kubectl create -f pod-redis.yaml</code>创建 pod 资源：  </p><center><img src="/images/k8s/pod-yaml.png" alt=""></center><p>可以看到，成功创建一个 Pod，<code>ContainerCreating</code>表示 Pod 中的容器正在执行镜像的下载和安装过程，过一会儿，就显示<code>Running</code>了，表明 Pod 应用部署完成。</p><h4 id="service"><a href="#service" class="headerlink" title="service"></a>service</h4><p>定义 service 配置文件，命名为 httpd-svc.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1  </span><br><span class="line">kind: Service  # 指明资源类型是 service</span><br><span class="line">metadata:  </span><br><span class="line">    name: httpd-svc # service 的名字是 httpd-svc</span><br><span class="line">    labels:  </span><br><span class="line">        name: httpd-svc </span><br><span class="line">spec:  </span><br><span class="line">    ports:  # 将 service 8080 端口映射到 pod 的 80 端口，使用 TCP 协议</span><br><span class="line">    - port: 8080</span><br><span class="line">        targetPort: 80  </span><br><span class="line">        protocol: TCP  </span><br><span class="line">    selector:  </span><br><span class="line">        run: httpd # 指明哪些 label 的 pod 作为 service 的后端</span><br></pre></td></tr></table></figure><p>执行<code>kubectl create -f httpd-svc.yaml</code>创建 service 资源：  </p><center><img src="/images/k8s/svc-yaml.png" alt=""></center><p>可以看到，service httpd-svc 分配到一个 Cluster-IP 10.96.0.1，我们可以通过该 IP 访问 service 所维护的后端 Pod。</p><p>另外，还有一个 service kubernetes，这个是 Kubernetes API Server 的 service，Cluster 内部的各组件就是通过这个 service 来访问 API Server。 </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>yaml 是 K8S 资源对象的说明书，每个对象拥有哪些属性都可以在 yaml 中找到详尽的说明，初学者建议多写 yaml 文件，少用命令行。</p><p>以上三个例子只是对 yaml 文件做个简单说明，更详细的信息还是参考官网。</p><p>OK，本文就到此为止，下文我们开始进入豌豆荚之旅。觉得不错，别忘了转发分享给你的朋友哦。</p><blockquote><p>参考：<br><a href="https://www.kubernetes.org.cn/1414.html" target="_blank" rel="noopener">https://www.kubernetes.org.cn/1414.html</a></p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 04 架构是个好东西</title>
      <link href="/2018/09/05/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_04_%E6%9E%B6%E6%9E%84%E6%98%AF%E4%B8%AA%E5%A5%BD%E4%B8%9C%E8%A5%BF/"/>
      <url>/2018/09/05/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_04_%E6%9E%B6%E6%9E%84%E6%98%AF%E4%B8%AA%E5%A5%BD%E4%B8%9C%E8%A5%BF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/6.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第 4 篇。</p><p>任何技术的诞生，都会经历从架构设计到开发测试的过程，好的技术，往往也会有一套好的架构。架构是个好东西，它能帮助我们站在高处看清楚事物的整体结构，避免过早地进入细节而迷失方向。</p><a id="more"></a><p>上篇文章扫清了 K8S 的一些基本概念，今天这篇文章我们就来看看 K8S 的架构。</p><p>先上图：</p><center><img src="/images/k8s/architecture.png" alt=""></center><p>图中包括两种类型的节点：Master 和 Node，每个节点上运行着多种 K8S 服务。</p><h3 id="Master-节点"><a href="#Master-节点" class="headerlink" title="Master 节点"></a>Master 节点</h3><p>Master 是 K8S 集群的大脑，运行着如下 Daemon 服务：kube-apiserver、kube-controller-manager、kube-scheduler、etcd 等。</p><h4 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h4><p>如果把 K8S 环境看作是一个公司，那 API Server 就是这个公司的基础平台部，是公司最为核心的技术能力输出出口。它对外提供 HTTP/HTTPS REST API，统称 K8S API，可以供各类客户端工具（CLI 或 WebUI）、K8S 其他组件，以及第三方的平台接入。对内提供了 K8S 各类资源（如 Pod、Deployment、Service等）的增删改查和监控等操作，是集群内各个功能模块之间数据交互和通信的中心枢纽。</p><h4 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h4><p>Controller Manager 更像是公司的人力资源部，负责统筹公司的人员分布。它管理着 K8S 各类资源的生命周期，保证资源处于预期状态，如果现有状态和预期状态不符，它会自动化执行修正。</p><p>Controller Manager 由多种 Controller 组成，包括 Replication Controller、Node Controller、ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Service Controller、Token Controller 及 Endpoint Controller 等。每种 Controller 都负责一种具体的资源管控流程，而 Controller Manager 正是这些 Controller 的核心管理者。</p><h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>Scheduler 则像是公司各个部门的项目经理之类的角色，负责将具体的人力放到他们擅长的位置上，知人善用。具体来说，Scheduler 负责将待调度的 Pod 对象按照特定的调度策略绑定到集群中某个合适的节点上，调度策略会综合考虑集群的拓扑结构、节点的负载情况、以及应用对高可用、性能、数据亲和性的需求。</p><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>etcd 是一个高可用的分布式数据库，负责保存 K8S 的配置信息和各种资源的状态信息。当数据发生变化时，etcd 会及时告知集群中的其他组件。</p><h4 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h4><p>kubectl 是 K8S 的 CLI 工具，这是使用 K8S API 建立的一套命令行工具，使用它，可以非常方便地管理 K8S 集群。</p><h3 id="Node-节点"><a href="#Node-节点" class="headerlink" title="Node 节点"></a>Node 节点</h3><p>Node 是 K8S 集群的具体执行者，也运行着多种服务，如：kubelet、kube-proxy、container runtime、Pod 网络等。Node 可以看作是 Master 的代理，负责处理 Master 下发到本节点的任务，管理 Pod 和 Pod 中的容器，定期向 Master 汇报自身资源的使用情况。</p><h4 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h4><p>kubelet 更像是部门中各个小组的 Leader，对外从 API Server 拿资源，对内负责小组内各种资源的管理，比如从 Master 拿到 Pod 的具体配置信息（images、Volume 等）之后，kubelet 根据这些信息创建和运行容器。</p><h4 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h4><p>kube-proxy 则像穿插在公司各个部门之间的接口人，对内和内部人员沟通，对外协调部门之间的各种事宜，展示部门风采。kube-proxy 作用于 Service，通过前面学习，我们知道 Service 是对一组 Pod 的抽象，统一对外提供某种服务，当外部访问 Service 时，实际上是需要访问 Service 所管辖的 Pod 中的容器，kube-proxy 在这里就是负责将访问 Service 的数据流转发到后端的容器，如果有多个副本，kube-proxy 会实现负载均衡。</p><h4 id="cAdvisor"><a href="#cAdvisor" class="headerlink" title="cAdvisor"></a>cAdvisor</h4><p>cAdvisor 对 Node 上的资源进行实时监控和性能数据采集，包括 CPU 使用情况、内存使用情况、网络吞吐量及文件系统使用情况等。cAdvisor 集成在 kubelet 中，当 kubelet 启动时会自动启动 cAdvisor，一个cAdvisor 仅对一台 Node 机器进行监控。</p><h4 id="container-runtime"><a href="#container-runtime" class="headerlink" title="container runtime"></a>container runtime</h4><p>container runtime 是真正运行容器的地方，为容器提供运行环境，主流的三种 container runtime 是 lxc、runc 和 rkt，K8S 都支持它们，但常用的事 runc，原因是 runc 是 Docker 默认的 runtime。在 K8S 的容器应用中，Docker 是主流。</p><p>OK，K8S 架构介绍就到此为止。</p><p>最后，还是继续送书，容器网络专家倪朋飞写的《K8S 指南》电子书，如有需要后台回复“K8S”（之前回复过就不用回复了）。如需加群学习回复“加群”。</p><p>下文我们开始对 K8S 的说明书一探究竟。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 03 扫清概念</title>
      <link href="/2018/09/03/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_03_%E6%89%AB%E6%B8%85%E6%A6%82%E5%BF%B5/"/>
      <url>/2018/09/03/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_03_%E6%89%AB%E6%B8%85%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/5.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络，欢迎大家和我一起学 K8S，这是系列第三篇。</p><p>每一种技术，为了描述清楚它的设计理念，都会自定义一堆概念或术语。在进入一门技术的研究之前，我们有必要扫清它的基本概念。</p><a id="more"></a><h3 id="资源对象"><a href="#资源对象" class="headerlink" title="资源对象"></a>资源对象</h3><p>K8S 的操作实体，在 K8S 中，有很多的操作对象，比如容器、Pod、Deployment、Service、Node 等，我们统统称它们为资源对象。</p><h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p>K8S 集群，是计算、存储和网络资源的集合，K8S 基于这些资源来承载容器化的应用。</p><h3 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h3><p>K8S 集群的大脑，负责整个集群的管控、资源调度。可以部署在普通物理机或虚拟机上，为了实现高可用，可以部署多个 Master。</p><h3 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h3><p>K8S 集群的执行者，受 Master 指挥，负责运行和监控容器应用、管理容器的生命周期，并向 Master 定期汇报容器的状态。同样，Node 也可以部署在物理机或虚拟机之上，也可以部署多个。</p><h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><p>在 K8S 集群中，Pod 是资源调度的最小单位，一个 Pod 可以包含一个或多个容器应用，这些容器应用彼此之间存在某种强关联。Pod 内的所有容器应用共享计算、存储、网络资源。</p><h3 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h3><p>Controller 是 K8S 中负责管理 Pod 的资源对象，它定义 Pod 的部署属性，比如有几个副本，副本异常怎么处理等，如果把 Pod 副本看做是一个公司职员，那么 Controller 就像是 HR，会不断根据人员的变动来招人满足公司的发展需求。</p><p>为了满足多种业务场景，K8S 提供了多种 Controller，包括 Deployment、ReplicaSet、DaemonSet、StatefulSet、Job 等。</p><h4 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h4><p>Deployment 是最常用的 Controller，定义了用户的期望场景，实现了 Pod 的多副本管理，如果运行过程中有一个副本挂了（员工离职），那么 K8S 会根据 Deployment 的定义重新拉起一个副本继续工作（招一个新员工），始终保证 Pod 按照用户期望的状态运行。</p><h4 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h4><p>ReplicaSet 和 Deployment 实现了同样的功能，确切的说是 Deployment 通过 ReplicaSet 来实现 Pod 的多副本管理。我们通常不需要直接使用 ReplicaSet。</p><h4 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h4><p>DaemonSet 用于每个 Node 最多只运行一个副本的场景，通常用于运行 Daemon。</p><h4 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h4><p>Job 用于运行结束就删除的应用，而其他 Controller 则是会长期保持运行。</p><h4 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h4><p>以上 Controller 都是无状态的，也就是说副本的状态信息会改变，比如当某个 Pod 副本异常重启时，其名称会改变。StatefulSet 提供有状态的服务，能够保证 Pod 的每个副本在其生命周期中名称保持不变。这是通过持久化的存储卷来实现的。</p><h3 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h3><p>Label 定义了其他资源对象所属的标签，类似于你在公司被分到 A 小组、B 小组。有了标签，就可以针对性地对每个小组进行管理。比如把某个小组搬到哪个办公区（把某个 Pod 部署到哪个 Node 上）。给指定的资源对象定义一个或多个不同的标签能够实现多维度的资源分组管理，方便进行资源分配、调度、配置、部署等管理工作。</p><h3 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h3><p>Label 选择器，K8S 通过 Selector 来过滤筛选指定的资源对象，类似于 SQL 语句中的 where 查询条件，Label 实现了简单又通用的对象查询机制。</p><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>在 K8S 中，Service 是对 Pod 对象的抽象，通常，Pod 会以多副本的形式部署，每个 Pod 都有自己的 IP，都可以对外提供服务，但 Pod 是脆弱的，也就是说，它随时都有可能被频繁地销毁和重启，IP 也会随之改变，这样，服务的访问就会出现问题。</p><p>Service 就是提出来解决这个问题的，它定义了一个虚拟 IP（也叫集群 IP），这个 IP 在 Service 的整个生命周期内都不会改变。当有访问到达时，Service 会将请求导向 Pod，如果存在多个 Pod，Service 还能实现负载均衡。</p><h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><p>K8S 的存储卷，定义了一个 Pod 中多个容器可访问的共享目录。和 Docker 的 Volume 不太一样的是，K8S 的 Volume 是以 Pod 为单位的，也就是 Volume 的生命周期和 Pod 相关，和 Pod 内的容器不相关，即使容器终止或重启，Volume 中的数据也不会丢失，只有当 Pod 被删除时，数据才会丢失。</p><h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><p>当有多个用户或租户使用同一个 K8S 集群时，如何区分它们创建的资源呢？答案就是 Namespace。</p><p>Namespace 将一个物理的集群从逻辑上划分成多个虚拟的集群，每个集群就是一个 Namespace，不同 Namespace 里的资源是完全隔离的。每个用户在自己创建的 Namespace 里操作，都不会影响到其他用户。</p><h3 id="Annotation"><a href="#Annotation" class="headerlink" title="Annotation"></a>Annotation</h3><p>Annotation 与 Label 类似，但和 Label 不同 的事，Annotation 不用于过滤筛选，它只是用户定义的某一种资源的附加信息，目的是方便外部查找该资源。有点类似于我们常说的别名，没有它完全可以，但有了它可以很方便查找。</p><p>最后，还是继续送书，容器网络专家倪朋飞写的《K8S 指南》电子书，如有需要后台回复“K8S”（之前回复过就不用回复了）。如需加群学习回复“加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 02 demo 初体验</title>
      <link href="/2018/08/31/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_02_demo_%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
      <url>/2018/08/31/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_02_demo_%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/4.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络！欢迎大家和我一起学习 K8S。</p><p>从前面的文章我们知道，Kubernetes 脱胎于 Google 的 Borg，Borg 在 Kubernetes 诞生之初已经在 Google 内部身经百战 10 余年，且不说它的历史源远流长，就凭它是出自 Google 那帮天才工程师之手，就知道它的学习难度不低。</p><a id="more"></a><p>对于这种有一定学习门槛的技术，最好的入门方式是先玩起来，如果刚开始就沉迷在那些理论中，很容易从入门到放弃。</p><p>可喜的是，Google 已经考虑到了这一点，官方文档提供了一个很小的 demo，麻雀虽小，五脏俱全，这个 demo 基本涵盖了 K8S 的基本概念，通过它，可以轻松构建一个 K8S 集群，玩转 K8S，我们现在就去玩一玩。（PS：下面提到的概念，我们后面会详细讨论，不理解可以暂时跳过）</p><p>打开：<br><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics" target="_blank" rel="noopener">https://kubernetes.io/docs/tutorials/kubernetes-basics</a>  </p><p>映入眼帘的是图文并茂的 6 个步骤： </p><center><img src="/images/k8s/k8sbasic.png" alt=""></center><ol><li>创建一个 K8S 集群</li><li>部署 APP</li><li>探索 APP</li><li>访问 APP</li><li>APP 弹性伸缩</li><li>更新 APP</li></ol><p>在开始每个步骤之前，先来了解个东西——minikube。顾名思义，这是一个迷你版的 K8S，一个轻量级的 K8S 实现，对于平常的学习体验，使用它可以达到和使用 K8S 一样的效果。它的部署方式足够简单，All-In-One，一个集群只有一个节点，K8S 所有组件都部署在这个节点上。</p><p>用户也可以使用 Web UI 和 minikube CLI 的方式来管理 K8S 集群，比如：启动，停止，删除，获取状态等。官方的 demo 就是使用 minikube CLI 来完成的。</p><p>话不多说，下面我们就开始体验下 K8S 之旅吧。</p><p><font color="red" size="5">第一步：</font><strong>创建一个 K8S 集群</strong>  </p><center><img src="/images/k8s/createcluster.png" alt=""></center><p>在交互界面输入 <code>minikube start</code> 就创建了一个 K8S 集群，这个集群创建在一台 VM 上，K8S 所有组件都跑在这台 VM 上。</p><p>接下来我们就可以使用 K8S 命令行工具 <code>kubectl</code> 来操作这个集群了。</p><p><code>kubectl version</code> 查看 K8S 的版本号：</p><center><img src="/images/k8s/kubectlversion.png" alt=""></center><p>看到两个 version，<code>client version</code> 指 kubectl 的 version，<code>server version</code> 就是 K8S 的 version。</p><p><code>kubectl get nodes</code> 获取集群节点数：</p><center><img src="/images/k8s/kubegetnodes.png" alt=""></center><p>可以看到这个 demo 只有一个节点，就是前面创建的 VM。<code>status</code> 是 <code>ready</code>，说明该节点准备好部署 APP 了。</p><p><font color="red" size="5">第二步：</font><strong>部署一个 APP</strong>  </p><p>执行命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl run kubernetes-bootcamp --images=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080</span><br></pre></td></tr></table></figure></p><p>就完成了一个 APP 的部署。</p><center><img src="/images/k8s/createdeployment.png" alt=""></center><p>其中，<code>--images</code> 指定 APP 的 Docker 镜像，<code>--port</code> 设置 APP 对外服务的端口，<code>kubectl run</code> 会下载镜像，然后创建 <code>deployment</code>，根据 deployment 创建 APP。deployment 就像是 APP 的说明书，它指导怎么创建 和维护 APP。APP 创建完运行在 Docker 容器中，使用 <code>kubectl get deployments</code> 可以查看 deployment 的信息。</p><p><font color="red" size="5">第三步：</font><strong>探索 APP</strong>  </p><p>上一步创建完 deployment，会接着创建 Pod 来运行 APP 容器，K8S 使用 Pod 来管理容器资源，一个 Pod 可以包含一个或多个容器，在这个例子，一个 Pod 就只有一个 APP 容器。使用 <code>kubectl get pods</code> 查看当前 Pod 信息。</p><center><img src="/images/k8s/getpods.png" alt=""></center><p>更详细信息使用 <code>kubectl describe pods</code> 查看。</p><p>kubectl 工具对于排错很有帮助，下面几个是较为常用的命令：</p><ul><li><strong>kubectl get</strong> - 列出资源</li><li><strong>kubectl describe</strong> - 显示资源的详细信息</li><li><strong>kubectl logs</strong> - 输出 Pod 中容器的日志</li><li><strong>kubectl exec</strong> - 在 Pod 容器中执行命令</li></ul><p><font color="red" size="5">第四步：</font><strong>访问 APP</strong>  </p><p>默认情况下，所有 Pod 都只能在集群内部访问，上面看到每个 Pod 有 IP 和端口，Pod 之间可以直接访问。外部想要访问 Pod， 需要将端口暴露出去，执行如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl expose deployment/kubernetes-bootcamp --type=&quot;NodePort&quot; --port 8080</span><br></pre></td></tr></table></figure></p><p>将容器的端口（8080）映射到节点的端口。</p><p>执行 <code>kubectl get services</code> 查看映射到的节点的端口。</p><center><img src="/images/k8s/service2.png" alt=""></center><p>可以看到容器的 8080 端口已经映射到节点的 31915 端口。外部可以通过 <code>NodeIP:Port</code> 的方式就可以访问到 Pod 内的容器，如下：</p><center><img src="/images/k8s/curservice.png" alt=""></center><p><code>service</code> 是 K8S 中对 Pod 的进一步抽象，是外部访问 Pod 的入口。如果把 K8S 集群想象成一个组织，那么 service 就是这个组织的接口人，为什么需要 service，这个留作后面的内容再讲，在这里你可以把它暂时理解成端口映射。  </p><p><font color="red" size="5">第五步：</font><strong>APP 的弹性伸缩</strong> </p><p>为了满足高可用，Pod 可以自动扩容和缩容。默认情况下，Pod 只会运行一个副本，这是由 deployment 定义的，可以通过 <code>kubectl get deployments</code> 查看副本数，通过 <code>kubectl scale deployments/app --replicas=num</code> 增加或减少副本数。 </p><p>比如，增加副本数到 4 个： </p><center><img src="/images/k8s/scaleup.png" alt=""></center><p>看到 Pod 数也增加到了 4 个。</p><p>减少副本数为 2 个：  </p><center><img src="/images/k8s/scale_down.png" alt=""></center><p>看到两个副本显示 Terminating，表示正在中止，过段时间再看就只有两个了。</p><p>对于多副本的情况，访问 APP 会实现负载均衡，如下：</p><center><img src="/images/k8s/scaleservice.png" alt=""></center><p>看到每次请求访问都落在不同的 Pod 上，这个功能是由 service 来完成的。</p><p><font color="red" size="5">第六步：</font><strong>更新 APP</strong>   </p><p>当前 APP 使用的镜像版本是 v1，需要升级到 v2，执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl set images deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2</span><br></pre></td></tr></table></figure><center><img src="/images/k8s/update.png" alt=""></center><p>看到升级过程是先中止之前的 4 个副本，再重开 4 个副本。</p><p>如果回退到 v1 版本，只用执行如下命令即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployments/kubernetes-bootcamp</span><br></pre></td></tr></table></figure><center><img src="/images/k8s/rollout.png" alt=""></center><p>至此，我们已经通过官方这个 demo 体验了一把 K8S 的功能和使用方法，下面我会陆陆续续把自己学习 K8S 的笔记整理出来，分享给你，希望对你有帮助。如有可能，请随手转发分享一下，让更多的人也参与进来。</p><p>最后，还是继续送书，容器网络专家倪朋飞写的《K8S 指南》电子书，如有需要后台回复“K8S”（之前回复过就不用回复了）。如需加群学习回复“加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 笔记 01 初识 Kubernetes 新时代的领航者</title>
      <link href="/2018/08/30/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_01_%E5%88%9D%E8%AF%86_Kubernetes_%E6%96%B0%E6%97%B6%E4%BB%A3%E7%9A%84%E9%A2%86%E8%88%AA%E8%80%85/"/>
      <url>/2018/08/30/tech/cloud/k8s/Kubernetes_%E7%AC%94%E8%AE%B0_01_%E5%88%9D%E8%AF%86_Kubernetes_%E6%96%B0%E6%97%B6%E4%BB%A3%E7%9A%84%E9%A2%86%E8%88%AA%E8%80%85/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/k8s/2.jpg" alt=""></center><p>Hi，大家好，我是 Linux云计算网络！欢迎大家和我一起学习 K8S。</p><p>大明王朝时期，明成祖朱棣为了发展海外贸易和建立自己的声望，派郑和七下西洋，创下了这段中国古代规模最大、船只最多（240多艘）、海员最多（2.7 万人）、时间最久的，比欧洲国家航海时间早半个多世纪的远洋航行壮举。</p><a id="more"></a><center><img src="/images/k8s/3.jpg" alt=""></center><p>Kubernetes 这个名字起源于古希腊，是「舵手」的意思，所以它的 Logo 既像一张渔网，又像一个罗盘。如果 Docker 把自己定位为驮着集装箱在大海上遨游的鲸鱼，那么 Kubernetes 就是掌舵大航海时代话语权的舵手，指挥着这条鲸鱼按照主人设定的路线巡游。</p><center><img src="/images/k8s/dockervsk8s.jpg" alt=""></center><p>Kubernetes 脱胎于 Google 老牌的集群管理软件「Borg」，虽然自诞生至今才三年多（第一个正式版本 Kubernetes 1.0 于 2015 年 7 月才正式发布），但要论其历史，却是早已在 Google 内部身经百战 10 余年，Kubernetes 站在 Borg 这个前辈的肩膀上，吸取了它过去十年间的经验和教训，才有了今天的成绩。这也是 Docker 火了之后，Google 迫不及待想推 Kubernetes 的原因。</p><p>Kubernetes 也常被人们称为「K8S」，原因是 K 和 S 之间正好有 8 个字母，读音上也和 8 相似，为了阅读方便，人们都乐于这么称呼。</p><p>有了 Google 的背书，K8S 一经推出就一鸣惊人，迅速称霸容器技术领域。</p><p>可以说，K8S 是以容器技术为基础打造的一个云计算时代的全新分布式系统架构，它的架构设计开放，除了支持 Docker，还支持其他容器技术，比如 Rocket、RKT 等。</p><p>Google 的加持，开放的生态，让它在与其他竞争对手的 博弈中占据上风，轻松拿下容器编排这个市场。</p><p>2017年9月，Mesosphere 宣布支持 K8S，接着，10月，Docker 在 DockerCon EU 2017 大会上也宣布拥抱 K8S，至此，K8S 正式霸占容器技术领域，彻底掌控容器技术的未来。</p><center><img src="/images/k8s/vsplat.jpg" alt=""></center><p>K8S 为了扩大影响力，推出没多久就加入 OpenStack 阵营，目的是希望 K8S 能被 OpenStack 生态圈所容纳，与 KVM 虚拟机一样成为 OpenStack 平台上的一等公民。</p><blockquote><p>这意味着以容器为代表的应用形态和以虚拟机为代表的系统形态将完美融合于 OpenStack 之上，并与软件定义网络和软件定义存储一起统治下一代数据中心。</p></blockquote><p>K8S 在云计算领域刮起了一道强劲之风，但凡跟云计算相关的公司都无法无视它的存在，错过它，也许就错过了未来。我们来看看它从诞生至今的 Google 趋势（和 Docker Swarm 和 Mesos 进行了对比）：</p><center><img src="/images/k8s/k8strend.jpg" alt=""></center><p>可以看到，K8S 从诞生之初便一路飙升，将对手甩开了十几条街，未来也将会以火箭的速度保持上升。</p><p>目前，除了云计算相关的公司，很多互联网公司、甚至传统企业都在纷纷布局自家的 K8S 产品，可以说，K8S 是当前容器行业最炙手可热的明星。</p><p>作为一个 IT 从业人员，你无法忽视它的存在。谁能比别人领先一步掌握新技术，谁就能在竞争中赢得了先机。</p><p>虽然说，现在学习 K8S 并不是最佳时机，但还不算太晚，就像一句话说的：</p><blockquote><p>学习一门技术最好的时间是 10 年前，其次是现在。</p></blockquote><p>后面我会推一个我学习 K8S 的笔记教程，一方面是加深自己对知识的理解，另一方面也是希望能分享给有需要的人。分享是一种美德，你在看到我的分享的同时，也希望你能动动手指把它分享给你的朋友，这样我的分享也没有白费。</p><p>最后，我这里有一份 《K8S 指南》，这是容器网络专家倪朋飞利用自己业余时间写的一本小册子，质量还是挺不错的，有需要的后台回复“K8S”。另外需要加群学习的后台回复“加群”。</p><blockquote><p>文中图片来源于网络，侵权必删<br>参考：k8s 指南</p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 新时代的宠儿</title>
      <link href="/2018/08/28/tech/cloud/k8s/Kubernetes_%E6%96%B0%E6%97%B6%E4%BB%A3%E7%9A%84%E5%AE%A0%E5%84%BF/"/>
      <url>/2018/08/28/tech/cloud/k8s/Kubernetes_%E6%96%B0%E6%97%B6%E4%BB%A3%E7%9A%84%E5%AE%A0%E5%84%BF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="Kubernetes-是什么"><a href="#Kubernetes-是什么" class="headerlink" title="Kubernetes 是什么"></a>Kubernetes 是什么</h3><hr><p>Kubernetes 简称为 K8S。简单说，K8S 是一个用于容器集群的分布式系统架构。首先，它是基于容器技术，容器是和虚拟机并列的一种虚拟化技术，相比虚拟机来说，容器更加轻量，资源利用率更高，更适合于云原生应用。</p><a id="more"></a><p>其次，K8S 掌管的是容器集群，就像它的名字一样，一个舵手指挥着一个个的集装箱航行。容器会被频繁地销毁、重建和调度，为了最大化地利用集群资源和减少人力成本，K8S 在其中以高效的策略，自动化的运维方式指挥着这一切，就像一台永动机一样，管理员可以一劳永逸。</p><center><img src="/images/k8s/k8swhat.jpg" alt=""></center><p>最后，K8S 的架构非常开放，分布式的组件结构，使得它可以轻松地适应大规模的集群环境，Google 庞大的数据中心就是它最好的历练。</p><h3 id="为什么-K8S-能赢？"><a href="#为什么-K8S-能赢？" class="headerlink" title="为什么 K8S 能赢？"></a>为什么 K8S 能赢？</h3><hr><p>随着 2014 年 Docker 大火之后，已经涌现出大量的容器集群管理平台，其中，Docker 自家的 Swarm，在 Twitter 内部久经考验的 Mesos，以及 Google 的 K8S 最为知名，号称容器编排三驾马车。下图是三家的热度走势图：</p><center><img src="/images/k8s/k8strend.jpg" alt=""></center><p>K8S 自诞生日起便一骑绝尘，甩对手十几条街。为什么 K8S 能赢？我自以为是生态。</p><p>K8S 架构开放，向下可以容纳各种 container runtime，便不是没了 Docker 不行。向上可以承载各种 PaaS 平台，还能和 OpenStack、VMware 这些 IaaS 平台和平相处。它由此组建的生态系统，随随便便可以吃下任何一个平台。再加上 Google 的加持，谁能不爱？</p><h3 id="有哪些公司在使用-K8S？"><a href="#有哪些公司在使用-K8S？" class="headerlink" title="有哪些公司在使用 K8S？"></a>有哪些公司在使用 K8S？</h3><hr><p>据不完全统计，除了 AWS、Azure、Google、Microsoft 等巨头在容器领域里多年的博弈外，国内的很多互联网公司，如 BAT、蚂蚁、今日头条、滴滴等技术大厂，也都将容器和 K8S 列入未来的战略重心，无数中小型企业也正走在容器化的道路上。</p><p>从长远角度来看，K8S 将会成为企业服务器端技术栈标准中的一环，并连同它所推崇的容器化理念，成为广大后端技术人员和开发者的一门必修课。</p><h3 id="怎么学-K8S？"><a href="#怎么学-K8S？" class="headerlink" title="怎么学 K8S？"></a>怎么学 K8S？</h3><hr><p>现在快餐时代，如何学习才能更高效？我觉得排在第一位的应该是站在巨人的肩膀上学习。国内有很多研究 K8S 的大牛，其中一批是浙江大学研究所的研究员，他们出了国内第一本深入解读 Docker 和 K8S 原理的书《容器与容器云》。</p><p>看书虽然效果是奇好的，但效率并不高，想要效率高，我觉得学习大牛的知识总结可能才是最有效的。</p><p>正巧，今天我看到那批研究员中的一位作者张磊（现在在微软研究院）在极客时间开了一个 K8S 专栏，我觉得是雨后逢甘露，第一时间就买了。</p><p>这里简单给大家介绍下，有需要的朋友一定不要错过。</p><p>课程有 51 节，原价 99 元，现在优惠 68 元，9月8日恢复原价，如果你扫我下面的二维码买的话，我<font color="red">返你 8 元</font>（注意：这个是我特地给你的福利，别的地方是没有的），也就是说，你只用<font color="red"> 60 元</font>就可以买了，每节课 1 块多一点。另外，你买了之后还可以以同样的方式分享给你的好友，双方都受益。如果你讨厌这样的方式，那么忽略就好。</p><p>我作为一名云计算爱好者，一方面是希望为前辈们宣传一波，另一方面我也是给我的读者们尽量争取一些实打实的福利。希望能帮助到你。</p><p>大家买了之后记得加我微信哈，我返你钱。</p><p>另外，K8S 是用 Go 语言写的，我推荐大家和 Go 一起学效果最好，大家可以看我之前写的这篇文章学习 Go 语言最好的时间是 10 年前，其次是现在。</p><p>下面还有一个目录，大家可以看看，真的很良心。</p><center><img src="/images/k8s/k8scourse.jpg" alt=""></center><p>课程目录：</p><center><img src="/images/k8s/k8sdir.jpg" alt=""></center><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集群：孙悟空分身术</title>
      <link href="/2018/08/23/tech/cloud/k8s/%E9%9B%86%E7%BE%A4%EF%BC%9A%E5%AD%99%E6%82%9F%E7%A9%BA%E5%88%86%E8%BA%AB%E6%9C%AF/"/>
      <url>/2018/08/23/tech/cloud/k8s/%E9%9B%86%E7%BE%A4%EF%BC%9A%E5%AD%99%E6%82%9F%E7%A9%BA%E5%88%86%E8%BA%AB%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>在孙悟空的七十二变中，我觉得最厉害的非分身能力莫属，这也是他百试不得其爽的终极大招，每每都能打得妖怪摸不着北。</p><a id="more"></a><center><img src="/images/k8s/mon.jpg" alt=""></center><p>集群，学名叫 Cluster，可以翻译为簇、聚类、集群等多种意思，不同的翻译，在技术世界里所表示的意思都不尽相同，但都有一个共同的指向，即群体。集群就是由一组计算机所组成的实体，通常作为一个整体向用户提供资源。<br>集群的研究和发展离不开人们对高性能计算的追求，像我们熟悉的向量机、对称多处理机、工作站、超级计算机等等都是对高性能计算追求下的产物。</p><p>这些系统要么是提高 CPU 的主频和总线带宽来提高系统性能，要么是增加 CPU 个数和内存容量来提高性能，但这些手段对性能的提高都是有限的。有人做过实验，当 CPU 个数超过某一阈值时，系统的性能反而会变差。其主要的瓶颈就在于 CPU 访问内存的带宽并不能随着 CPU 个数的增加而有效增加。</p><center><img src="/images/k8s/clusterperf.jpg" alt=""></center><p>相反，集群系统的性能可扩展能力是线性增长的。我们可以简单通过增加机器数来增加集群的运算能力，相比购买高性能的大型计算机，同等运算能力下，我们可以获得更高的性价比。同时，系统的可靠性也得到了增强。</p><h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><hr><p>早在七十年代计算机厂商和研究机构就开始了对集群系统的研究和开发，首先创造性发明集群的是 Seymour Cray（西摩·克雷）—— 超级计算机之父。</p><center><img src="/images/k8s/cray.jpg" alt=""></center><p>Seymour 是一位美国工程师，在 1960 年代，CDC 公司涉足高性能计算领域，彼时还是大型机的天下，这些大型机设计非常复杂，生产周期漫长，价格还非常昂贵。当时在 CDC 公司担任总设计师的 Seymour 决心建造出心目中的高性能计算机。</p><p>Seymour 出于工程师的直觉，很快想到并行是提高计算机性能的有效方式。他使用廉价的方式来获得跟大型机一样的运算能力。他将多个普通的处理器连接起来，使它们能够协同工作，这就是高性能计算机的原型。</p><p>后来，IBM、HP 等公司学习了 Seymour 的这套架构，高性能计算机开始迅速推广，逐步取代原有的大型机。高性能计算机为当时的登月计划等大型科研项目作出了非常重要的贡献。</p><p>然后进入八十年代，在摩尔定律的指导下，CPU 频率不断提高，芯片不断降价，个人计算机强势崛起。苹果、微软等公司借助这股东风成为个人计算机时代的王者。随之而来的就是高性能计算机市场遭到了吞噬，被迫只能退守公司服务器市场。</p><p>但很快，随着互联网的普及，高性能计算机又迎来新的一波热潮。互联网上用户量庞大，普通 PC 难以应付如此众多的网络请求，必须要依赖由高性能计算机组成的服务器集群。在 2000 年左右的网络泡沫时期，成就了很多像 Sun 这样的服务器生产商。</p><p>如今，IT 行业向云计算冲击，诸如 Google、Apple、Amazon 等很巨头纷纷建立起了自己的数据中心。集群的规模在不断扩大，为海量的数据提高基础设施提供了支撑。根据不同的应用场景，集群也演变出多种形态，比如高性能集群、高可用集群、负载均衡集群等等。</p><h3 id="集群元素"><a href="#集群元素" class="headerlink" title="集群元素"></a>集群元素</h3><hr><p>集群不是简单的硬件堆叠，而是硬件和软件的结合。从软件上说，集群至少需要：</p><p>构建于 TCP/IP 协议上的通信软件，用于集群中节点之间的通信。</p><p>一套中心管理软件，用于统一管理集群中节点的资源、任务和容错等等。</p><p>这两点比较好理解，集群的规模往往是比较庞大的，对于管理员来说，需要随时能够知晓集群中各节点的业务正常与否，出问题了应该怎么保证业务能够不中断，遇到流量高峰和低谷的时候，又该怎么响应，这些操作如果纯靠人工来完成那必将很惨烈。依靠软件和网络来完成自动化的管理方式，可以将管理员解放出来。当然，以上说的两点是比较宽泛的，用户可以根据自身需求来部署不同的集群元素。</p><p>一个比较经典的集群模型当属 Beowulf 集群，它通过一个节点统一将来自网络的请求分配给各个节点进行计算处理。</p><center><img src="/images/k8s/beowulf.jpg" alt=""></center><h3 id="集群与分布式"><a href="#集群与分布式" class="headerlink" title="集群与分布式"></a>集群与分布式</h3><hr><p>集群与分布式像一对孪生兄弟，傻傻分不清楚。在我看来，它们之间没有特别明确的分界线，集群离不开分布式，分布式也需要集群。如果一定要做个区分，可以套用一个比喻来描述两者的区别：</p><p>一家餐厅刚开业，由于成本限制招了一个厨师，慢慢地，餐厅生意越做越好，一个厨师已经很难应付过来，于是又招了一个，这两个厨师水平相当，都能做同样的事，两个厨师之间的关系就是集群。两厨师除了炒菜，还要负责洗菜、配菜等等的活，工作负荷已经严重超标，为了让厨师能专心炒菜，把菜做到极致，餐厅又招了配菜师来辅助厨师，厨师和配菜师之间的关系就是分布式。</p><p>这个例子比较形象，在网站开发中也有类似的关系，两个全栈工程师之间就是集群的关系，前端工程师和后端工程师之间就属于分布式的关系。</p><center><img src="/images/k8s/cluint.jpg" alt=""></center><p>@知乎大闲人柴毛毛</p><p>所以，一定要有区分的话就是：集群是一个业务部署在多个服务器上，而分布式是一个业务拆分成多个子业务部署在不同的服务器上。但在实际部署中，为了高性能，需要分布式部署，为了高可用，需要集群部署，这两者都是业务所必须的指标。所以，集群和分布式之间的关系是相互补充的。</p><h3 id="虚拟化"><a href="#虚拟化" class="headerlink" title="虚拟化"></a>虚拟化</h3><hr><p>随着虚拟化技术的发展，一台服务器可以虚拟出多个虚拟机，对外提供业务，这种方式大大提高了资源的利用率，集群的部署也逐步从物理机过渡到虚拟机，灵活性大大提高。但同时也带来了更多新的研究课题。虚拟化计算、虚拟化存储、虚拟化网络、虚拟化安全等等这些课题共同推动着云计算产业迈出一个又一个的台阶。</p><h3 id="数据中心"><a href="#数据中心" class="headerlink" title="数据中心"></a>数据中心</h3><hr><p>数据中心是集中存放和运行服务器的地方，是规模最大的集群。随着云计算和大数据概念的风起云涌，Google、Amazon 等这些明星公司幕后的数据中心也开始走入大众的视野。数据中心要求有优秀的架构设计、电路设计、空间设计等等，还要有机制能够应对各种各样的意外，否则一点小小的失误，公司的股价恐怕就要跳水。</p><p>地理位置的选择也是数据中心考虑的一个指标，随着绿色数据中心概念的兴起，越来越多人关注数据中心所带来的能源问题和环境问题，选择一个远离市区，并且能利用天然水源和气温的地方，将会为数据中心的建设节约大量的成本。Google 等大公司的数据中心就有意放在高纬度、高海拔的地区，以及有湖泊、河流流经地区，以享受天然的空调和冷却水。</p><center><img src="/images/k8s/datacenter.jpg" alt=""></center><p>云计算之所以能被称之为“云”计算，是因为具有体量庞大的资源池，集群就是用来构建这个资源池的，说集群是云计算的基石一点都不为过。目前有很多集群管理软件，大家比较熟悉的像 Mesos，k8s 都属于这个范畴，后面会带来相关的干货，大家尽情期待。</p><blockquote><p>PS：文中图片均来自于网络，侵权必删<br>参考：<a href="http://dwz.cn/h9lwjvOR" target="_blank" rel="noopener">http://dwz.cn/h9lwjvOR</a></p></blockquote><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> 集群 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>再谈云计算技能图谱</title>
      <link href="/2018/08/15/tech/cloud/%E5%86%8D%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1/"/>
      <url>/2018/08/15/tech/cloud/%E5%86%8D%E8%B0%88%E4%BA%91%E8%AE%A1%E7%AE%97%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>之前，我写过一篇「云计算技能图谱」的文章，涵盖了云计算领域绝大部分的分支，很多人看了表示不淡定了——学完这个要等到猴年马月！</p><p>其实那份图谱涉及到很多应用场景，比如说大数据，机器学习，这些是基于云计算引申的技术分支，底层用的是云计算的基础设施，但要说可不可以独立于云计算来做，可以，只是一个规模的问题罢了。</p><p>为了能给很多初学者一个好的引导，我重新整理了这份图谱，把一些相关联的技术分支去掉了，只保留了基础设施部分（包括计算、存储、网络、安全这几个部分）。如下：</p><center><img src="/images/cloud/cloud_tupu.jpg" alt=""></center><p>备注：图片为防抄袭迫不得已加水印，想要原图的可以加我微信私信我</p><p>这样来看，就显得精简多了。</p><p>可能你看到这个还是会很焦虑，其实大可不必焦虑，图谱更多告诉你的是这个领域有什么，至于做不做完全根据你自己的情况选择，比如你想做个 T 型人才 ，那就尽可能去学，想做个 I 型人才，那就专注在某一个领域就好了。这两者没有绝对的孰是孰非，最终都是要解决问题。就像一句话说的，不管黑猫白猫，能捉到老鼠的就是好猫。</p><p>我知道关注我的读者当中，什么人才都有，我目前知道的，有学生，有工作了好几年的老司机，也有博士，首先要感谢大家的关注，我相信大家关注我肯定是因为我的哪一篇文章触动了你或者对你有帮助才会关注的。</p><p>我想说，大家关注我肯定是没错的，我这个号专注的内容就是上面这份图谱提到的内容，你可以在这里看到最基础的开发实践内容（比如 Linux、C/C++、Python、Go 技术栈），也可以看到云计算框架的解读（比如 KVM，OpenStack，Docker，Kubernetes），还可以看到最前沿技术的探讨。当然也有一些非技术的内容，比如行业资讯，以及我一些不吐不快的碎碎念。</p><p>其实我进入这个领域也不算早，跟很多读者比起来，是不折不扣的菜鸟，但正因为我是菜鸟，我写出的文章才会通俗易懂，因为我要保证和我一样的菜鸟能听得懂，当然了，质量肯定是第一位的，你们要是看过我以前写的一些文章就知道质量如何了，绝对是很良心的分享。说这个主要是希望大家能多多向你身边的朋友推荐下我这个号，有更多的朋友加入，我的写作动力就越强，就能给你们输出更多更好的文章。</p><p>为了能让大家有一个交流的氛围，我建了一个群，想加入的可以后台回复“加群”。</p><p>另外，我这里还收藏了一套很有价值的技能图谱，包括上面说的很多细分领域，比如 Python、Docker、Kubernetes、DevOps，还有一些其他的分支，比如机器学习，大数据，架构师，运维，嵌入式等等等等，大概就像下面这样子：</p><center><img src="/images/cloud/cloud_tupuinfo.jpg" alt=""></center><p>这些资料是我精心为大家整理的，整理不易，大家如果需要，有一点点要求，只要你乐于分享即可。这里要说明一点，我觉得好的东西，就是要让更多的人看到，你可以说是诱导你分享，但扪心自问，遇到好的东西谁又不乐意分享呢，让你的朋友看到你分享好东西给他们又何尝不是一种快乐呢。</p><p>获取技能图谱方法：</p><ol><li>转发本文到你的朋友圈；</li><li>添加我的微信号：aLinux云计算网络，或长按下方二维码加我微信；</li><li>加好友后发朋友圈截图给我，我看到会发一整套技能图谱给你，或者你也可以回复“加群”，我拉你进我的技术交流群。</li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 01 云计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 技能图谱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux探秘之用户态与内核态</title>
      <link href="/2018/07/06/tech/linux/Linux%E6%8E%A2%E7%A7%98%E4%B9%8B%E7%94%A8%E6%88%B7%E6%80%81%E4%B8%8E%E5%86%85%E6%A0%B8%E6%80%81/"/>
      <url>/2018/07/06/tech/linux/Linux%E6%8E%A2%E7%A7%98%E4%B9%8B%E7%94%A8%E6%88%B7%E6%80%81%E4%B8%8E%E5%86%85%E6%A0%B8%E6%80%81/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="Unix-Linux的体系架构"><a href="#Unix-Linux的体系架构" class="headerlink" title="Unix/Linux的体系架构"></a>Unix/Linux的体系架构</h3><hr><p>如下图所示，从宏观上来看，Linux 操作系统的体系架构分为用户态和内核态（或者用户空间和内核）。</p><center><img src="/images/linux/linuxlevel.jpg" alt=""></center><p>内核从本质上看是一种软件——控制计算机的硬件资源，并提供上层应用程序运行的环境。用户态即上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源，包括 CPU 资源、存储资源、I/O 资源等。为了使上层应用能够访问到这些资源，内核必须为上层应用提供访问的接口：即系统调用。</p><p>系统调用是操作系统的最小功能单位，这些系统调用根据不同的应用场景可以进行扩展和裁剪，现在各种版本的 Unix 实现都提供了不同数量的系统调用，如 Linux 的不同版本提供了 240-260 个系统调用，FreeBSD 大约提供了 320 个（reference：UNIX 环境高级编程）。</p><p>我们可以把系统调用看成是一种不能再化简的操作（类似于原子操作，但是不同概念），有人把它比作一个汉字的一个“笔画”，而一个“汉字”就代表一个上层应用，我觉得这个比喻非常贴切。因此，有时候如果要实现一个完整的汉字（给某个变量分配内存空间），就必须调用很多的系统调用。如果从实现者（程序员）的角度来看，这势必会加重程序员的负担，良好的程序设计方法是：重视上层的业务逻辑操作，而尽可能避免底层复杂的实现细节。</p><p>库函数正是为了将程序员从复杂的细节中解脱出来而提出的一种有效方法。它实现对系统调用的封装，将简单的业务逻辑接口呈现给用户，方便用户调用，从这个角度上看，库函数就像是组成汉字的“偏旁”。这样的一种组成方式极大增强了程序设计的灵活性，对于简单的操作，我们可以直接调用系统调用来访问资源，如“人”，对于复杂操作，我们借助于库函数来实现，如“仁”。显然，这样的库函数依据不同的标准也可以有不同的实现版本，如ISO C 标准库，POSIX 标准库等。</p><p>Shell 是一个特殊的应用程序，俗称命令行，本质上是一个命令解释器，它下通系统调用，上通各种应用，通常充当着一种“胶水”的角色，来连接各个小功能程序，让不同程序能够以一个清晰的接口协同工作，从而增强各个程序的功能。</p><p>同时，Shell 是可编程的，它可以执行符合 Shell 语法的文本，这样的文本称为 Shell 脚本，通常短短的几行 Shell 脚本就可以实现一个非常大的功能，原因就是这些 Shell 语句通常都对系统调用做了一层封装。为了方便用户和系统交互，一般，一个 Shell 对应一个终端，终端是一个硬件设备，呈现给用户的是一个图形化窗口。我们可以通过这个窗口输入或者输出文本。这个文本直接传递给 Shell 进行分析解释，然后执行。</p><p>总结一下，用户态的应用程序可以通过三种方式来访问内核态的资源：</p><ul><li>系统调用</li><li>库函数</li><li>Shell 脚本</li></ul><p>下图是对上图的一个细分结构，从这个图上可以更进一步对内核所做的事有一个“全景式”的印象。主要表现为：向下控制硬件资源，向内管理操作系统资源：包括进程的调度和管理、内存的管理、文件系统的管理、设备驱动程序的管理以及网络资源的管理，向上则向应用程序提供系统调用的接口。</p><center><img src="/images/linux/linuxarch.jpg" alt=""></center><p>从整体上来看，整个操作系统分为两层：用户态和内核态，这种分层的架构极大地提高了资源管理的可扩展性和灵活性，而且方便用户对资源的调用和集中式的管理，带来一定的安全性。</p><h3 id="用户态和内核态的切换"><a href="#用户态和内核态的切换" class="headerlink" title="用户态和内核态的切换"></a>用户态和内核态的切换</h3><hr><p>因为操作系统的资源是有限的，如果访问资源的操作过多，必然会消耗过多的资源，而且如果不对这些操作加以区分，很可能造成资源访问的冲突。</p><p>所以，为了减少有限资源的访问和使用冲突，Unix/Linux 的设计哲学之一就是：对不同的操作赋予不同的执行等级，就是所谓特权的概念。简单说就是有多大能力做多大的事，与系统相关的一些特别关键的操作必须由最高特权的程序来完成。Intel 的 X86 架构的 CPU 提供了 0 到 3 四个特权级，数字越小，特权越高。</p><p>Linux 操作系统中主要采用了 0 和 3 两个特权级，分别对应的就是内核态和用户态。运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制。</p><p>很多程序开始时运行于用户态，但在执行的过程中，一些操作需要在内核权限下才能执行，这就涉及到一个从用户态切换到内核态的过程。比如C函数库中的内存分配函数 malloc()，它具体是使用 sbrk() 系统调用来分配内存，当malloc() 调用 sbrk() 的时候就涉及一次从用户态到内核态的切换，类似的函数还有 printf()，调用的是 wirte() 系统调用来输出字符串，等等。</p><center><img src="/images/linux/linuxchg.jpg" alt=""></center><p>那到底在什么情况下会发生从用户态到内核态的切换，一般存在以下三种情况：</p><ol><li>当然就是系统调用：原因如上的分析。</li><li>异常事件： 当 CPU 正在执行运行在用户态的程序时，突然发生某些预先不可知的异常事件，这个时候就会触发从当前用户态执行的进程转向内核态执行相关的异常事件，典型的如缺页异常。</li><li>外围设备的中断：当外围设备完成用户的请求操作后，会向 CPU 发出中断信号，此时，CPU 就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换。</li></ol><p><strong>注意：</strong> 系统调用的本质其实也是中断，相对于外围设备的硬中断，这种中断称为软中断，这是操作系统为用户特别开放的一种中断，如 Linux int 80h 中断。所以，从触发方式和效果上来看，这三种切换方式是完全一样的，都相当于是执行了一个中断响应的过程。但是从触发的对象来看，系统调用是进程主动请求切换的，而异常和硬中断则是被动的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>本文仅是从宏观的角度去理解 Linux 用户态和内核态的设计，并没有去深究它们的具体实现方式。从实现上来看，必须要考虑到的一点我想就是性能问题，因为用户态和内核态之间的切换会消耗大量资源。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文掌握 Linux 性能分析之网络篇（续）</title>
      <link href="/2018/06/15/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1_Linux_%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87%EF%BC%88%E7%BB%AD%EF%BC%89/"/>
      <url>/2018/06/15/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1_Linux_%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87%EF%BC%88%E7%BB%AD%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>这是 Linux 性能分析系列的第五篇，前四篇在这里：<br>一文掌握 Linux 性能分析之 CPU 篇<br>一文掌握 Linux 性能分析之内存篇<br>一文掌握 Linux 性能分析之 IO 篇<br>一文掌握 Linux 性能分析之网络篇</p><p>在上篇中，我们已经介绍了几个 Linux 网络方向的性能分析工具，本文再补充几个。总结下来，余下的工具包括但不限于以下几个：</p><ul><li>sar：统计信息历史</li><li>traceroute：测试网络路由</li><li>dtrace：TCP/IP 栈跟踪</li><li>iperf / netperf / netserver：网络性能测试工具</li><li>perf 性能分析神器</li></ul><p>由于篇幅有限，本文会先介绍前面两个，其他工具留作后面介绍，大家可以持续关注。</p><h4 id="sar"><a href="#sar" class="headerlink" title="sar"></a><strong>sar</strong></h4><p>sar 是一个系统历史数据统计工具。统计的信息非常全，包括 CPU、内存、磁盘 I/O、网络、进程、系统调用等等信息，是一个集大成的工具，非常强大。在 Linux 系统上 <code>sar --help</code> 一下，可以看到它的完整用法。</p><ul><li>-A：所有报告的总和</li><li>-u：输出 CPU 使用情况的统计信息</li><li>-v：输出 inode、文件和其他内核表的统计信息</li><li>-d：输出每一个块设备的活动信息</li><li>-r：输出内存和交换空间的统计信息</li><li>-b：显示 I/O和传送速率的统计信息</li><li>-a：文件读写情况</li><li>-c：输出进程统计信息，每秒创建的进程数</li><li>-R：输出内存页面的统计信息</li><li>-y：终端设备活动情况</li><li>-w：输出系统交换活动信息</li><li>-n：输出网络设备统计信息</li></ul><p>在平时使用中，我们常常用来分析网络状况，其他几项的通常有更好的工具来分析。所以，本文会重点介绍 sar 在网络方面的分析手法。</p><p>Linux 系统用以下几个选项提供网络统计信息：</p><ul><li>-n DEV：网络接口统计信息。</li><li>-n EDEV：网络接口错误。</li><li>-n IP：IP 数据报统计信息。</li><li>-n EIP：IP 错误统计信息。</li><li>-n TCP：TCP 统计信息。</li><li>-n ETCP：TCP 错误统计信息。</li><li>-n SOCK：套接字使用。</li></ul><p>我们来看几个示例：</p><p><strong>（1）每秒打印 TCP 的统计信息：</strong></p><p><code>sar -n TCP 1</code></p><center><img src="/images/linux/sarn.png" alt=""></center><p>几个参数了解一下：</p><ul><li>active/s：新的 TCP 主动连接（也就是 socket 中的 connect() 事件），单位是：连接数/s。</li><li>passive/s：新的 TCP 被动连接（也就是 socket 中的 listen() 事件）。</li><li>iseg/s：接收的段（传输层以段为传输单位），单位是：段/s</li><li>oseg/s：发送的段。<br>通过这几个参数，我们基本可以知道当前系统 TCP 连接的负载情况。</li></ul><p><strong>（2）每秒打印感兴趣的网卡的统计信息：</strong></p><p><code>sar -n DEV 1 | awk &#39;NR == 3 || $3 == &quot;eth0&quot;&#39;</code></p><center><img src="/images/linux/sarnd.png" alt=""></center><p>几个参数了解一下：</p><ul><li>rxpck/s / txpck/s：网卡接收/发送的数据包，单位是：数据包/s。</li><li>rxkB/s / txkB/s：网卡接收/发送的千字节，单位是：千字节/s。</li><li>rxcmp/s / txcmp/s：网卡每秒接受/发送的压缩数据包，单位是：数据包/s。</li><li>rxmcst/s：每秒接收的多播数据包，单位是：数据包/s。</li><li>%ifutil：网络接口的利用率。<br>这几个参数对于分析网卡接收和发送的网络吞吐量很有帮助。</li></ul><p><strong>（3）错误包和丢包情况分析：</strong></p><p><code>sar -n EDEV 1</code></p><center><img src="/images/linux/sarne.png" alt=""></center><p>几个参数了解一下：</p><ul><li>rxerr/s / txerr/s：每秒钟接收/发送的坏数据包</li><li>coll/s：每秒冲突数</li><li>rxdrop/s：因为缓冲充满，每秒钟丢弃的已接收数据包数</li><li>txdrop/s：因为缓冲充满，每秒钟丢弃的已发送数据包数</li><li>txcarr/s：发送数据包时，每秒载波错误数</li><li>rxfram/s：每秒接收数据包的帧对齐错误数</li><li>rxfifo/s / txfifo/s：接收/发送的数据包每秒 FIFO 过速的错误数</li></ul><p>当发现接口传输数据包有问题时，查看以上参数能够让我们快速判断具体是出的什么问题。</p><p>OK，这个工具就介绍到这里，以上只是抛砖引玉，更多技巧还需要大家动手去探索，只有动手，才能融会贯通。</p><h4 id="traceroute"><a href="#traceroute" class="headerlink" title="traceroute"></a><strong>traceroute</strong></h4><p>traceroute 也是一个排查网络问题的好工具，它能显示数据包到达目标主机所经过的路径（路由器或网关的 IP 地址）。如果发现网络不通，我们可以通过这个命令来进一步判断是主机的问题还是网关的问题。</p><p>它通过向源主机和目标主机之间的设备发送一系列的探测数据包（UDP 或者 ICMP）来发现设备的存在，实现上利用了递增每一个包的 TTL 时间，来探测最终的目标主机。比如开始 TTL = 1，当到达第一个网关设备的时候，TTL - 1，当 TTL = 0 导致网关响应一个 ICMP 超时报文，这样，如果没有防火墙拦截的话，源主机就知道网关设备的地址。以此类推，逐步增加 TTL 时间，就可以探测到目标主机之间所经过的路径。</p><p>为了防止发送和响应过程出现问题导致丢包，traceroute 默认会发送 3 个探测包，我们可以用 -q x 来改变探测的数量。如果中间设备设置了防火墙限制，会导致源主机收不到响应包，就会显示 * 号。如下是 <code>traceroute baidu</code> 的结果：</p><center><img src="/images/linux/traceroute.png" alt=""></center><p>每一行默认会显示设备名称（IP 地址）和对应的响应时间。发送多少个探测包，就显示多少个。如果只想显示 IP 地址可以用 -n 参数，这个参数可以避免 DNS 域名解析，加快响应时间。</p><p>和这个工具类似的还有一个工具叫 pathchar，但平时用的不多，我就不介绍了。<br>以上就是两个工具的简单介绍，工具虽然简单，但只要能解决问题，就是好工具。当然，性能分析不仅仅依靠工具就能解决的，更多需要我们多思考、多动手、多总结，逐步培养自己的系统能力，才能融会贯通。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> 性能分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文掌握 Linux 性能分析之网络篇</title>
      <link href="/2018/06/01/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1_Linux_%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87/"/>
      <url>/2018/06/01/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1_Linux_%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8B%E7%BD%91%E7%BB%9C%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>这是 Linux 性能分析系列的第四篇。</p><p>比较宽泛地讲，网络方向的性能分析既包括主机测的网络配置查看、监控，又包括网络链路上的包转发时延、吞吐量、带宽等指标分析。包括但不限于以下分析工具：</p><ul><li>ping：测试网络连通性</li><li>ifconfig：接口配置</li><li>ip：网络接口统计信息</li><li>netsat：多种网络栈和接口统计信息</li><li>ifstat：接口网络流量监控工具</li><li>netcat：快速构建网络连接</li><li>tcpdump：抓包工具</li><li>sar：统计信息历史</li><li>traceroute：测试网络路由</li><li>pathchar：确定网络路径特征</li><li>dtrace：TCP/IP 栈跟踪</li><li>iperf / netperf / netserver：网络性能测试工具</li><li>perf ：性能分析神器</li></ul><p>本文先来看前面 7 个。</p><h4 id="ping"><a href="#ping" class="headerlink" title="ping"></a><strong>ping</strong></h4><hr><p>ping 发送 ICMP echo 数据包来探测网络的连通性，除了能直观地看出网络的连通状况外，还能获得本次连接的往返时间（RTT 时间），丢包情况，以及访问的域名所对应的 IP 地址（使用 DNS 域名解析），比如：</p><center><img src="/images/linux/ping.jpg" alt=""></center><p>我们 <code>ping baidu.com，-c</code>参数指定发包数。可以看到，解析到了 baidu 的一台服务器 IP 地址为 220.181.112.244。RTT 时间的最小、平均、最大和算术平均差分别是 40.732ms、40.762ms、40.791ms 和 0.248。</p><h3 id="ifconfig"><a href="#ifconfig" class="headerlink" title="ifconfig"></a><strong>ifconfig</strong></h3><hr><p>ifconfig 命令被用于配置和显示 Linux 内核中网络接口的统计信息。通过这些统计信息，我们也能够进行一定的网络性能调优。</p><p><strong>1）ifconfig 显示网络接口配置信息</strong></p><center><img src="/images/linux/ifconfig.jpg" alt=""></center><p>其中，RX/TX packets 是对接收/发送数据包的情况统计，包括错误的包，丢掉多少包等。RX/TX bytes 是接收/发送数据字节数统计。其余还有很多参数，就不一一述说了，性能调优时可以重点关注 MTU（最大传输单元） 和 txqueuelen（发送队列长度），比如可以用下面的命令来对这两个参数进行微调：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 txqueuelen 2000</span><br><span class="line">ifconfig eth0 mtu 1500</span><br></pre></td></tr></table></figure></p><p><strong>2）网络接口地址配置</strong></p><p>ifconfig 还常用来配置网口的地址，比如：<br>为网卡配置和删除 IPv6 地址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 add 33ffe:3240:800:1005::2/64    #为网卡eth0配置IPv6地址</span><br><span class="line">ifconfig eth0 del 33ffe:3240:800:1005::2/64    #为网卡eth0删除IPv6地址</span><br></pre></td></tr></table></figure></p><p>修改MAC地址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 hw ether 00:AA:BB:CC:dd:EE</span><br></pre></td></tr></table></figure></p><p>配置IP地址：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 192.168.2.10</span><br><span class="line">ifconfig eth0 192.168.2.10 netmask 255.255.255.0</span><br><span class="line">ifconfig eth0 192.168.2.10 netmask 255.255.255.0 broadcast 192.168.2.255</span><br></pre></td></tr></table></figure></p><h4 id="IP"><a href="#IP" class="headerlink" title="IP"></a><strong>IP</strong></h4><hr><p>ip 命令用来显示或设置 Linux 主机的网络接口、路由、网络设备、策略路由和隧道等信息，是 Linux 下功能强大的网络配置工具，旨在替代 ifconfig 命令，如下显示 IP 命令的强大之处，功能涵盖到 ifconfig、netstat、route 三个命令。</p><center><img src="/images/linux/iptool.jpg" alt=""></center><h4 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a><strong>netstat</strong></h4><hr><p>netstat 可以查看整个 Linux 系统关于网络的情况，是一个集多钟网络工具于一身的组合工具。</p><p>常用的选项包括以下几个：</p><ul><li>默认：列出连接的套接字</li><li>-a：列出所有套接字的信息</li><li>-s：各种网络协议栈统计信息</li><li>-i：网络接口信息</li><li>-r：列出路由表</li><li>-l：仅列出有在 Listen 的服务状态</li><li>-p：显示 PID 和进程名称</li></ul><p>各参数组合使用实例如下：</p><ul><li>netstat -at 列出所有 TCP 端口</li><li>netstat -au 列出所有 UDP 端口</li><li>netstat -lt 列出所有监听 TCP 端口的 socket</li><li>netstat -lu 列出所有监听 UDP 端口的 socket</li><li>netstat -lx 列出所有监听 UNIX 端口的 socket</li><li>netstat -ap | grep ssh 找出程序运行的端口</li><li>netstat -an | grep ‘:80’ 找出运行在指定端口的进程</li></ul><p><strong>1）netstat 默认显示连接的套接字数据</strong></p><center><img src="/images/linux/netstat.jpg" alt=""></center><p>整体上来看，输出结果包括两个部分：</p><ul><li>Active Internet connections ：有源 TCP 连接，其中 Recv-Q 和 Send-Q 指的是接收队列和发送队列，这些数字一般都是 0，如果不是，说明请求包和回包正在队列中堆积。</li><li>Active UNIX domain sockets：有源 UNIX 域套接口，其中 proto 显示连接使用的协议，RefCnt 表示连接到本套接口上的进程号，Types 是套接口的类型，State 是套接口当前的状态，Path 是连接到套接口的进程使用的路径名。</li></ul><p><strong>2）netstat -i 显示网络接口信息</strong></p><center><img src="/images/linux/netstati.png" alt=""></center><p>接口信息包括网络接口名称（Iface）、MTU，以及一系列接收（RX-）和传输（TX-）的指标。其中 OK 表示传输成功的包，ERR 是错误包，DRP 是丢包，OVR 是超限包。</p><p>这些参数有助于我们对网络收包情况进行分析，从而判断瓶颈所在。</p><p><strong>3）netstat -s 显示所有网络协议栈的信息</strong></p><center><img src="/images/linux/netstats.jpg" alt=""></center><p>可以看到，这条命令能够显示每个协议详细的信息，这有助于我们针对协议栈进行更细粒度的分析。</p><p><strong>4）netstat -r 显示路由表信息</strong></p><center><img src="/images/linux/netstat.jpg" alt=""></center><p>这条命令能够看到主机路由表的一个情况。当然查路由我们也可以用 ip route 和 route 命令，这个命令显示的信息会更详细一些。</p><h4 id="ifstat"><a href="#ifstat" class="headerlink" title="ifstat"></a><strong>ifstat</strong></h4><hr><p>ifstat 主要用来监测主机网口的网络流量，常用的选项包括：</p><ul><li>-a：监测主机所有网口</li><li>-i：指定要监测的网口</li><li>-t：在每行输出信息前加上时间戳</li><li>-b：以 Kbit/s 显示流量数据，而不是默认的 KB/s</li><li>delay：采样间隔（单位是 s），即每隔 delay 的时间输出一次统计信息</li><li>count：采样次数，即共输出 count 次统计信息</li></ul><p>比如，通过以下命令统计主机所有网口某一段时间内的流量数据：</p><center><img src="/images/linux/ifstat.png" alt=""></center><p>可以看出，分别统计了三个网口的流量数据，前面输出的时间戳，有助于我们统计一段时间内各网口总的输入、输出流量。</p><h4 id="netcat"><a href="#netcat" class="headerlink" title="netcat"></a><strong>netcat</strong></h4><hr><p>netcat，简称 nc，命令简单，但功能强大，在排查网络故障时非常有用，因此它也在众多网络工具中有着“瑞士军刀”的美誉。</p><p>它主要被用来构建网络连接。可以以客户端和服务端的方式运行，当以服务端方式运行时，它负责监听某个端口并接受客户端的连接，因此可以用它来调试客户端程序；当以客户端方式运行时，它负责向服务端发起连接并收发数据，因此也可以用它来调试服务端程序，此时它有点像 Telnet 程序。</p><p>常用的选项包括以下几种：</p><ul><li>-l：以服务端的方式运行，监听指定的端口。默认是以客户端的方式运行。</li><li>-k：重复接受并处理某个端口上的所有连接，必须与 -l 一起使用。</li><li>-n：使用 IP 地址表示主机，而不是主机名，使用数字表示端口号，而不是服务名称。</li><li>-p：当以客户端运行时，指定端口号。</li><li>-s：设置本地主机发出的数据包的 IP 地址。</li><li>-C：将 CR 和 LF 两个字符作为结束符。</li><li>-U：使用 UNIX 本地域套接字通信。</li><li>-u：使用 UDP 协议通信，默认使用的是 TCP 协议。</li><li>-w：如果 nc 客户端在指定的时间内未检测到任何输入，则退出。</li><li>-X：当 nc 客户端与代理服务器通信时，该选项指定它们之间的通信协议，目前支持的代理协议包括 “4”（SOCKS v.4），“5”（SOCKS v.5）和 “connect” （HTTPs Proxy），默认使用 SOCKS v.5。</li><li>-x：指定目标代理服务器的 IP 地址和端口号。</li></ul><p>下面举一个简单的例子，使用 nc 命令发送消息：<br>首先，启动服务端，用 nc -l 0.0.0.0 12345 监听端口 12345 上的所有连接。</p><center><img src="/images/linux/netcat.png" alt=""></center><p>然后，启动客户端，用 nc -p 1234 127.0.0.1 12345 使用 1234 端口连接服务器 127.0.0.1::12345。</p><center><img src="/images/linux/netcatc.png" alt=""></center><p>接着就可以在两端互发数据了。这里只是抛砖引玉，更多例子大家可以多实践。</p><h4 id="tcpdump"><a href="#tcpdump" class="headerlink" title="tcpdump"></a><strong>tcpdump</strong></h4><hr><p>最后是 tcpdump，强大的网络抓包工具。虽然有 wireshark 这样更易使用的图形化抓包工具，但 tcpdump 仍然是网络排错的必备利器。</p><p>tcpdump 选项很多，我就不一一列举了，大家可以看文章末尾的引用来进一步了解。这里列举几种 tcpdump 常用的用法。</p><p><strong>1）捕获某主机的数据包</strong></p><p>比如想要捕获主机 200.200.200.100 上所有收到和发出的所有数据包，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump host 200.200.200.100</span><br></pre></td></tr></table></figure></p><p><strong>2）捕获多个主机的数据包</strong></p><p>比如要捕获主机 200.200.200.1 和主机 200.200.200.2 或 200.200.200.3 的通信，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump host 200.200.200.1 and \(200.200.200.2 or \)</span><br></pre></td></tr></table></figure></p><p>同样要捕获主机 200.200.200.1 除了和主机 200.200.200.2 之外所有主机通信的 IP 包。使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump ip host 200.200.200.1 and ! 200.200.200.2</span><br></pre></td></tr></table></figure></p><p><strong>3）捕获某主机接收或发出的某种协议类型的包</strong><br>比如要捕获主机 200.200.200.1 接收或发出的 Telnet 包，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump tcp port 23 host 200.200.200.1</span><br></pre></td></tr></table></figure></p><p><strong>4）捕获某端口相关的数据包</strong></p><p>比如捕获在端口 6666 上通过的包，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump port 6666</span><br></pre></td></tr></table></figure></p><p><strong>5）捕获某网口的数据包</strong><br>比如捕获在网口 eth0 上通过的包，使用：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i eth0</span><br></pre></td></tr></table></figure></p><p>下面还是举个例子，抓取 TCP 三次握手的包：<br>首先，用 nc 启动一个服务端，监听端口 12345 上客户端的连接：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -v -l 0.0.0.0 12345</span><br></pre></td></tr></table></figure></p><p>接着，启动 tcpdump 监听端口 12345 上通过的包：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcpdump -i any &apos;port 12345&apos; -XX -nn -vv -S</span><br></pre></td></tr></table></figure></p><p>然后，再用 nc 启动客户端，连接服务端：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -v 127.0.0.1 12345</span><br></pre></td></tr></table></figure></p><p>最后，我们看到 tcpdump 抓到包如下：</p><center><img src="/images/linux/tcpdump.jpg" alt=""></center><p>怎么分析是 TCP 的三次握手，就当做小作业留给大家吧，其实看图就已经很明显了。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><hr><p>本文总结了几种初级的网络工具，一般的网络性能分析，通过组合以上几种工具，基本都能应付，但对于复杂的问题，以上工具可能就无能为力了。更多高阶的工具将在下文送上，敬请期待。</p><p>Reference：</p><ol><li>ip 和 ipconfig：<br><a href="https://blog.csdn.net/freeking101/article/details/68939059" target="_blank" rel="noopener">https://blog.csdn.net/freeking101/article/details/68939059</a></li><li>性能之巅：Linux网络性能分析工具<br><a href="http://www.infoq.com/cn/articles/linux-networking-performance-analytics" target="_blank" rel="noopener">http://www.infoq.com/cn/articles/linux-networking-performance-analytics</a></li><li>抓包工具tcpdump用法说明<br><a href="https://www.cnblogs.com/f-ck-need-u/p/7064286.html" target="_blank" rel="noopener">https://www.cnblogs.com/f-ck-need-u/p/7064286.html</a></li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
            <tag> 性能分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文掌握 Linux 性能分析之 IO 篇</title>
      <link href="/2018/05/24/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8BIO%E7%AF%87/"/>
      <url>/2018/05/24/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8BIO%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>这是 Linux 性能分析系列的第三篇。</p><p>IO 和 存储密切相关，存储可以概括为磁盘，内存，缓存，三者读写的性能差距非常大，磁盘读写是毫秒级的（一般 0.1-10ms），内存读写是微妙级的（一般 0.1-10us），cache 是纳秒级的（一般 1-10ns）。但这也是牺牲其他特性为代价的，速度快的，价格越贵，容量也越小。</p><p>IO 性能这块，我们更多关注的是读写磁盘的性能。首先，先了解下磁盘的基本信息。</p><h3 id="磁盘基本信息"><a href="#磁盘基本信息" class="headerlink" title="磁盘基本信息"></a>磁盘基本信息</h3><hr><h4 id="fdisk"><a href="#fdisk" class="headerlink" title="fdisk"></a><strong>fdisk</strong></h4><p>查看磁盘信息，包括磁盘容量，扇区大小，IO 大小等信息，常用 <code>fdisk -l</code>查看：</p><center><img src="/images/linux/fdisk.jpg" alt=""></center><p>可以看到 /dev/ 下有一个 40G 的硬盘，一共 8K 多万个扇区，每个扇区 512字节，IO 大小也是 512 字节。</p><h4 id="df"><a href="#df" class="headerlink" title="df"></a><strong>df</strong></h4><p>查看磁盘使用情况，通常看磁盘使用率：</p><center><img src="/images/linux/df.jpg" alt=""></center><h3 id="磁盘性能分析"><a href="#磁盘性能分析" class="headerlink" title="磁盘性能分析"></a>磁盘性能分析</h3><hr><p>主要分析磁盘的读写效率（IOPS：每秒读写的次数；吞吐量：每秒读写的数据量），IO 繁忙程度，及 IO 访问对 CPU 的消耗等性能指标。</p><h4 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a><strong>vmstat</strong></h4><p>第一个较为常用的还是这个万能的 vmstat：</p><center><img src="/images/linux/vmstatio.jpg" alt=""></center><p>对于 IO，我们常关注三个部分：</p><ul><li>b 值：表示因为 IO 阻塞排队的任务数</li><li>bi 和 bo 值：表示每秒读写磁盘的块数，bi（block in）是写磁盘，bo（block out）是读磁盘。</li><li>wa 值：表示因为 IO 等待（wait）而消耗的 CPU 比例。</li></ul><p>一般这几个值偏大，都意味着系统 IO 的消耗较大，对于读请求较大的服务器，b、bo、wa 的值偏大，而写请求较大的服务器，b、bi、wa 的值偏大。</p><h4 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a><strong>iostat</strong></h4><p>vmstat 虽然万能，但是它分析的东西有限，iostat 是专业分析 IO 性能的工具，可以方便查看 CPU、网卡、tty 设备、磁盘、CD-ROM 等等设备的信息，非常强大，总结下来，共有以下几种用法：</p><p><strong>1）iostat -c 查看部分 CPU 使用情况：</strong></p><center><img src="/images/linux/iostat.png" alt=""></center><p>这里显示的是多个 CPU 的平均值，每个字段的含义我就不多解释了，我一般会重点关注 %iowait 和 %idle，分别表示 CPU 等待 IO 完成时间的百分比和 CPU 空闲时间百分比。</p><p>如果 %iowait 较高，则表明磁盘存在 IO 瓶颈，如果 %idle 较高，则 CPU 比较空闲，如果两个值都比较高，则有可能 CPU 在等待分配内存，瓶颈在内存，此时应该加大内存，如果 %idle 较低，则此时瓶颈在 CPU，应该增加 CPU 资源。</p><p><strong>2）iostat -d 查看磁盘使用情况，主要是显示 IOPS 和吞吐量信息</strong>（-k : 以 KB 为单位显示，-m：以 M 为单位显示）：</p><center><img src="/images/linux/iostatd.png" alt=""></center><p>其中，几个参数分别解释如下：</p><ul><li>tps：设备每秒的传输次数（transfers per second），也就是读写次数。</li><li>kB_read/s 和 kB_wrtn/s：每秒读写磁盘的数据量。</li><li>kB_read 和 kB_wrtn：读取磁盘的数据总量。</li></ul><p><strong>3）iostat -x 查看磁盘详细信息：</strong></p><center><img src="/images/linux/iostatx.jpg" alt=""></center><p>其中，几个参数解释如下；</p><ul><li>rrqm/s 和 wrqm/s：分别每秒进行合并的读操作数和写操作数，这是什么意思呢，合并就是说把多次 IO 请求合并成少量的几次，这样可以减小 IO 开销，buffer 存在的意义就是为了解决这个问题的。</li><li>r/s 和 w/s：每秒磁盘读写的次数。这两个值相加就是 tps。</li><li>rkB/s 和 wkB/s：每秒磁盘读写的数据量，这两个值和上面的 kB_read/s、kB_wrnt/s 是一样的。</li><li>avgrq-sz：平均每次读写磁盘扇区的大小。</li><li>avgqu-sze：平均 IO 队列长度。队列长度越短越好。</li><li>await：平均每次磁盘读写的等待时间（ms）。</li><li>svctm：平均每次磁盘读写的服务时间（ms）。</li><li>%util：一秒钟有百分之多少的时间用于磁盘读写操作。</li></ul><p>以上这些参数太多了，我们并不需要每个都关注，可以重点关注两个：</p><p><strong>a. %util：衡量 IO 的繁忙程度</strong></p><p>这个值越大，说明产生的 IO 请求较多，IO 压力较大，我们可以结合 %idle 参数来看，如果 %idle &lt; 70% 就说明 IO 比较繁忙了。也可以结合 vmstat 的 b 参数（等待 IO 的进程数）和 wa 参数（IO 等待所占 CPU 时间百分比）来看，如果 wa &gt; 30% 也说明 IO 较为繁忙。</p><p><strong>b. await：衡量 IO 的响应速度</strong></p><p>通俗理解，await 就像我们去医院看病排队等待的时间，这个值和医生的服务速度（svctm）和你前面排队的人数（avgqu-size）有关。如果 svctm 和 await 接近，说明磁盘 IO 响应时间较快，排队较少，如果 await 远大于 svctm，说明此时队列太长，响应较慢，这时可以考虑换性能更好的磁盘或升级 CPU。</p><p><strong>4）iostat 1 2 默认显示 cpu 和 吞吐量信息，1 定时 1s 显示，2 显示 2 条信息</strong></p><center><img src="/images/linux/iostat12.jpg" alt=""></center><h3 id="进程-IO-性能分析"><a href="#进程-IO-性能分析" class="headerlink" title="进程 IO 性能分析"></a>进程 IO 性能分析</h3><hr><p>有了以上两个命令，基本上能对磁盘 IO 的信息有个全方位的了解了。但如果要确定具体哪个进程的 IO 开销较大，这就得借助另外的工具了。</p><h4 id="iotop"><a href="#iotop" class="headerlink" title="iotop"></a><strong>iotop</strong></h4><p>这个命令类似 top，可以显示每个进程的 IO 情况，有了这个命令，就可以定位具体哪个进程的 IO 开销比较大了。</p><center><img src="/images/linux/iostop.jpg" alt=""></center><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>OK，最后还是总结下，fdisk -l 和 df 查看磁盘基本信息，iostat -d 查看磁盘 IOPS 和吞吐量，iostat -x 结合 vmstat 查看磁盘的繁忙程度和处理效率。</p><p>下文我们将探讨网络方面的的性能分析问题。</p><p>Reference：<br>1.linux 性能分析：<br><a href="http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html" target="_blank" rel="noopener">http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html</a></p><ol start="2"><li>linux 性能分析工具总结：<br><a href="http://rdc.hundsun.com/portal/article/731.html" target="_blank" rel="noopener">http://rdc.hundsun.com/portal/article/731.html</a></li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 性能分析 </tag>
            
            <tag> I/O </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文掌握 Linux 性能分析之内存篇</title>
      <link href="/2018/05/18/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8B%E5%86%85%E5%AD%98%E7%AF%87/"/>
      <url>/2018/05/18/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8B%E5%86%85%E5%AD%98%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="内存信息"><a href="#内存信息" class="headerlink" title="内存信息"></a>内存信息</h3><hr><p>同样在分析内存之前，我们得知到怎么查看系统内存信息，有以下几种方法。</p><h4 id="proc-meminfo"><a href="#proc-meminfo" class="headerlink" title="/proc/meminfo"></a><strong>/proc/meminfo</strong></h4><p>这个文件记录着比较详细的内存配置信息，使用 cat /proc/meminfo 查看。</p><center><img src="/images/linux/meminfo.jpg" alt=""></center><p>我们比较关心的是下面几个字段：</p><ul><li>MemTotal：系统总内存，由于 BIOS、内核等会占用一些内存，所以这里和配置声称的内存会有一些出入，比如我这里配置有 2G，但其实只有 1.95G 可用。</li><li>MemFree：系统空闲内存。</li><li>MemAvailable：应用程序可用内存。有人会比较奇怪和 MemFree 的区别，可以从两个层面来区分，MemFree 是系统层面的，而 MemAvailable 是应用程序层面的。系统中有些内存虽然被使用了但是有一部分是可以回收的，比如 Buffers、Cached 及 Slab 这些内存，这部分可以回收的内存加上 MemFree 才是 MemAvailable 的内存值，这是内核通过特定算法算出来的，是一个估算值。</li><li>Buffers：缓冲区内存</li><li>Cached：缓存</li></ul><p>上面信息没有 MemUsed 的值，虽然可以用现有的值大致估算出来，但是我们想一步到位，就用下面的 free 命令。</p><h4 id="free"><a href="#free" class="headerlink" title="free"></a><strong>free</strong></h4><p>这个命令估计用的人就多了（我一般都是用这个命令）。</p><center><img src="/images/linux/free.png" alt=""></center><p>这里存在一个计算公式：</p><p><strong>MemTotal = used + free + buff/cache（单位 K）</strong></p><p>几个字段和上面 /proc/meminfo 的字段是对应的。还有个 shared 字段，这个是多进程的共享内存空间，不常用。</p><p>我们注意到 free 很小，buff/cache 却很大，这是 Linux 的内存设计决定的，Linux 的想法是内存闲着反正也是闲着，不如拿出来做系统缓存和缓冲区，提高数据读写的速率。但是当系统内存不足时，buff/cache 会让出部分来，非常灵活的操作。</p><p>要看比较直观的值，可以加 -h 参数：</p><center><img src="/images/linux/freeh.png" alt=""></center><h4 id="dmidecode"><a href="#dmidecode" class="headerlink" title="dmidecode"></a><strong>dmidecode</strong></h4><p>同样可以使用这个命令，对于内存，可以使用 dmidecode -t memory 查看：</p><center><img src="/images/linux/dmimem.jpg" alt=""></center><h4 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a><strong>vmstat</strong></h4><p>这个命令也是非常常用了。但对于内存，显示信息有限。它更多是用于进行系统全局分析和 CPU 分析。详细可以看 CPU 分析一文。</p><center><img src="/images/linux/vmstatmem.jpg" alt=""></center><h3 id="进程内存使用情况分析"><a href="#进程内存使用情况分析" class="headerlink" title="进程内存使用情况分析"></a>进程内存使用情况分析</h3><hr><p>最常用的两个命令 ps 和 top，虽然很简单的两个命令，但还是有不少学问的。</p><h4 id="top-htop"><a href="#top-htop" class="headerlink" title="top/htop"></a><strong>top/htop</strong></h4><p>top 命令运行时默认是按照 CPU 利用率进行排序的，如果要按照内存排序，该怎么操作呢？两种方法，一种直接按 “M”（相应的按 “P” 是 CPU），另外一种是在键入 top 之后，按下 “F”，然后选择要排序的字段，再按下 “s” 确认即可。</p><center><img src="/images/linux/topmem.jpg" alt=""></center><p>可以看到，我按照 “%MEM” 排序的结果。这个结果对于查看系统占用内存较多的哪些进程是比较有用的。</p><p>然后这里我们会重点关注几个地方，上面横排区，和前面几个命令一样可以查看系统内存信息，中间标注的横条部分，和内存相关的有三个字段：VIRT、RES、SHR。</p><ul><li>VIRT：virtual memory usage，进程占用的虚拟内存大小。</li><li>RES：resident memory usage，进程常驻内存大小，也就是实际内存占用情况，一般我们看进程占用了多少内存，就是看的这个值。</li><li>SHR：shared memory，共享内存大小，不常用。</li></ul><h4 id="ps"><a href="#ps" class="headerlink" title="ps"></a><strong>ps</strong></h4><p>ps 同样可以查看进程占用内存情况，一般常用来查看 Top n 进程占用内存情况，如：</p><p><code>ps aux --sort=rss | head -n</code>，表示按 rss 排序，取 Top n。</p><center><img src="/images/linux/psmem.jpg" alt=""></center><p>这里也关注三个字段：</p><ul><li>%MEM：进程使用物理内存所占百分比。</li><li>VSZ：进程使用虚拟内存大小。</li><li>RSS：进程使用物理内存大小，我们会重点关注这个值。</li></ul><h4 id="pmap"><a href="#pmap" class="headerlink" title="pmap"></a><strong>pmap</strong></h4><p>这个命令用于查看进程的内存映像信息，能够查看进程在哪些地方用了多少内存。常用 <code>pmap -x pid</code> 来查看。</p><center><img src="/images/linux/pmap1.jpg" alt=""></center><br><center><img src="/images/linux/pmap2.jpg" alt=""></center><p>可以看到该进程内存被哪些库、哪些文件所占用，据此我们定位程序对内存的使用。</p><p>几个字段介绍一下：</p><ul><li>Address：占用内存的文件的内存起始地址。</li><li>Kbytes：占用内存的字节数。</li><li>RSS：实际占用内存大小。</li><li>Dirty：脏页大小。</li><li>Mapping：占用内存的文件，[anon] 为已分配的内存，[stack] 为程序堆栈</li><li>最后的 total 为统计的总值。我们可以使用 pmap -x pid | tail -1 这样只显示最后一行，循环显示最后一行，达到监控该进程的目的。使用：<br><code>while true; do pmap -x pid | tail -1; sleep 1; done</code></li></ul><p>OK，以上工具都是 Linux 自带的，当然还有很多高阶的工具，比如 atop、memstat 等等，对于内存泄漏有一个比较常用的检测工具 Valgrind，这些等之后再找时间跟大家分享了。</p><p>通过以上手段，我们基本上就能定位内存问题所在了，究竟是内存太小，还是进程占用内存太多，有哪些进程占用较多，这些进程又究竟有哪些地方占用较多，这些问题通过以上方法都能解决。</p><p>最后简单总结下，以上不少工具可能有人会犯选择困难症了。对于我来说，查看系统内存用 free -h，分析进程内存占用用 ps 或者 top（首选 ps），深入分析选择 pmap，就酱。</p><p>Reference：<br>1.Linux下查看内存使用情况的多种方法：<br><a href="http://stor.51cto.com/art/201804/570236.htm" target="_blank" rel="noopener">http://stor.51cto.com/art/201804/570236.htm</a></p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 性能分析 </tag>
            
            <tag> 内存 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CPU 拓扑：从 SMP 谈到 NUMA （理论篇）</title>
      <link href="/2018/05/10/tech/linux/cpu/CPU%E6%8B%93%E6%89%91%EF%BC%9A%E4%BB%8ESMP%E8%B0%88%E5%88%B0NUMA%EF%BC%88%E5%AE%9E%E8%B7%B5%E7%AF%87%EF%BC%89/"/>
      <url>/2018/05/10/tech/linux/cpu/CPU%E6%8B%93%E6%89%91%EF%BC%9A%E4%BB%8ESMP%E8%B0%88%E5%88%B0NUMA%EF%BC%88%E5%AE%9E%E8%B7%B5%E7%AF%87%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>本文的一些概念依赖于上一篇 “CPU 拓扑：从 SMP 谈到 NUMA （理论篇）”，如果你对这块还没有概念，建议看完那篇再来看，如果对这块有自己独到的见解，欢迎探讨。</p><p>本文主要会侧重 NUMA 这块，我会通过自己的环境验证 NUMA  的几个概念，以便对 NUMA 的架构有个较深的印象。</p><h3 id="NUMA-的几个概念：Node，Socket，Core，Thread"><a href="#NUMA-的几个概念：Node，Socket，Core，Thread" class="headerlink" title="NUMA 的几个概念：Node，Socket，Core，Thread"></a>NUMA 的几个概念：Node，Socket，Core，Thread</h3><hr><p>NUMA 技术的主要思想是将 CPU 进行分组，Node 即是分组的抽象，一个 Node 表示一个分组，一个分组可以由多个 CPU 组成。每个 Node 都有自己的本地资源，包括内存、IO 等。每个 Node 之间通过互联模块（QPI）进行通信，因此每个 Node 除了可以访问自己的本地内存之外，还可以访问远端 Node 的内存，只不过性能会差一些，一般用 distance 这个抽象的概念来表示各个 Node 之间互访资源的开销。</p><center><img src="/images/linux/numaarch1.jpg" alt=""></center><p>Node 是一个逻辑上的概念，与之相对的 Socket，是物理上的概念。它表示一颗物理 CPU 的封装，是主板上的 CPU 插槽，所以，一般就称之为插槽（敲黑板，这 Y 不是套接字吗？emmm……）</p><p>Core 就是 Socket 里独立的一组程序执行单元，称之为物理核。有了物理核，自然就有逻辑核，Thread 就是逻辑核。更专业的说法应该称之为超线程。</p><p>超线程是为了进一步提高 CPU 的处理能力，Intel 提出的新型技术。它能够将一个 Core 从逻辑上划分成多个逻辑核（一般是两个），每个逻辑核有独立的寄存器和中断逻辑，但是一个 Core 上的多个逻辑核共享 Core 内的执行单元和 Cache，频繁调度可能会引起资源竞争，影响性能。超线程必须要 CPU 支持才能开启。</p><center><img src="/images/linux/ht.jpg" alt=""></center><p>综上所述，一个 NUMA Node 可以有一个或者多个 Socket，每个 Socket 也可以有一个（单核）或者多个（多核）Core，一个 Core 如果打开超线程，则会变成两个逻辑核（Logical Processor，简称 Processor）。</p><p>所以，几个概念从大到小排序依次是：</p><p><strong>Node &gt; Socket &gt; Core &gt; Processor</strong>。</p><h3 id="验证-CPU-拓扑"><a href="#验证-CPU-拓扑" class="headerlink" title="验证 CPU 拓扑"></a>验证 CPU 拓扑</h3><hr><p>了解了以上基本概念，下面在实际环境中查看这些概念并验证。</p><h4 id="Node"><a href="#Node" class="headerlink" title="Node"></a><strong>Node</strong></h4><p>用 <code>numactl --hardware</code> 查看当前系统的 NUMA Node（numactl 是设定进程 NUMA 策略的命令行工具）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Linux # numactl --hardware</span><br><span class="line">available: 2 nodes (0-1)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 12 13 14 15 16 17</span><br><span class="line">node 0 size: 49043 MB</span><br><span class="line">node 0 free: 20781 MB</span><br><span class="line">node 1 cpus: 6 7 8 9 10 11 18 19 20 21 22 23</span><br><span class="line">node 1 size: 49152 MB</span><br><span class="line">node 1 free: 31014 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0   1 </span><br><span class="line">0:  10  21 </span><br><span class="line">1:  21  10</span><br></pre></td></tr></table></figure></p><p>可以得出的信息有：1）系统的 Node 数为 2；2）每个 Node 包含的 Processor 数为 12；3）每个 Node 的总内存大小和空闲内存大小；4）每个 Node 之间的 distance。</p><p>还可以查看 <code>/sys/devices/system/node/</code> 目录，这里记录着具体哪些 Node。</p><h4 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a><strong>Socket</strong></h4><p><code>/proc/cpuinfo</code> 中记录着 Socket 信息，用 “physical id” 表示，可以用 <code>cat /proc/cpuinfo | grep &quot;physical id&quot;</code> 查看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Linux # cat /proc/cpuinfo | grep &quot;physical id&quot;</span><br><span class="line">physical id     : 0</span><br><span class="line">physical id     : 0</span><br><span class="line">physical id     : 0</span><br><span class="line">physical id     : 0</span><br><span class="line">physical id     : 1</span><br><span class="line">physical id     : 1</span><br><span class="line">physical id     : 1</span><br><span class="line">physical id     : 1</span><br></pre></td></tr></table></figure><p>可以看到有 2 个 Socket，我们还可以查看以下这几种变种：</p><p><strong>1）查看有几个 Socket</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Linux # grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;&#123;print $2 | &quot;sort -un&quot;&#125;&apos;</span><br><span class="line"></span><br><span class="line">0</span><br><span class="line">1</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux # grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;&#123;print $2 | &quot;sort -un&quot;&#125;&apos; | wc -l</span><br><span class="line"></span><br><span class="line">2</span><br></pre></td></tr></table></figure><p><strong>2）查看每个 Socket 有几个 Processor</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Linux # grep &apos;physical id&apos; /proc/cpuinfo | awk -F: &apos;&#123;print $2&#125;&apos; | sort | uniq -c</span><br><span class="line"></span><br><span class="line">12 0</span><br><span class="line">12 1</span><br></pre></td></tr></table></figure></p><p><strong>3）查看每个 Socket 有哪几个 Processor</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Linux # awk -F: &apos;&#123; </span><br><span class="line">&gt;     if ($1 ~ /processor/) &#123;</span><br><span class="line">&gt;         gsub(/ /,&quot;&quot;,$2);</span><br><span class="line">&gt;         p_id=$2;</span><br><span class="line">&gt;     &#125; else if ($1 ~ /physical id/)&#123;</span><br><span class="line">&gt;         gsub(/ /,&quot;&quot;,$2);</span><br><span class="line">&gt;         s_id=$2;</span><br><span class="line">&gt;         arr[s_id]=arr[s_id] &quot; &quot; p_id</span><br><span class="line">&gt;     &#125;</span><br><span class="line">&gt; &#125; </span><br><span class="line">&gt; </span><br><span class="line">&gt; END&#123;</span><br><span class="line">&gt;     for (i in arr) </span><br><span class="line">&gt;         print arr[i];</span><br><span class="line">&gt; &#125;&apos; /proc/cpuinfo | cut -c2-</span><br><span class="line"></span><br><span class="line">0 1 2 3 4 5 12 13 14 15 16 17</span><br><span class="line">6 7 8 9 10 11 18 19 20 21 22 23</span><br></pre></td></tr></table></figure></p><h4 id="Core"><a href="#Core" class="headerlink" title="Core"></a><strong>Core</strong></h4><p>同样在 /proc/cpuinfo 中查看 Core 信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Linux # cat /proc/cpuinfo |grep &quot;core id&quot; | sort -u</span><br><span class="line">core id    : 0</span><br><span class="line">core id    : 1</span><br><span class="line">core id    : 2</span><br><span class="line">core id    : 3</span><br><span class="line">core id    : 4</span><br><span class="line">core id    : 5</span><br></pre></td></tr></table></figure><p>上面的结果表明一个 Socket 有 5 个 Core。上面查到有 2 个 Socket，则一共就有 10 个 Core。</p><h4 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a><strong>Processor</strong></h4><p>上面查看 Socket 信息时已经能够得到 Processor 的信息，总共有 24 个 Processor，不过也可以直接从 /proc/cpuinfo 中获取：</p><p><strong>1）获取总的 Processor 数，查看 “processor” 字段：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux # cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l</span><br><span class="line"></span><br><span class="line">24</span><br></pre></td></tr></table></figure></p><p><strong>2）获取每个 Socket 的 Processor 数，查看 “siblings” 字段：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux # cat /proc/cpuinfo | grep &quot;siblings&quot; | sort -u</span><br><span class="line"></span><br><span class="line">12</span><br></pre></td></tr></table></figure></p><h4 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a><strong>Cache</strong></h4><p>Cache 也一样通过 /proc/cpuinfo 查看：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">processor  : 0</span><br><span class="line"></span><br><span class="line">cache size  : 15360 KB</span><br><span class="line"></span><br><span class="line">cache_alignment  : 64</span><br></pre></td></tr></table></figure></p><p>不过这里的值 cache size 比较粗略，我们并不知道这个值是哪一级的 Cache 值（L1？L2？L3？），这种方法不能确定，我们换一种方法。</p><p>其实详细的 Cache 信息可以通过 <code>sysfs</code> 查看，如下：</p><p>比如查看 cpu0 的 cache 情况：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux # ls /sys/devices/system/cpu/cpu0/cache/</span><br><span class="line"></span><br><span class="line">index0/  index1/  index2/  index3/</span><br></pre></td></tr></table></figure></p><p>其中包含四个目录：<br>index0 存 L1 数据 Cache，index1 存 L1 指令 Cache，index2 存 L2 Cache，index3 存 L3 Cache。每个目录里面包含一堆描述 Cache 信息的文件。我们选 index0 具体看下：</p><center><img src="/images/linux/cache_info.jpg" alt=""></center><p>其中，shared_cpu_list 和 shared_cpu_map 表示意思是一样的，都表示该 cache 被哪几个 processor 共享。对 <strong>shared_cpu_map</strong> 具体解释一下。</p><p>这个值表面上看是二进制，但其实是 16 进制，每个数字有 4 个bit，代表 4 个 cpu。比如上面的 001001 拆开后是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0000 0000 0001 0000 0000 0001，1 bit 处即对应 cpu 标号，即 cpu0 和 cpu12。</span><br></pre></td></tr></table></figure></p><p>同样我们可以对其他 index 进行统计，可以得出：<br><strong>/proc/cpuinfo 中的 cache size 对应的 L3 Cache size</strong>。</p><p>最后，综合以上所有信息我们可以绘制出一下的 CPU 拓扑图：</p><center><img src="/images/linux/cpu_topo.jpg" alt=""></center><p>我们发现以上命令用得不太顺手，要获取多个数据需要输入多条命令，能不能一条命令就搞定，当然是有的，lscpu 就可以做到，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Linux # lscpu </span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                24   //共有24个逻辑CPU（threads）</span><br><span class="line">On-line CPU(s) list:   0-23</span><br><span class="line">Thread(s) per core:    2   //每个 Core 有 2 个 Threads</span><br><span class="line">Core(s) per socket:    12  //每个 Socket 有 12 个 Threads</span><br><span class="line">Socket(s):             2  //共有 2 个 Sockets</span><br><span class="line">NUMA node(s):          2  //共有 2 个 Nodes</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 63</span><br><span class="line">Stepping:              2</span><br><span class="line">CPU MHz:               2401.000</span><br><span class="line">BogoMIPS:             4803.16</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K  //L1 data cache 32k</span><br><span class="line">L1 cache:             32K   //L1 instruction cache 32k  </span><br><span class="line">L2 cache:              256K //L2 instruction cache 256k</span><br><span class="line">L3 cache:              15360K //L3 instruction cache 15M</span><br><span class="line">NUMA node0 CPU(s):     0-5,12-17</span><br><span class="line">NUMA node1 CPU(s):     6-11,18-23</span><br></pre></td></tr></table></figure></p><p>当然了，没有完美的命令，lscpu 也只能显示一些宽泛的信息，只是相对比较全面而已，更详细的信息，比如 Core 和 Cache 信息就得借助 cpuinfo 和 sysfs 了。</p><p>下面给大家提供一个脚本，能够比较直观的显示以上所有信息，有 shell 版的和 python 版的（不是我写的，文末附上了引用出处）。</p><p>大家有需要可以回复 “CPU” 获取，我就不贴出来了，显示的结果大概就是长下面这个样子：</p><p><strong>python 版：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">============================================================</span><br><span class="line">Core and Socket Information (as reported by &apos;/proc/cpuinfo&apos;)</span><br><span class="line">============================================================</span><br><span class="line"></span><br><span class="line">cores =  [0, 1, 2, 3, 4, 5]</span><br><span class="line"></span><br><span class="line">sockets =  [0, 1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Socket 0        Socket 1        </span><br><span class="line"></span><br><span class="line">--------        --------        </span><br><span class="line"></span><br><span class="line">Core 0 [0, 12]         [6, 18]         </span><br><span class="line"></span><br><span class="line">Core 1 [1, 13]         [7, 19]         </span><br><span class="line"></span><br><span class="line">Core 2 [2, 14]         [8, 20]         </span><br><span class="line"></span><br><span class="line">Core 3 [3, 15]         [9, 21]         </span><br><span class="line"></span><br><span class="line">Core 4 [4, 16]         [10, 22]        </span><br><span class="line"></span><br><span class="line">Core 5 [5, 17]         [11, 23]</span><br></pre></td></tr></table></figure></p><p>Reference：</p><ol><li>玩转 CPU 拓扑：<br><a href="http://blog.itpub.net/645199/viewspace-1421876/" target="_blank" rel="noopener">http://blog.itpub.net/645199/viewspace-1421876/</a></li><li>NUMA 体系结构详解<br><a href="https://blog.csdn.net/ustc_dylan/article/details/45667227（shell" target="_blank" rel="noopener">https://blog.csdn.net/ustc_dylan/article/details/45667227（shell</a> 代码引用）</li><li>dpdk 源代码（python 代码引用）</li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> CPU </tag>
            
            <tag> NUMA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CPU 拓扑：从 SMP 谈到 NUMA （理论篇）</title>
      <link href="/2018/05/03/tech/linux/cpu/CPU%E6%8B%93%E6%89%91%E4%BB%8ESMP%E8%B0%88%E5%88%B0NUMA%EF%BC%88%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%89/"/>
      <url>/2018/05/03/tech/linux/cpu/CPU%E6%8B%93%E6%89%91%E4%BB%8ESMP%E8%B0%88%E5%88%B0NUMA%EF%BC%88%E7%90%86%E8%AE%BA%E7%AF%87%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>随着计算机技术（特别是以芯片为主的硬件技术）的快速发展，CPU 架构逐步从以前的单核时代进阶到如今的多核时代，在多核时代里，多颗 CPU 核心构成了一个小型的“微观世界”。每颗 CPU 各司其职，并与其他 CPU 交互，共同支撑起了一个“物理世界”。从这个意义上来看，我们更愿意称这样的“微观世界”为 CPU 拓扑，就像一个物理网络一样，各个网络节点通过拓扑关系建立起连接来支撑起整个通信系统。</p><center><img src="/images/linux/processor.jpg" alt=""></center><h3 id="单核-or-多核-or-多-CPU-or-超线程"><a href="#单核-or-多核-or-多-CPU-or-超线程" class="headerlink" title="单核 or 多核 or 多 CPU or 超线程 ?"></a>单核 or 多核 or 多 CPU or 超线程 ?</h3><hr><p>在单核时代，为了提升 CPU 的处理能力，普遍的做法是提高 CPU 的主频率，但一味地提高频率对于 CPU 的功耗也是影响很大的（CPU 的功耗正比于主频的三次方）。</p><p>另外一种做法是提高 IPC （每个时钟周期内执行的指令条数），这种做法要么是提高指令的并行度，要么是增加核数。显然，后一种方法更具有可扩展性，这也是摩尔定律的必然性。</p><p>CPU 的性能到底是如何体现的呢？为了弄清楚这个问题，我们结合单核 CPU 和多核 CPU 的结构来进一步剖析。</p><p>首先，对于一个单核结构，即一颗物理 CPU 封装里只集成了一个物理核心，其主要组件可以简化为：CPU 寄存器集合、中断逻辑、执行单元和 Cache，如下图：</p><center><img src="/images/linux/one_core.jpg" alt=""></center><p>对于一个多线程程序，主要通过时间片轮转的方式来获得 CPU 的执行权，从内部来看，这其实是串行执行的，性能自然并不怎么高。</p><p>其次，对于多核结构，则是在一颗物理 CPU 封装里集成了多个对等的物理核心，所谓对等，就是每个核心都有相同的内部结构。多个核心之间通过芯片内部总线来完成通信。随着 CPU 制造工艺的提升，每颗 CPU 封装中集成的物理核心也在不断提高。</p><center><img src="/images/linux/multi_core.jpg" alt=""></center><p>对于一个多线程程序，这种情况能够实现真正的并发，但线程在不同核之间切换会存在一定的开销，但由于走的是芯片内部总线，开销相对会比较小。</p><p>除了上述两种结构，还有另外一种结构是多 CPU 结构，也就是多颗单独封装的 CPU 通过外部总线相连，构成的一个统一的计算平台。每个 CPU 都需要独立的电路支持，有自己的 Cache。它们之间的通信通过主板上的总线来完成。</p><center><img src="/images/linux/multi_cpu.jpg" alt=""></center><p>同样对于一个多线程程序，不同于上一种情况的是，线程间的切换走的是外部总线，延迟较大，开销自然较大，而且对于有共享的数据还会因 Cache 一致性带来一定的开销（关于 Cache 下一小节说明）。</p><p>上述结构，一个 CPU 核心同一时间内只能执行一个线程，效率低下，为了提高 CPU 的利用率，CPU 芯片厂商又推出超线程（Hyper-Thread-ing）技术，即让一个物理核心同时执行多个线程，使整体性能得到提升。虽然物理上只有一个核心，但逻辑上被划分了多个逻辑核心，它们之间是完全隔离的。</p><center><img src="/images/linux/ht.jpg" alt=""></center><p>对于每个逻辑核心，拥有完整独立的寄存器集合和中断逻辑，共享执行单元和 Cache。由于是共享执行单元，所以对高 IPC 的应用，其性能提升有限。</p><h3 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h3><hr><p>Cache 是一种 SRAM （Static Random Access Memory，静态访问随机存储器）。出于成本和生产工艺考虑，一般将 Cache 分为三级。一级（L1）访问速度最快，但是容量最小，一般只有几十 KB；二级（L2）次之，一般有几百 KB 到几 MB 不等，三级（LLC，Last Level Cache）最慢，但是容量也最大，一般有几 MB 到几十 MB。</p><p>一级 Cache 又分为数据 Cache 和指令 Cache，顾名思义，数据 Cache 用来存数据，指令 Cache 用来存指令。下图是一个简单的 Cache 系统逻辑示意图。</p><center><img src="/images/linux/cache.jpg" alt=""></center><p>在多核结构中，每个物理核心都拥有独立的一级 Cache 和二级 Cache，而三级 Cache 是所有核心共享。这种共享需要解决的一个问题是公平地为每个核心分配 Cache 大小，避免 Cache 命中率低的问题。</p><p>对于现代计算机系统，说到 Cache，不得不提 TLB（Translation Look-aside Buffer） Cache。简单理解，如果说 Cache 存放的是内存中的内容，那么 TLB Cache 存放的是页表项。</p><p>为什么页表项需要用 Cache 存，原因当然是快。你可能觉得用三级 Cache 存就行了，为什么还要专门上 TLB Cache。</p><p>这里有两点考虑，一点是 TLB 采用基于内容的访问存储器 CAM，这种存储器能做到根据虚拟地址查询直接返回物理地址，效率极高，不需要像传统方式那样采用多级页表查询。另外一点是 Cache 的“淘汰”机制决定，Cache 会根据算法淘汰掉那些不常使用的内容，这对于页表这种需要频繁访问（每次程序寻址都要访问页表）的特性显然是矛盾的，所以就需要专门为页表提供一种特殊的 Cache，即 TLB Cache。</p><h3 id="SMP-or-NUMA-or-MPP"><a href="#SMP-or-NUMA-or-MPP" class="headerlink" title="SMP or NUMA or MPP?"></a>SMP or NUMA or MPP?</h3><hr><p>如果说前面咱们讨论的是 CPU 内部的“微观世界”，那么本节将跳出来，探讨一个系统级的“宏观世界”。</p><p>首先是 SMP，对称多处理器系统，指的是一种多个 CPU 处理器共享资源的电脑硬件架构，其中，每个 CPU 没有主从之分，地位平等，它们共享相同的物理资源，包括总线、内存、IO、操作系统等。每个 CPU 访问内存所用时间都是相同的，因此这种系统也被称为一致存储访问结构（UMA，Uniform Memory Access）。</p><center><img src="/images/linux/smp.jpg" alt=""></center><p>这种系统由于共享资源，不可避免地要加锁来解决资源竞争的问题，带来一定的性能开销，另外，扩展能力还非常有限，实验证明，SMP 系统最好的情况是有 2-4 个 CPU，适用于 PC、笔记本电脑和小型服务器等。</p><p>tips: 查看系统是否是 SMP 结构：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /sys/devices/system/node/     # 如果只看到一个 node0 那就是 SMP 架构</span><br></pre></td></tr></table></figure></p><p>为了应对大规模的系统要求（特别是云计算环境），就研制出了 NUMA 结构，即非一致存储访问结构。</p><p>这种结构引入了 CPU 分组的概念，用 Node 来表示，一个 Node 可能包含多个物理 CPU 封装，从而包含多个 CPU 物理核心。每个 Node 有自己独立的资源，包括内存、IO 等。每个 Node 之间可以通过互联模块总线（QPI）进行通信，所以，也就意味着每个 Node 上的 CPU 都可以访问到整个系统中的所有内存，但很显然，访问远端 Node 的内存比访问本地内存要耗时很多，这也是 NUMA 架构的问题所在，我们在基于 NUMA 架构开发上层应用程序要尽可能避免跨 Node 内存访问。</p><center><img src="/images/linux/numaarch.jpg" alt=""></center><p>NUMA 架构在 SMP 架构的基础上通过分组的方式增强了可扩展性，但从性能上看，随着 CPU 数量的增加，并不能线性增加系统性能，原因就在于跨 Node 内存访问的问题。所以，一般 NUMA 架构最多支持几百个 CPU 就不错了。</p><p>但对于很多大型计算密集型的系统来说，NUMA 显然有些吃力，所以，后来又出现了 MPP 架构，即海量并行处理架构。这种架构也有分组的概念，但和 NUMA 不同的是，它不存在异地内存访问的问题，每个分组内的 CPU 都有自己本地的内存、IO，并且不与其他 CPU 共享，是一种完全无共享的架构，因此它的扩展性最好，可以支持多达数千个 CPU 的量级。</p><center><img src="/images/linux/mpp.jpg" alt=""></center><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>1、在芯片技术已然发展成熟的今天，性能低，上核不是问题，但上核就一定能提高性能吗，另外上核怎么很好地利用多核来完成自身进化，这些问题都值得深思。</p><p>2、NUMA 架构算是多核时代应用较大的一种 CPU 架构，本文从核心谈到系统，让大家有个全面的了解，下文会特别针对 NUMA 架构做一些实验验证。</p><p>Reference：<br>1.《深入浅出 DPDK》</p><ol start="2"><li>SMP、NUMA、MPP体系结构介绍：<br><a href="https://www.cnblogs.com/yubo/archive/2010/04/23/1718810.html" target="_blank" rel="noopener">https://www.cnblogs.com/yubo/archive/2010/04/23/1718810.html</a></li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> CPU </tag>
            
            <tag> NUMA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文掌握 Linux 性能分析之 CPU 篇</title>
      <link href="/2018/04/26/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8BCPU%E7%AF%87/"/>
      <url>/2018/04/26/tech/perf/%E4%B8%80%E6%96%87%E6%8E%8C%E6%8F%A1Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B9%8BCPU%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>平常工作会涉及到一些 Linux 性能分析的问题，因此决定总结一下常用的一些性能分析手段，仅供参考。</p><p>说到性能分析，基本上就是 CPU、内存、磁盘 IO 以及网络这几个部分，本文先来看 CPU 这个部分。</p><h3 id="1-CPU-基础信息"><a href="#1-CPU-基础信息" class="headerlink" title="1 CPU 基础信息"></a>1 CPU 基础信息</h3><hr><p>进行性能分析之前，首先得知道 CPU 有哪些信息，可以通过以下方法查看 CPU 配置信息。</p><h4 id="lscpu"><a href="#lscpu" class="headerlink" title="lscpu"></a><strong>lscpu</strong></h4><p>在 Linux 下，类似 lsxxx 这样的命令都是用来查看基本信息的，如 ls 查看当前目录文件信息，lscpu 就用来查看 CPU 信息，类似还有 lspci 查看 PCI 信息。</p><center><img src="/images/linux/lscpu.jpg" alt=""></center><p>可以看到我的机器配置很低，1 核 2.5GHz（在阿里云买的最低配的服务器）。</p><h4 id="proc-cpuinfo"><a href="#proc-cpuinfo" class="headerlink" title="/proc/cpuinfo"></a><strong>/proc/cpuinfo</strong></h4><p>/proc 目录是内核透传出来给用户态使用的，里面记录着很多信息文件，比如还有内存文件 meminfo 等。可以使用 cat /proc/cpuinfo 查看 CPU 信息。</p><center><img src="/images/linux/cpuinfo.jpg" alt=""></center><p>这里显示的信息可以具体到每个逻辑核上，由于我只有一个核，所以只显示一组信息。</p><h4 id="dmidecode"><a href="#dmidecode" class="headerlink" title="dmidecode"></a><strong>dmidecode</strong></h4><p>这个命令是用来获取 DMI（Desktop Management Interface）硬件信息的，包括 BIOS、系统、主板、处理器、内存、缓存等等。对于 CPU 信息，可以使用 dmidecode -t processor 来查看。</p><center><img src="/images/linux/dmi.jpg" alt=""></center><h3 id="2-CPU-使用情况分析"><a href="#2-CPU-使用情况分析" class="headerlink" title="2 CPU 使用情况分析"></a>2 CPU 使用情况分析</h3><hr><p>知道了 CPU 的基本信息，我们就可以使用另外的命令来对 CPU 的使用情况分析一通了。</p><h4 id="top"><a href="#top" class="headerlink" title="top"></a><strong>top</strong></h4><p>相信大家对下面这玩意不陌生，Windows 的任务管理器，top 的作用和它是一样的。</p><center><img src="/images/linux/top_w.jpg" alt=""></center><p>top 显示的效果虽说不像它这么华丽，但已然让人惊呼他俩怎么长得这么像。</p><center><img src="/images/linux/top.jpg" alt=""></center><p>我们重点关注这么几个字段：</p><ul><li>load average：三个数字分别表示最近 1 分钟，5 分钟和 15 分钟的负责，数值越大负载越重。一般要求不超过核数，比如对于单核情况要 &lt; 1。如果机器长期处于高于核数的情况，说明机器 CPU 消耗严重了。</li><li>%Cpu(s)：表示当前 CPU 的使用情况，如果要查看所有核（逻辑核）的使用情况，可以按下数字 “1” 查看。这里有几个参数，表示如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- us    用户空间占用 CPU 时间比例</span><br><span class="line">- sy    系统占用 CPU 时间比例</span><br><span class="line">- ni    用户空间改变过优先级的进程占用 CPU 时间比例</span><br><span class="line">- id    CPU 空闲时间比</span><br><span class="line">- wa    IO等待时间比（IO等待高时，可能是磁盘性能有问题了）</span><br><span class="line">- hi    硬件中断</span><br><span class="line">- si    软件中断</span><br><span class="line">- st    steal time</span><br></pre></td></tr></table></figure><p>每个进程的使用情况：这里可以罗列每个进程的使用情况，包括内存和 CPU 的，如果要看某个具体的进程，可以使用 <code>top -p pid</code> 查看。</p><p>和 top 一样的还有一个改进版的工具：htop，功能和 top 一样的，只不过比 top 表现更炫酷，使用更方便，可以看下它的效果。</p><center><img src="/images/linux/htop.jpg" alt=""></center><h4 id="ps"><a href="#ps" class="headerlink" title="ps"></a><strong>ps</strong></h4><p>可能很多人会忽略这个命令，觉得这不是查看进程状态信息的吗，其实非也，这个命令配合它的参数能显示很多功能。比如 ps aux。如果配合 watch，可以达到跟 top 一样的效果，如：<code>watch -n 1 &quot;ps aux&quot;</code>（-n 1 表示每隔 1s 更新一次）</p><center><img src="/images/linux/ps.jpg" alt=""></center><h4 id="vmstat"><a href="#vmstat" class="headerlink" title="vmstat"></a><strong>vmstat</strong></h4><p>这个命令基本能看出当前机器的运行状态和问题，非常强大。可以使用 <code>vmstat n</code> 后面跟一个数字，表示每隔 ns 显示系统的状态，信息包括 CPU、内存和 IO 等。</p><center><img src="/images/linux/vmstat.jpg" alt=""></center><p>几个关键的字段：</p><ul><li>r 值：表示在 CPU 运行队列中等待的进程数，如果这个值很大，表示很多进程在排队等待执行，CPU 压力山大。</li><li>in 和 cs 值：表示中断次数和上下文切换次数，这两个值越大，表示系统在进行大量的进程（或线程）切换。切换的开销是非常大的，这时候应该减少系统进程（或线程）数。</li><li>us、sy、id、wa 值：这些值上面也提到过，分别表示用户空间进程，系统进程，空闲和 IO 等待的 CPU 占比，这里只有 id 很高是好的，表示系统比较闲，其他值飚高都不好。</li></ul><p>这个工具强大之处在于它不仅可以分析 CPU，还可以分析内存、IO 等信息，犹如瑞士军刀。</p><h4 id="dstat"><a href="#dstat" class="headerlink" title="dstat"></a><strong>dstat</strong></h4><p>这个命令也很强大，能显示 CPU 使用情况，磁盘 IO 情况，网络发包情况和换页情况，而且输出是彩色的，可读性比较强，相对于 vmstat 更加详细和直观。使用时可以直接输入命令，也可以带相关参数。</p><center><img src="/images/linux/dstat.jpg" alt=""></center><h3 id="3-进程使用-CPU-情况分析"><a href="#3-进程使用-CPU-情况分析" class="headerlink" title="3 进程使用 CPU 情况分析"></a>3 进程使用 CPU 情况分析</h3><hr><p>上面说的是系统级的分析，现在来看单个进程的 CPU 使用情况分析，以便于我们能对占用 CPU 过多的进程进行调试和分析，优化程序性能。</p><p>其实前面 top 和 ps 这样的命令就可以看每个进程的 CPU 使用情况，但我们需要更专业的命令。</p><h4 id="pidstat"><a href="#pidstat" class="headerlink" title="pidstat"></a><strong>pidstat</strong></h4><p>这个命令默认统计系统信息，也包括 CPU、内存和 IO 等，我们常用 pidstat -u -p pid [times] 来显示 CPU 统计信息。如下统计 pid = 802 的 CPU 信息。</p><center><img src="/images/linux/pidstat.jpg" alt=""></center><h4 id="strace"><a href="#strace" class="headerlink" title="strace"></a><strong>strace</strong></h4><p>这个命令用来分析进程的系统调用情况，可以看进程都调用了哪些库和哪些系统调用，进而可以进一步优化程序。比如我们分析 ls 的系统调用情况，就可以用 strace ls：</p><center><img src="/images/linux/strace.jpg" alt=""></center><p>可以看到，一个简单的 ls 命令，其实有不少系统调用的操作。</p><p>此外，还可以 attach（附着）到一个正在运行的进程上进行分析，比如我 attach 到 802 这个进程显示：</p><center><img src="/images/linux/stracep.jpg" alt=""></center><p>根据这些输出信息，其实就能够很好地帮我们分析问题，从而定位到问题所在了。</p><p>OK，以上就是平常比较常用的一些工具，当然除了这些，还有很多很多工具，下面放一张图，来自 Linux 大牛，Netflix 高级性能架构师 Brendan Gregg。看完了，你也许会感叹“这世界太疯狂了（just crazy）”。</p><center><img src="/images/linux/tools.jpg" alt=""></center><p>Reference：<br>[1]. <a href="http://rdc.hundsun.com/portal/article/731.html" target="_blank" rel="noopener">http://rdc.hundsun.com/portal/article/731.html</a></p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> CPU </tag>
            
            <tag> 性能分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>各种容器网络方案对比</title>
      <link href="/2018/04/20/tech/cloud/container/%E5%90%84%E7%A7%8D%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%96%B9%E6%A1%88%E5%AF%B9%E6%AF%94/"/>
      <url>/2018/04/20/tech/cloud/container/%E5%90%84%E7%A7%8D%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%96%B9%E6%A1%88%E5%AF%B9%E6%AF%94/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>前面单主机容器网络和多主机容器网络两篇文章，咱们已经从原理上总结了多种容器网络方案，也通过这篇文章探讨了容器网络的背后原理。本文再基于一个宏观的视角，对比几种网络方案，让大家有个完整的认识。</p><p>单主机网络就不多说了，因为也比较简单，我们重点对比几种多主机网络方案。</p><p>对比的维度有以下几种：网络模型，IP 地址池管理（IP Address Management，IPAM），服务发现，连通与隔离，性能。</p><p>网络模型指的是构成跨主机通信的网络结构和实现技术，比如是纯二层转发，还是纯三层转发；是 overlay 网络还是 underlay 网络等等。</p><p>IPAM 指的是如何管理容器网络的 IP 池。当容器集群比较大，管理的主机比较多的时候，如何分配各个主机上容器的 IP 是一个比较棘手的问题。Docker 网络有个 subnet 的概念，通常一个主机分配一个 subnet，但也有多个主机共用一个 subnet 的情况，具体的网络方案有不同的考量。具体看下面的表格总结。</p><p>服务发现本质上是一个分布式的 key-value 存储系统，用于跨主机通信时保存并同步各主机的网络信息，便于快速建立起各主机之间的网络连接。由于各网络方案实现上各有千秋，并不是所有的跨主机网络方案都要依据服务发现。</p><p>连通与隔离指的是容器跨主机之间是否能够互相通信，以及容器与外网（外网不一定指 Internet）之间如何通信。</p><p>性能具体指的是通信的时延，我们仅从各个网络方案的原理上来分析得出结论，所以这里的结论并不一定正确，因为不同的部署环境会对性能有一些影响，建议大家还是根据自己的环境动手实验验证为妙。</p><p>从原理上说，underlay 网络性能要优于 overlay 网络，因为 overlay 网络存在封包和拆包操作，存在额外的 CPU 和网络开销，所以，几种方案中，macvlan、flannel host-gw、calico 的性能会优于 overlay、flannel vxlan 和 weave。但是这个也不能作为最终生产环境采用的标准，因为 overlay 网络采用 vxlan 作为隧道的话，能支持更多的二层网段，安全性也更高，所以，需要综合考虑。</p><p>通过以上分析，我们可以得出以下的结论：</p><center><img src="/images/docker/docker_net_com.jpg" alt=""></center><p>参考：<br><a href="http://www.cnblogs.com/CloudMan6/p/7587532.html" target="_blank" rel="noopener">http://www.cnblogs.com/CloudMan6/p/7587532.html</a></p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
            <tag> 容器网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>那么多容器网络的解决方案，其背后的原理到底是什么？</title>
      <link href="/2018/04/18/tech/cloud/container/%E9%82%A3%E4%B9%88%E5%A4%9A%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%8C%E5%85%B6%E8%83%8C%E5%90%8E%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/"/>
      <url>/2018/04/18/tech/cloud/container/%E9%82%A3%E4%B9%88%E5%A4%9A%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%8C%E5%85%B6%E8%83%8C%E5%90%8E%E7%9A%84%E5%8E%9F%E7%90%86%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>知其然而不知其所以然，不知也。老古人说得多好，学知识不懂得知识背后的原理，等于白学。</p><p>通过前面两篇文章，我们知道了容器的单主机网络和多主机网络，对于这么多网络方案，我们看到对 Docker 的整体网络结构好像没有改动，都是水平扩展的，那 Docker 网络究竟是怎么集成这么多网络方案而不改变自身原有的结构呢？本文就来一探究竟。</p><h3 id="1-Docker-的总体框架"><a href="#1-Docker-的总体框架" class="headerlink" title="1 Docker 的总体框架"></a>1 Docker 的总体框架</h3><hr><p>要回答这个问题，得从 Docker 的总体框架说起。</p><center><img src="/images/docker/docker_netarch.jpg" alt=""></center><p>容器和虚拟机一样，都是虚拟化的产品，都包括计算虚拟化，存储虚拟化和 IO 虚拟化。容器作为轻量级的进程，不像虚拟机那般复杂，这三块分别靠三个 Driver 来完成的，execdriver 负责计算虚拟化，networkdriver 负责网络虚拟化，graphdriver 负责存储虚拟化。由此可见，Docker 靠 Driver 这种设计思想来支撑起它的基础平台，再往深了挖，它的每个子模块都随处可见这种设计思想，就网络这个子模块来看，也是如此。</p><h3 id="2-Docker-的网络模型"><a href="#2-Docker-的网络模型" class="headerlink" title="2 Docker 的网络模型"></a>2 Docker 的网络模型</h3><hr><h4 id="docker-engine-libcontainer"><a href="#docker-engine-libcontainer" class="headerlink" title="docker engine + libcontainer"></a>docker engine + libcontainer</h4><hr><p>期初的 Docker 网络子模块的代码是分散在 docker daemon 和 libcontainer 中的，libcontainer 是一个独立的容器管理包，execdriver 和 networkdriver 都是通过 libcontainer 来实现对容器的具体操作。</p><p>随着业务场景越来越复杂，这种内嵌的方式很难针对不同的网络场景进行扩展。后来，Docker 收购了一个做多主机网络解决方案的公司 SocketPlane，然后让那帮人专门来解决这个问题。这就是接下来要介绍的 libnetwork。</p><h4 id="libnetwork-amp-amp-CNM"><a href="#libnetwork-amp-amp-CNM" class="headerlink" title="libnetwork &amp;&amp; CNM"></a>libnetwork &amp;&amp; CNM</h4><hr><p>libnetwork 起初的做法很简单，就是将 docker engine 和 libcontainer 中网络相关的代码抽出来，合并成一个单独的库，做成网络抽象层，并对外提供 API。Docker 的愿景就是希望 libnetwork 能够做像 libcontainer 那样，成为一个多平台的容器网络基础包。</p><p>后来受一个 GitHub issue ( <a href="https://github.com/moby/moby/issues/9983" target="_blank" rel="noopener">https://github.com/moby/moby/issues/9983</a>) 的启发，libnetwork 引入容器网络模型（Container Network Model，CNM），该模型进一步对 Docker 的网络结构进行了细分，提出了三个概念：network、sandbox 和 endpoint。</p><h5 id="network"><a href="#network" class="headerlink" title="network"></a><strong>network</strong></h5><p>network 是一个抽象的概念，你可以把它理解成一个网络的插件，或者是网络的 Driver，比如说单主机网络的 Driver 就有 none、host、bridge，joined container 这四种，多主机网络就有 overlay、macvlan、flannel 这些。network 可以独立出去做，只需调用 Docker 对外提供的 API 就可以作为插件集成到 Docker 网络中使用。</p><h5 id="sandbox"><a href="#sandbox" class="headerlink" title="sandbox"></a><strong>sandbox</strong></h5><p>sandbox 实现了容器内部的网络栈，它定义容器的虚拟网卡，路由表和 DNS 等配置，其实就是一个标准的 linux network namespace 实现。</p><h5 id="endpoint"><a href="#endpoint" class="headerlink" title="endpoint"></a><strong>endpoint</strong></h5><p>network 实现了一个第三方的网络栈，sandbox 则实现了容器内部的网络栈，那这两者怎么联系起来呢？答案就是通过 endpoint，endpoint 实现了 veth pair，一个 endpoint 就表示一对 veth pair，一端挂在容器中，另一端挂在 network 中。</p><center><img src="/images/docker/cnm.jpg" alt=""></center><p>network、sandbox 和 endpoint 三者之间的关系：<br>一个 network 可以包含多个 endpoint，自然也就包含多个 sandbox。<br>一个 sandbox 可以包含多个 endpoint，可以属于多个 network。<br>一个 endpoint 只可以属于一个 network，并且只属于一个 sandbox。</p><p>如上图显示三个容器，每个容器一个 sandbox，除了第二个容器有两个 endpoint 分别接入 network1 和 network2 之外，其余 sandbox 都只有一个 endpoint 分别接入不同的 network。</p><p>到此，我们就可以解答文章开篇提到的问题，“不同的网络方案如何集成到 Docker 网络模型中而不改变原有结构？”</p><p>答案就是基于 libnetwork CNM，将各个网络模型以插件或 Driver 的形式集成到 Docker 网络中来，与 docker daemon 协同工作，实现容器网络。Docker 原生的 Driver 包括单主机的 none、bridge、joined container 和 多主机的 overlay、macvlan，第三方 Driver 就包括多主机的 flannel、weave、calico 等。</p><center><img src="/images/docker/libnetwork.png" alt=""></center><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h3><hr><ol><li>libnetwork 基于 CNM 模型将 Docker 网络结构从原生方案中抽离出来，增强了网络扩展性，以至于现在各种网络方案层出不穷，都可以轻松集成到 Docker 中。</li><li>network，sandbox，endpoint 三个概念。</li></ol><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
            <tag> 容器网络 </tag>
            
            <tag> libnetwork </tag>
            
            <tag> CNM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>容器网络之多主机网络</title>
      <link href="/2018/04/16/tech/cloud/container/docker/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
      <url>/2018/04/16/tech/cloud/container/docker/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>上篇文章介绍了容器网络的单主机网络，本文将进一步介绍多主机网络，也就是跨主机的网络。总结下来，多主机网络解决方案包括但不限于以下几种：overlay、macvlan、flannel、weave、cacico 等，下面将分别一一介绍这几种网络，</p><p>PS：本文仅从原理上对几种网络进行简单的对比总结，不涉及太多的细节。</p><h3 id="1-overlay"><a href="#1-overlay" class="headerlink" title="1 overlay"></a>1 overlay</h3><hr><p>俗称隧道网络，它是基于 VxLAN 协议来将二层数据包封装到 UDP 中进行传输的，目的是扩展二层网段，因为 VLAN 使用 12bit 标记 VLAN ID，最多支持 4094 个 VLAN，这对于大型云网络会成为瓶颈，而 VxLAN ID 使用 24bit 来标记，支持多达 16777216 个二层网段，所以 VxLAN 是扩展了 VLAN，也叫做大二层网络。</p><p>overlay 网络需要一个全局的“上帝”来记录它网络中的信息，比如主机地址，子网等，这个上帝在 Docker 中是由服务发现协议来完成的，服务发现本质上是一个 key-value 数据库，要使用它，首先需要向它告知（注册）一些必要的信息（如网络中需要通信的主机），然后它就会自动去收集、同步网络的信息，同时，还会维护一个 IP 地址池，分配给主机中的容器使用。Docker 中比较有名的服务发现有 Consul、Etcd 和 ZooKeeper。overlay 网络常用 Consul。</p><center><img src="/images/docker/overlay.jpg" alt=""></center><p>创建 overlay 网络会创建一个 Linux bridge br0，br0 会创建两个接口，一个 veth2 作为与容器的虚拟网卡相连的 veth pair，另一个 vxlan1 负责与其他 host 建立 VxLAN 隧道，跨主机的容器就通过这个隧道来进行通信。</p><p>为了保证 overlay 网络中的容器与外网互通，Docker 会创建另一个 Linux bridge docker_gwbridge，同样，该 bridge 也存在一对 veth pair，要与外围通信的容器可以通过这对 veth pair 到达 docker_gwbridge，进而通过主机 NAT 访问外网。</p><h3 id="2-macvlan"><a href="#2-macvlan" class="headerlink" title="2 macvlan"></a>2 macvlan</h3><hr><p>macvlan 就如它的名字一样，是一种网卡虚拟化技术，它能够将一个物理网卡虚拟出多个接口，每个接口都可以配置 MAC 地址，同样每个接口也可以配自己的 IP，每个接口就像交换机的端口一样，可以为它划分 VLAN。</p><p>macvlan 的做法其实就是将这些虚拟出来的接口与 Docker 容器直连来达到通信的目的。一个 macvlan 网络对应一个接口，不同的 macvlan 网络分配不同的子网，因此，相同的 macvlan 之间可以互相通信，不同的 macvlan 网络之间在二层上不能通信，需要借助三层的路由器才能完成通信，如下，显示的就是两个不同的 macvlan 网络之间的通信流程。</p><center><img src="/images/docker/macvlan.jpg" alt=""></center><p>我们用一个 Linux 主机，通过配置其路由表和 iptables，将其配成一个路由器（当然是虚拟的），就可以完成不同 macvlan 网络之间的数据交换，当然用物理路由器也是没毛病的。</p><h3 id="3-flannel"><a href="#3-flannel" class="headerlink" title="3 flannel"></a>3 flannel</h3><hr><p>flannel 网络也需要借助一个全局的上帝来同步网络信息，一般使用的是 etcd。</p><p>flannel 网络不会创建新的 bridge，而是用默认的 docker0，但创建 flannel 网络会在主机上创建一个虚拟网卡，挂在 docker0 上，用于跨主机通信。</p><center><img src="/images/docker/flannel.jpg" alt=""></center><p>组件方式让 flannel 多了几分灵活性，它可以使用二层的 VxLAN 隧道来封装数据包完成跨主机通信，也可以使用纯三层的方案来通信，比如 host-gw，只需修改一个配置文件就可以完成转化。</p><h3 id="4-weave"><a href="#4-weave" class="headerlink" title="4 weave"></a>4 weave</h3><hr><p>weave 网络没有借助服务发现协议，也没有 macvlan 那样的虚拟化技术，只需要在不同主机上启动 weave 组件就可以完成通信。</p><p>创建 weave 网络会创建两个网桥，一个是 Linux bridge weave，一个是 datapath，也就是 OVS，weave 负责将容器加入 weave 网络中，OVS 负责将跨主机通信的数据包封装成 VxLAN 包进行隧道传输。</p><center><img src="/images/docker/weave.jpg" alt=""></center><p>同样，weave 网络也不支持与外网通信，Docker 提供 docker0 来满足这个需求。</p><p>weave 网络通过组件化的方式使得网络分层比较清晰，两个网桥的分工也比较明确，一个用于跨主机通信，相当于一个路由器，一个负责将本地网络加入 weave 网络。</p><h3 id="5-calico"><a href="#5-calico" class="headerlink" title="5 calico"></a>5 calico</h3><hr><p>calico 是一个纯三层的网络，它没有创建任何的网桥，它之所以能完成跨主机的通信，是因为它记住 etcd 将网络中各网段的路由信息写进了主机中，然后创建的一对的 veth pair，一块留在容器的 network namespace 中，一块成了主机中的虚拟网卡，加入到主机路由表中，从而打通不同主机中的容器通信。</p><center><img src="/images/docker/calico.jpg" alt=""></center><p>calico 相较其他几个网络方案最大优点是它提供 policy 机制，用户可以根据自己的需求自定义 policy，一个 policy 可能对应一条 ACL，用于控制进出容器的数据包，比如我们建立了多个 calico 网络，想控制其中几个网络可以互通，其余不能互通，就可以修改 policy 的配置文件来满足要求，这种方式大大增加了网络连通和隔离的灵活性。</p><h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h3><hr><p>1、除了以上的几种方案，跨主机容器网络方案还有很多，比如：Romana，Contiv 等，本文就不作过多展开了，大家感兴趣可以查阅相关资料了解。</p><p>2、跨主机的容器网络通常要为不同主机的容器维护一个 IP 池，所以大多方案需要借助第三方的服务发现方案。</p><p>3、跨主机容器网络按传输方式可以分为纯二层网络，隧道网络（大二层网络），以及纯三层网络。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
            <tag> 容器网络 </tag>
            
            <tag> macvlan </tag>
            
            <tag> overlay </tag>
            
            <tag> flannel </tag>
            
            <tag> weave </tag>
            
            <tag> cacico </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>virtio-user 简介</title>
      <link href="/2018/04/11/tech/cloud/virt/virtio_user_%E7%AE%80%E4%BB%8B/"/>
      <url>/2018/04/11/tech/cloud/virt/virtio_user_%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>在看本文之前，建议先看 virtio 简介，vhost 简介，vhost-user 简介。</p><p>virtio-user 是 DPDK 针对特定场景提出的一种解决方案，它主要有两种场景的用途，一种是用于 DPDK 应用容器对 virtio 的支持，这是 DPDK v16.07 开始支持的；另一种是用于和内核通信，这是 DPDK v17.02 推出的。</p><h3 id="1-virtio-user-用于容器网络"><a href="#1-virtio-user-用于容器网络" class="headerlink" title="1 virtio_user 用于容器网络"></a>1 virtio_user 用于容器网络</h3><hr><p>我们知道，对于虚拟机，有 virtio 这套半虚拟化的标准协议来指导虚拟机和宿主机之间的通信，但对于容器的环境，直接沿用 virtio 是不行的，原因是虚拟机是通过 Qemu 来模拟的，Qemu 会将它虚拟出的整个 KVM 虚拟机的信息共享给宿主机，但对于 DPDK 加速的容器化环境来说显然是不合理的。因为 DPDK 容器与宿主机的通信只用得到虚拟内存中的大页内存部分，其他都是用不到的，全部共享也没有任何意义，DPDK 主要基于大页内存来收发数据包的。</p><p>所以，virtio_user 其实就是在 virtio PMD 的基础上进行了少量修改形成的，简单来说，就是添加大页共享的部分逻辑，并精简了整块共享内存部分的逻辑。</p><p>有兴趣可以对照 /driver/net/virtio 中的代码和 DPDK virtio_user 代码，其实大部分是相同的。</p><p>从 DPDK 的角度看，virtio_user 是作为一个虚拟设备（vdev）来加载的，它充当的是一个 virtio 前端驱动，与之对应的后端通信驱动，是用户态的 vhost_user，在使用的时候，我们只需要定义好相应的适配接口即可，如下：</p><center><img src="/images/docker/virtio_user.jpg" alt=""></center><p>vhost 和 vhost_user 本质上是采用共享内存的 IPC 方式，通过在 host 端创建 vhost_user 共享内存文件，然后 virtio_user 启动的时候指定该文件即可，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1）首先创建 vhost_user 共享内存文件</span><br><span class="line">--vdev &apos;eth_vhost_user0,iface=/tmp/vhost_user0&apos;</span><br><span class="line">2）启动 virtio_user 指定文件路径</span><br><span class="line">--vdev=virtio_user0,path=/tmp/vhost_user0</span><br></pre></td></tr></table></figure></p><h3 id="2-virtio-user-作为-exception-path-用于与内核通信"><a href="#2-virtio-user-作为-exception-path-用于与内核通信" class="headerlink" title="2 virtio_user 作为 exception path 用于与内核通信"></a>2 virtio_user 作为 exception path 用于与内核通信</h3><hr><p>virtio_user 的一个用途就是作为 exception path 用于与内核通信。我们知道，DPDK 是旁路内核的转包方案，这也是它高性能的原因，但有些时候从 DPDK 收到的包（如控制报文）需要丢到内核网络协议栈去做进一步的处理，这个路径在 DPDK 中就被称为 exception path。</p><p>在这之前，已经存在几种 exception path 的方案，如传统的 Tun/Tap，KNI（Kernel NIC Interface），AF_Packet 以及基于 SR-IOV 的 Flow Bifurcation。这些方案就不做过多介绍了，感兴趣的可看 DPDK 官网，上面都有介绍。</p><center><img src="/images/docker/virtio_user1.jpg" alt=""></center><p>和容器网络的方案使用 vhost_user 作为后端驱动一样，要使得 virtio_user 和内核通信，只需加载内核模块 vhost.ko，让它充当的是 virtio_user 的后端通信驱动即可。</p><p>所以，我们可以看到，其实这两种方案本质上是一样，只是换了个后端驱动而已，这也是 virtio 的优势所在，定义一套通用的接口标准，需要什么类型的通信方式只需加载相应驱动即可，改动非常少，扩展性非常高。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> DPDK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
            <tag> 容器网络 </tag>
            
            <tag> DPDK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DPDK 入门最佳指南</title>
      <link href="/2018/04/10/tech/net/dpdk/DPDK_%E5%85%A5%E9%97%A8%E6%9C%80%E4%BD%B3%E6%8C%87%E5%8D%97/"/>
      <url>/2018/04/10/tech/net/dpdk/DPDK_%E5%85%A5%E9%97%A8%E6%9C%80%E4%BD%B3%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="01-写在前面"><a href="#01-写在前面" class="headerlink" title="01 写在前面"></a>01 写在前面</h3><hr><p>我的读者当中应该有一部分人是做 DPDK 相关的，我自己虽然现在已经不做 DPDK 了，但对这块仍然有兴趣，今天这篇文章就来总结下 DPDK 的技术栈。注意：这篇文章是小白文，不适合大神哦。</p><p>文章从 DPDK 的产生背景，到核心技术，再到应用场景，都进行了阐述，有可能是你见过的讲得最全面的文章了，当然，讲得全面自然会少了深度，你如果不屑忽略就好了。</p><p>其实，本文之前已经发过，但在整理的时候不小心删了，索性就重发一次，但这一次多了一些内容，文末我还会推荐一些学习资料，有需要的可以拉到最下面查看获取方法。</p><h3 id="02-高性能网络技术"><a href="#02-高性能网络技术" class="headerlink" title="02 高性能网络技术"></a>02 高性能网络技术</h3><hr><p>随着云计算产业的异军突起，网络技术的不断创新，越来越多的网络设备基础架构逐步向基于通用处理器平台的架构方向融合，从传统的物理网络到虚拟网络，从扁平化的网络结构到基于 SDN 分层的网络结构，无不体现出这种创新与融合。</p><p>这在使得网络变得更加可控制和成本更低的同时，也能够支持大规模用户或应用程序的性能需求，以及海量数据的处理。究其原因，其实是高性能网络编程技术随着网络架构的演进不断突破的一种必然结果。</p><h3 id="03-C10K-到-C10M-问题的演进"><a href="#03-C10K-到-C10M-问题的演进" class="headerlink" title="03 C10K 到 C10M 问题的演进"></a>03 C10K 到 C10M 问题的演进</h3><hr><p>说到高性能网络编程，一定逃不过 C10K 问题（即单机 1 万个并发连接问题），不过这个问题已经成为历史了，很多技术可以解决它，如常用的 I/O 多路复用模型，select, poll, epoll 等。在此基础上也出了很多优秀的框架，比如 Nginx 基于事件驱动的 Web 服务框架，以及基于 Python 开发的 Tornado 和 Django 这种非阻塞的 Web 框架。</p><p>如今，关注的更多是 C10M 问题（即单机 1 千万个并发连接问题）。很多计算机领域的大佬们从硬件上和软件上都提出了多种解决方案。从硬件上，比如说，现在的类似很多 40Gpbs、32-cores、256G RAM 这样配置的 X86 服务器完全可以处理 1 千万个以上的并发连接。</p><p>但是从硬件上解决问题就没多大意思了，首先它成本高，其次不通用，最后也没什么挑战，无非就是堆砌硬件而已。所以，抛开硬件不谈，我们看看从软件上该如何解决这个世界难题呢？</p><p>这里不得不提一个人，就是 Errata Security 公司的 CEO Robert Graham，他在 Shmoocon 2013 大会上很巧妙地解释了这个问题。有兴趣可以查看其 YouTube 的演进视频： C10M Defending The Internet At Scale。</p><center><img src="/images/sdn/c10m.jpg" alt=""></center><p>他提到了 UNIX 的设计初衷其实为电话网络的控制系统而设计的，而不是一般的服务器操作系统，所以，它仅仅是一个负责数据传送的系统，没有所谓的控制层面和数据层面的说法，不适合处理大规模的网络数据包。最后他得出的结论是：</p><blockquote><p>OS 的内核不是解决 C10M 问题的办法，恰恰相反 OS 的内核正式导致 C10M 问题的关键所在。</p></blockquote><h3 id="04-为什么这么说？基于-OS-内核的数据传输有什么弊端？"><a href="#04-为什么这么说？基于-OS-内核的数据传输有什么弊端？" class="headerlink" title="04 为什么这么说？基于 OS 内核的数据传输有什么弊端？"></a>04 为什么这么说？基于 OS 内核的数据传输有什么弊端？</h3><hr><p><strong>1、中断处理：</strong> 当网络中大量数据包到来时，会产生频繁的硬件中断请求，这些硬件中断可以打断之前较低优先级的软中断或者系统调用的执行过程，如果这种打断频繁的话，将会产生较高的性能开销。</p><p><strong>2、内存拷贝：</strong> 正常情况下，一个网络数据包从网卡到应用程序需要经过如下的过程：数据从网卡通过 DMA 等方式传到内核开辟的缓冲区，然后从内核空间拷贝到用户态空间，在 Linux 内核协议栈中，这个耗时操作甚至占到了数据包整个处理流程的 57.1%。</p><p><strong>3、上下文切换：</strong> 频繁到达的硬件中断和软中断都可能随时抢占系统调用的运行，这会产生大量的上下文切换开销。另外，在基于多线程的服务器设计框架中，线程间的调度也会产生频繁的上下文切换开销，同样，锁竞争的耗能也是一个非常严重的问题。</p><p><strong>4、局部性失效：</strong> 如今主流的处理器都是多个核心的，这意味着一个数据包的处理可能跨多个 CPU 核心，比如一个数据包可能中断在 cpu0，内核态处理在 cpu1，用户态处理在 cpu2，这样跨多个核心，容易造成 CPU 缓存失效，造成局部性失效。如果是 NUMA 架构，更会造成跨 NUMA 访问内存，性能受到很大影响。</p><p><strong>5、内存管理：</strong> 传统服务器内存页为 4K，为了提高内存的访问速度，避免 cache miss，可以增加 cache 中映射表的条目，但这又会影响 CPU 的检索效率。</p><p>综合以上问题，可以看出内核本身就是一个非常大的瓶颈所在。那很明显解决方案就是想办法绕过内核。</p><h3 id="05-解决方案探讨"><a href="#05-解决方案探讨" class="headerlink" title="05 解决方案探讨"></a>05 解决方案探讨</h3><hr><p>针对以上弊端，分别提出以下技术点进行探讨。</p><p><strong>1、控制层和数据层分离：</strong> 将数据包处理、内存管理、处理器调度等任务转移到用户空间去完成，而内核仅仅负责部分控制指令的处理。这样就不存在上述所说的系统中断、上下文切换、系统调用、系统调度等等问题。</p><p><strong>2、多核技术：</strong> 使用多核编程技术代替多线程技术，并设置 CPU 的亲和性，将线程和 CPU 核进行一比一绑定，减少彼此之间调度切换。</p><p><strong>3、NUMA 亲和性：</strong> 针对 NUMA 系统，尽量使 CPU 核使用所在 NUMA 节点的内存，避免跨内存访问。</p><p><strong>4、大页内存：</strong> 使用大页内存代替普通的内存，减少 cache-miss。</p><p><strong>5、无锁技术：</strong> 采用无锁技术解决资源竞争问题。</p><p>经研究，目前业内已经出现了很多优秀的集成了上述技术方案的高性能网络数据处理框架，如 6wind、Windriver、Netmap、DPDK 等，其中，Intel 的 DPDK 在众多方案脱颖而出，一骑绝尘。</p><center><img src="/images/sdn/dpdk.png" alt=""></center><p>DPDK 为 Intel 处理器架构下用户空间高效的数据包处理提供了库函数和驱动的支持，它不同于 Linux 系统以通用性设计为目的，而是专注于网络应用中数据包的高性能处理。</p><p>也就是 DPDK 绕过了 Linux 内核协议栈对数据包的处理过程，在用户空间实现了一套数据平面来进行数据包的收发与处理。在内核看来，DPDK 就是一个普通的用户态进程，它的编译、连接和加载方式和普通程序没有什么两样。</p><h3 id="06-DPDK-的突破"><a href="#06-DPDK-的突破" class="headerlink" title="06 DPDK 的突破"></a>06 DPDK 的突破</h3><hr><p>相对传统的基于内核的网络数据处理，DPDK 对从内核层到用户层的网络数据流程进行了重大突破，我们先看看传统的数据流程和 DPDK 中的网络流程有什么不同。</p><p><strong>传统 Linux 内核网络数据流程：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">硬件中断---&gt;取包分发至内核线程---&gt;软件中断---&gt;内核线程在协议栈中处理包---&gt;处理完毕通知用户层</span><br><span class="line">用户层收包--&gt;网络层---&gt;逻辑层---&gt;业务层</span><br></pre></td></tr></table></figure></p><p><strong>dpdk 网络数据流程：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">硬件中断---&gt;放弃中断流程</span><br><span class="line">用户层通过设备映射取包---&gt;进入用户层协议栈---&gt;逻辑层---&gt;业务层</span><br></pre></td></tr></table></figure></p><p>下面就具体看看 DPDK 做了哪些突破？</p><h4 id="6-1-UIO-（用户空间的-I-O-技术）的加持。"><a href="#6-1-UIO-（用户空间的-I-O-技术）的加持。" class="headerlink" title="6.1 UIO （用户空间的 I/O 技术）的加持。"></a><strong>6.1 UIO （用户空间的 I/O 技术）的加持。</strong></h4><hr><p>DPDK 能够绕过内核协议栈，本质上是得益于 UIO 技术，通过 UIO 能够拦截中断，并重设中断回调行为，从而绕过内核协议栈后续的处理流程。</p><p>UIO 设备的实现机制其实是对用户空间暴露文件接口，比如当注册一个 UIO 设备 uioX，就会出现文件 /dev/uioX，对该文件的读写就是对设备内存的读写。除此之外，对设备的控制还可以通过 /sys/class/uio 下的各个文件的读写来完成。</p><center><img src="/images/sdn/uio.jpg" alt=""></center><h4 id="6-2-内存池技术"><a href="#6-2-内存池技术" class="headerlink" title="6.2 内存池技术"></a><strong>6.2 内存池技术</strong></h4><hr><p>DPDK 在用户空间实现了一套精巧的内存池技术，内核空间和用户空间的内存交互不进行拷贝，只做控制权转移。这样，当收发数据包时，就减少了内存拷贝的开销。</p><h4 id="6-3-大页内存管理"><a href="#6-3-大页内存管理" class="headerlink" title="6.3 大页内存管理"></a><strong>6.3 大页内存管理</strong></h4><hr><p>DPDK 实现了一组大页内存分配、使用和释放的 API，上层应用可以很方便使用 API 申请使用大页内存，同时也兼容普通的内存申请。</p><h4 id="6-4-无锁环形队列"><a href="#6-4-无锁环形队列" class="headerlink" title="6.4 无锁环形队列"></a><strong>6.4 无锁环形队列</strong></h4><hr><p>DPDK 基于 Linux 内核的无锁环形缓冲 kfifo 实现了自己的一套无锁机制。支持单生产者入列/单消费者出列和多生产者入列/多消费者出列操作，在数据传输的时候，降低性能的同时还能保证数据的同步。</p><h4 id="6-5-poll-mode-网卡驱动"><a href="#6-5-poll-mode-网卡驱动" class="headerlink" title="6.5 poll-mode 网卡驱动"></a><strong>6.5 poll-mode 网卡驱动</strong></h4><hr><p>DPDK 网卡驱动完全抛弃中断模式，基于轮询方式收包，避免了中断开销。</p><h4 id="6-6-NUMA"><a href="#6-6-NUMA" class="headerlink" title="6.6 NUMA"></a><strong>6.6 NUMA</strong></h4><hr><p>DPDK 内存分配上通过 proc 提供的内存信息，使 CPU 核心尽量使用靠近其所在节点的内存，避免了跨 NUMA 节点远程访问内存的性能问题。</p><h4 id="6-7-CPU-亲和性"><a href="#6-7-CPU-亲和性" class="headerlink" title="6.7 CPU 亲和性"></a><strong>6.7 CPU 亲和性</strong></h4><hr><p>DPDK 利用 CPU 的亲和性将一个线程或多个线程绑定到一个或多个 CPU 上，这样在线程执行过程中，就不会被随意调度，一方面减少了线程间的频繁切换带来的开销，另一方面避免了 CPU 缓存的局部失效性，增加了 CPU 缓存的命中率。</p><h4 id="6-8-多核调度框架"><a href="#6-8-多核调度框架" class="headerlink" title="6.8 多核调度框架"></a><strong>6.8 多核调度框架</strong></h4><hr><p>DPDK 基于多核架构，一般会有主从核之分，主核负责完成各个模块的初始化，从核负责具体的业务处理。</p><p>除了上述之外，DPDK 还有很多的技术突破，可以用下面这张图来概之。</p><center><img src="/images/sdn/dpdkarch.jpg" alt=""></center><h3 id="07-DPDK-的应用"><a href="#07-DPDK-的应用" class="headerlink" title="07 DPDK 的应用"></a>07 DPDK 的应用</h3><hr><p>DPDK 作为优秀的用户空间高性能数据包加速套件，现在已经作为一个“胶水”模块被用在多个网络数据处理方案中，用来提高性能。如下是众多的应用。</p><center><img src="/images/sdn/dpdkapp.jpg" alt=""></center><h4 id="数据面（虚拟交换机）"><a href="#数据面（虚拟交换机）" class="headerlink" title="数据面（虚拟交换机）"></a><strong>数据面（虚拟交换机）</strong></h4><hr><p><strong>OVS</strong></p><p>Open vSwitch 是一个多核虚拟交换机平台，支持标准的管理接口和开放可扩展的可编程接口，支持第三方的控制接入。</p><p><a href="https://github.com/openvswitch/ovs" target="_blank" rel="noopener">https://github.com/openvswitch/ovs</a></p><p><strong>VPP</strong></p><p>VPP 是 cisco 开源的一个高性能的包处理框架，提供了 交换/路由 功能，在虚拟化环境中，使它可以当做一个虚拟交换机来使用。在一个类 SDN 的处理框架中，它往往充当数据面的角色。经研究表明，VPP 性能要好于 OVS+DPDK 的组合，但它更适用于 NFV，适合做特定功能的网络模块。</p><p><a href="https://wiki.fd.io/view/VPP" target="_blank" rel="noopener">https://wiki.fd.io/view/VPP</a></p><p><strong>Lagopus</strong></p><p>Lagopus 是另一个多核虚拟交换的实现，功能和 OVS 差不多，支持多种网络协议，如 Ethernet，VLAN，QinQ，MAC-in-MAC，MPLS 和 PBB，以及隧道协议，如 GRE，VxLan 和 GTP。</p><p><a href="https://github.com/lagopus/lagopus/blob/master/QUICKSTART.md" target="_blank" rel="noopener">https://github.com/lagopus/lagopus/blob/master/QUICKSTART.md</a></p><p><strong>Snabb</strong></p><p>Snabb 是一个简单且快速的数据包处理工具箱。</p><p><a href="https://github.com/SnabbCo/snabbswitch/blob/master/README.md" target="_blank" rel="noopener">https://github.com/SnabbCo/snabbswitch/blob/master/README.md</a></p><h4 id="数据面（虚拟路由器）"><a href="#数据面（虚拟路由器）" class="headerlink" title="数据面（虚拟路由器）"></a><strong>数据面（虚拟路由器）</strong></h4><hr><p><strong>OPENCONTRAIL</strong></p><p>一个集成了 SDN 控制器的虚拟路由器，现在多用在 OpenStack 中，结合 Neutron 为 OpenStack 提供一站式的网络支持。</p><p><a href="http://www.opencontrail.org/" target="_blank" rel="noopener">http://www.opencontrail.org/</a></p><p><strong>CloudRouter</strong></p><p>一个分布式的路由器。</p><p><a href="https://cloudrouter.org/" target="_blank" rel="noopener">https://cloudrouter.org/</a></p><h4 id="用户空间协议栈"><a href="#用户空间协议栈" class="headerlink" title="用户空间协议栈"></a><strong>用户空间协议栈</strong></h4><hr><p><strong>mTCP</strong></p><p>mTCP 是一个针对多核系统的高可扩展性的用户空间 TCP/IP 协议栈。</p><p><a href="https://github.com/eunyoung14/mtcp/blob/master/README" target="_blank" rel="noopener">https://github.com/eunyoung14/mtcp/blob/master/README</a></p><p><strong>IwIP</strong></p><p>IwIP 针对 RAM 平台的精简版的 TCP/IP 协议栈实现。</p><p><a href="http://git.savannah.gnu.org/cgit/lwip.git/tree/README" target="_blank" rel="noopener">http://git.savannah.gnu.org/cgit/lwip.git/tree/README</a></p><p><strong>Seastar</strong></p><p>Seastar 是一个开源的，基于 C++ 11/14 feature，支持高并发和低延迟的异步编程高性能库。</p><p><a href="http://www.seastar-project.org/" target="_blank" rel="noopener">http://www.seastar-project.org/</a></p><p><strong>f-stack</strong></p><p>腾讯开源的用户空间协议栈，移植于 FreeBSD协议栈，粘合了 POSIX API，上层应用（协程框架，Nginx,Redis），纯 C 编写，易上手。</p><p><a href="https://github.com/f-stack/f-stack" target="_blank" rel="noopener">https://github.com/f-stack/f-stack</a></p><h4 id="存储加速"><a href="#存储加速" class="headerlink" title="存储加速"></a><strong>存储加速</strong></h4><hr><p><strong>SPDK</strong></p><p>SPDK 是 DPDK 的孪生兄弟，专注存储性能加速，目前的火热程度丝毫不亚于 DPDK，Intel 近来对 SPDK 特别重视，隔三差五就发布新版本。</p><p><a href="https://github.com/spdk/spdk" target="_blank" rel="noopener">https://github.com/spdk/spdk</a></p><h3 id="08-总结"><a href="#08-总结" class="headerlink" title="08 总结"></a>08 总结</h3><hr><p>DPDK 绕过了 Linux 内核协议栈，加速数据的处理，用户可以在用户空间定制协议栈，满足自己的应用需求，目前出现了很多基于 DPDK 的高性能网络框架，OVS 和 VPP 是常用的数据面框架，mTCP 和 f-stack 是常用的用户态协议栈，SPDK 是存储性能加速器，很多大公司都在使用 DPDK 来优化网络性能。</p><blockquote><p>PS：本文所有的图来自网络，侵权必删。</p></blockquote><h3 id="09-DPDK-资料推荐"><a href="#09-DPDK-资料推荐" class="headerlink" title="09 DPDK 资料推荐"></a>09 DPDK 资料推荐</h3><hr><p>在我看来，DPDK 最好的学习资料是官网，没有之一：</p><p><a href="http://core.dpdk.org/doc/" target="_blank" rel="noopener">http://core.dpdk.org/doc/</a></p><p>其次是看 Intel 技术专家出的书 《深入浅出 DPDK》。</p><center><img src="/images/sdn/dpdkbook.jpg" alt=""></center><p>本书详细介绍了DPDK 技术发展趋势，数据包处理，硬件加速技术，包处理和虚拟化 ，以及 DPDK 技术在 SDN，NFV ，网络存储等领域的实际应用。</p><p>本书是国内第一本全面的阐述网络数据面的核心技术的书籍，面向 IT 网络通讯行业的从业人员，以及大专院校的学生，用通俗易懂的文字打开了一扇通向新一代网络处理架构的大门。</p><p>本书我有电子版（但只有一部分，推荐大家买书），需要的公众号后台回复 “DPDK” 查看获取方式。</p><p>除了书之外，就是看大牛的博客，加入相关的群和优秀的人一起学习，我整理了几份网上较好的博客资料，和书一起附赠，如果想加群学习，回复 “加群”。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> DPDK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 性能分析 </tag>
            
            <tag> DPDK </tag>
            
            <tag> OVS </tag>
            
            <tag> NUMA </tag>
            
            <tag> SDN </tag>
            
            <tag> UIO </tag>
            
            <tag> 大页内存 </tag>
            
            <tag> VPP </tag>
            
            <tag> mTCP </tag>
            
            <tag> fstack </tag>
            
            <tag> SPDK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 容器网络之单主机网络</title>
      <link href="/2018/04/06/tech/cloud/container/docker/Docker_%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
      <url>/2018/04/06/tech/cloud/container/docker/Docker_%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%8D%95%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>当容器逐步向容器集群，容器云技术演进的时候，一个不得不面对的问题就是各个容器的管理问题，有些容器需要交互，有些容器需要隔离，如何保证这些操作能够顺利地进行，这个时候，很多容器管理和编排的平台就应运而生了。首先，当然是 Docker 社区自己开发的 Swarm+Machine+Compose 的集群管理套件，然后还有 Twitter 主推 Apache 的 Mesos，最有名的应该为 Google 开源的 Kubernetes。</p><p>这些平台综合了容器集群资源管理，服务发现和扩容缩融等问题，是一个集大成的解决方案，但其实这些问题本质上都离不开网络，资源在各容器之间调度需要网络，服务发现需要网络，所以，可以说网络是容器集群环境下最基础的一环。</p><p>Docker 容器网络根据容器的部署位置，可以分为单主机网络（host）和多主机网络（multi-host），本文先看 Docker host 网络。</p><p>Docker host 网络分为 none，host，joined container 和 bridge 网络。</p><p>none 网络模式下，Docker 容器拥有自己的 network namespace，但是并不为容器进行任何的网络配置，也就是说容器除了 network namespace 本身自带的 localback 网卡外什么都没有，包括网卡、IP、路由等信息。用户如何要使用 none 网络，就需要自己添加特定的网卡，并配置 IP、路由等信息，但一般不会这么干，none 网络很好地做到了隔离，一般是用来跑那些对安全性要求极高且不需要联网的应用。</p><p>比如某个容器的唯一用途是生成随机密码，就可以放到 none 网络中避免密码被窃取。</p><p>想要使 Docker 使用 none 网络，只需要在创建容器的时候附带 –network = none 即可。</p><p>host 网络，顾名思义就是 Docker 容器使用宿主机的网络，相当于和 host 共用了同一个 network namespace，Docker 容器使用 host 的网卡、IP 和路由等功能对外通信。</p><p>虽然这种模式下 Docker 没有创建独立的 network namespace，但其他 namespace 仍然是隔离的，如文件系统、进程列表等。host 网络最大的好处就是使得 Docker 容器对外通信变得简单，直接使用 host 的 IP 进行通信即可，但缺点也很明显，就是降低了隔离性，同时还会存在和其他容器对网络资源的竞争与冲突的问题。</p><p>同样要使用 host 网络，只需创建容器时附带 –network = host 即可。</p><center><img src="/images/docker/net_host.png" alt=""></center><p>joined container 网络和 host 网络不同的是，它是和其他的 container 共享一个 network namespace，一个 network namespace 可以被一个或者多个 Docker 容器共享。在这种模式下，两个容器之间可以通过 localback 回环网卡通信，增加了容器间通信的便利性。</p><p>同样可以在创建容器时，使用参数 –network = container:another_container_name 来和另外容器共享网络。</p><center><img src="/images/docker/net_con.png" alt=""></center><p>bridge 网络是最常用的网络模式，它兼顾了安全性和功能的完备性，但其与外部通信要通过 NAT 转换，在复杂的网络场景下会存在诸多不便。</p><p>bridge 网络在创建的时候通过 –network = bridge 指定，Docker daemon 会为创建的容器自动创建一个 Docker 网桥——docker0（也可以人为指定名称 –driver bridge my_net），这个 docker0 就用来联结 Docker 容器和 host 的桥梁。</p><p>然后，Docker daemon 会创建一对虚拟网卡 veth pair，一块网卡留在宿主机的 root network namespace 中，并绑定到 docker0 上，另一块放在新创建的 network namespace 中，命名为 eth0，并为其配置 IP 和路由，将其网关设为 docker0，如下，这样整个容器的 bridge 网络通信环境就建立起来了，其他容器的建立方式也是如此，最终各容器之间就通过 docker0 这个桥梁互联互通了。</p><center><img src="/images/docker/net_bridge.jpg" alt=""></center><p>下文将讨论更为复杂的 multi-host 网络。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
            <tag> 容器网络 </tag>
            
            <tag> Bridge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 基础技术之 Linux cgroups 详解</title>
      <link href="/2018/03/28/tech/cloud/container/docker/Docker_%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E4%B9%8B_Linux_cgroups_%E8%AF%A6%E8%A7%A3/"/>
      <url>/2018/03/28/tech/cloud/container/docker/Docker_%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E4%B9%8B_Linux_cgroups_%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>前面两篇文章我们总结了 Docker 背后使用的资源隔离技术 Linux namespace，本篇将讨论另外一个技术——资源限额，这是由 Linux cgroups 来实现的。</p><blockquote><p>cgroups 是 Linux 内核提供的一种机制，这种机制可以根据需求把一系列任务及子任务整合（或分隔）到按资源划分等级的不同组内，从而为系统资源管理提供一个统一的框架。（来自 《Docker 容器与容器云》）</p></blockquote><p>通俗来说，cgroups 可以限制和记录任务组（进程组或线程组）使用的物理资源（包括 CPU、内存、IO 等）。</p><p>为了方便用户（程序员）操作，cgroups 以一个伪文件系统的方式实现，并对外提供 API，用户对文件系统的操作就是对 cgroups 的操作。</p><p>从实现上来，cgroups 实际上是给每个执行任务挂了一个钩子，当任务执行过程中涉及到对资源的分配使用时，就会触发钩子上的函数对相应的资源进行检测，从而对资源进行限制和优先级分配。</p><h3 id="cgroups-的作用"><a href="#cgroups-的作用" class="headerlink" title="cgroups 的作用"></a>cgroups 的作用</h3><hr><p>总结下来，cgroups 提供以下四个功能：</p><p><strong>资源限制：</strong> cgroups 可以对任务使用的资源总额进行限制，如设定应用运行时使用内存的上限，一旦超过这个配额就发出 OOM（Out of Memory）提示。</p><p><strong>优先级分配：</strong> 通过分配的 CPU 时间片数量和磁盘 IO 带宽大小，实际上就相当于控制了任务运行的优先级。</p><p><strong>资源统计：</strong> cgroups 可以统计系统的资源使用，如 CPU 使用时长、内存用量等，这个功能非常适用于计费。</p><p><strong>任务控制：</strong> cgroups 可以对任务执行挂起、恢复等操作。</p><h3 id="cgroups-的子系统"><a href="#cgroups-的子系统" class="headerlink" title="cgroups 的子系统"></a>cgroups 的子系统</h3><hr><p>cgroups 在设计时根据不同的资源类别分为不同的子系统，一个子系统本质上是一个资源控制器，比如 CPU 资源对应 CPU 子系统，负责控制 CPU 时间片的分配，内存对应内存子系统，负责限制内存的使用量。进一步，一个子系统或多个子系统可以组成一个 cgroup，cgroups 中的资源控制都是以 cgroup 为单位来实现，一个任务（或进程或线程）可以加入某个 cgroup，也可以从一个 cgroup 移动到另一个 cgroup，但这里有一些限制，在此就不再赘述了，详细查阅相关资料了解。</p><p>对于我们来说，最关键的是知道怎么用，下面就针对 CPU、内存和 IO 资源来看 Docker 是如何使用的？</p><p>对于 CPU，Docker 使用参数 -c 或 –cpu-shares 来设置一个容器使用的 CPU 权重，权重的大小也影响了 CPU 使用的优先级。</p><p>如下，启动两个容器，并分配不同的 CPU 权重，最终 CPU 使用率情况：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --name &quot;container_A&quot; -c 1024 ubuntu</span><br><span class="line">docker run --name &quot;container_B&quot; -c 512 ubuntu</span><br></pre></td></tr></table></figure></p><center><img src="/images/docker/cgroup_cpu.png" alt=""></center><p>当只有一个容器时，即使指定较少的 CPU 权重，它也会占满整个 CPU，说明这个权重只是相对权重，如下将上面的 “container_A” 停止，“container_B” 就分配到全部可用的 CPU。</p><center><img src="/images/docker/cgroup_cpu1.png" alt=""></center><p>对于内存，Docker 使用 -m（设置内存的限额）和 –memory-swap（设置内存和 swap 的限额）来控制容器内存的使用量，如下，给容器限制 200M 的内存和 100M 的 swap，然后给容器内的一个工作线程分配 280M 的内存，因为 280M 在容许的 300M 范围内，没有问题。其内存分配过程是不断分配又释放，如下：</p><center><img src="/images/docker/cgroup_mem.jpg" alt=""></center><p>如果让工作线程使用内存超过 300M，则出现内存超限的错误，容器退出，如下：</p><center><img src="/images/docker/cgroup_mem1.jpg" alt=""></center><p>对于 IO 资源，其使用方式与 CPU 一样，使用 –blkio-weight 来设置其使用权重，IO 衡量的两个指标是 bps（byte per second，每秒读写的数据量） 和 iops（io per second， 每秒 IO 的次数）,实际使用，一般使用这两个指标来衡量 IO 读写的带宽，几种使用参数如下：</p><ul><li>–device-read-bps，限制读某个设备的 bps。</li><li>–device-write-bps，限制写某个设备的 bps。</li><li>–device-read-iops，限制读某个设备的 iops。</li><li>–device-write-iops，限制写某个设备的 iops。</li></ul><p>假如限制容器对其文件系统 /dev/sda 的 bps 写速率为 30MB/s，则在容器中用 dd 测试其写磁盘的速率如下，可见小于 30MB/s。</p><center><img src="/images/docker/cgroup_dev.png" alt=""></center><p>如果是正常情况下，我的机器可以达到 56.7MB/s，一般都是超 1G 的。</p><center><img src="/images/docker/cgroup_dev1.png" alt=""></center><p>上面几个资源使用限制的例子，本质上都是调用了 Linux kernel 的 cgroups 机制来实现的，每个容器创建后，Linux 会为每个容器创建一个 cgroup 目录，以容器的 ID 命名，目录在 /sys/fs/cgroup/ 中，针对上面的 CPU 资源限制的例子，我们可以在  /sys/fs/cgroup/cpu/docker 中看到相关信息，如下：</p><center><img src="/images/docker/cgroup_info.jpg" alt=""></center><p>其中，cpu.shares 中保存的就是限制的数值，其他还有很多项，感兴趣可以动手实验看看。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>cgroups 的作用，cgroups 的实现，cgroups 的子系统机制，CPU、内存和 IO 的使用方式，以及对应 Linux 的 cgroups 文件目录。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
            <tag> Cgroup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 基础技术之 Linux namespace 源码分析</title>
      <link href="/2018/03/13/tech/cloud/container/docker/Docker_%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E4%B9%8B_Linux_namespace_%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
      <url>/2018/03/13/tech/cloud/container/docker/Docker_%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E4%B9%8B_Linux_namespace_%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>上篇我们从进程 clone 的角度，结合代码简单分析了 Linux 提供的 6 种 namespace，本篇从源码上进一步分析 Linux namespace，让你对 Docker namespace 的隔离机制有更深的认识。我用的是 Linux-4.1.19 的版本，由于 namespace 模块更新都比较少，所以，只要 3.0 以上的版本都是差不多的。</p><h3 id="从内核进程描述符-task-struct-开始切入"><a href="#从内核进程描述符-task-struct-开始切入" class="headerlink" title="从内核进程描述符 task_struct 开始切入"></a>从内核进程描述符 task_struct 开始切入</h3><hr><p>由于 Linux namespace 是用来做进程资源隔离的，所以在进程描述符中，一定有 namespace 所对应的信息，我们可以从这里开始切入代码。</p><p>首先找到描述进程信息 task_struct，找到指向 namespace 的结构 struct *nsproxy（sched.h）：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">struct task_struct &#123;</span><br><span class="line">......</span><br><span class="line">/* namespaces */</span><br><span class="line">struct nsproxy *nsproxy;</span><br><span class="line">......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中 nsproxy 结构体定义在 nsproxy.h 中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">* A structure to contain pointers to all per-process</span><br><span class="line">* namespaces - fs (mount), uts, network, sysvipc, etc.</span><br><span class="line">*</span><br><span class="line">* &apos;count&apos; is the number of tasks holding a reference.</span><br><span class="line">* The count for each namespace, then, will be the number</span><br><span class="line">* of nsproxies pointing to it, not the number of tasks.</span><br><span class="line">*</span><br><span class="line">* The nsproxy is shared by tasks which share all namespaces.</span><br><span class="line">* As soon as a single namespace is cloned or unshared, the</span><br><span class="line">* nsproxy is copied.</span><br><span class="line">*/</span><br><span class="line">struct nsproxy &#123;</span><br><span class="line">    atomic_t count;</span><br><span class="line">    struct uts_namespace *uts_ns;</span><br><span class="line">    struct ipc_namespace *ipc_ns;</span><br><span class="line">    struct mnt_namespace *mnt_ns;</span><br><span class="line">    struct pid_namespace *pid_ns;</span><br><span class="line">    struct net        *net_ns;</span><br><span class="line">&#125;;</span><br><span class="line">extern struct nsproxy init_nsproxy;</span><br></pre></td></tr></table></figure></p><p>这个结构是被所有 namespace 所共享的，只要一个 namespace 被 clone 了，nsproxy 也会被 clone。注意到，由于 user namespace 是和其他 namespace 耦合在一起的，所以没出现在上述结构中。</p><p>同时，nsproxy.h 中还定义了一些对 namespace 的操作，包括 copy_namespaces 等。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">int copy_namespaces(unsigned long flags, struct task_struct *tsk);</span><br><span class="line">void exit_task_namespaces(struct task_struct *tsk);</span><br><span class="line">void switch_task_namespaces(struct task_struct *tsk, struct nsproxy *new);</span><br><span class="line">void free_nsproxy(struct nsproxy *ns);</span><br><span class="line">int unshare_nsproxy_namespaces(unsigned long, struct nsproxy **,</span><br><span class="line">struct fs_struct *);</span><br></pre></td></tr></table></figure></p><p>task_struct，nsproxy，几种 namespace 之间的关系如下所示：</p><center><img src="/images/docker/ns_rela.jpg" alt=""></center><h3 id="各个-namespace-的初始化"><a href="#各个-namespace-的初始化" class="headerlink" title="各个 namespace 的初始化"></a>各个 namespace 的初始化</h3><hr><p>在各个 namespace 结构定义下都有个 init 函数，nsproxy 也有个 init_nsproxy 函数，init_nsproxy 在  task 初始化的时候会被初始化，附带的，init_nsproxy 中定义了各个 namespace 的 init 函数，如下：<br>在 init_task 函数中（init_task.h）:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">*  INIT_TASK is used to set up the first task table, touch at</span><br><span class="line">* your own risk!. Base=0, limit=0x1fffff (=2MB)</span><br><span class="line">*/</span><br><span class="line">#define INIT_TASK(tsk)  \</span><br><span class="line">&#123;</span><br><span class="line">......</span><br><span class="line">.nsproxy  = &amp;init_nsproxy,        </span><br><span class="line">......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>继续跟进 init_nsproxy，在 nsproxy.c 中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">struct nsproxy init_nsproxy = &#123;</span><br><span class="line">.count      = ATOMIC_INIT(1),</span><br><span class="line">.uts_ns      = &amp;init_uts_ns,</span><br><span class="line">#if defined(CONFIG_POSIX_MQUEUE) || defined(CONFIG_SYSVIPC)</span><br><span class="line">.ipc_ns      = &amp;init_ipc_ns,</span><br><span class="line">#endif</span><br><span class="line">.mnt_ns      = NULL,</span><br><span class="line">.pid_ns_for_children  = &amp;init_pid_ns,</span><br><span class="line">#ifdef CONFIG_NET</span><br><span class="line">.net_ns      = &amp;init_net,</span><br><span class="line">#endif</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>可见，init_nsproxy 中，对 uts, ipc, pid, net 都进行了初始化，但 mount 却没有。</p><h3 id="创建新的-namespace"><a href="#创建新的-namespace" class="headerlink" title="创建新的 namespace"></a>创建新的 namespace</h3><hr><p>初始化完之后，下面看看如何创建一个新的 namespace，通过前面的文章，我们知道是通过 clone 函数来完成的，在 Linux kernel 中，fork/vfork() 对 clone 进行了封装，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">#ifdef __ARCH_WANT_SYS_FORK</span><br><span class="line">SYSCALL_DEFINE0(fork)</span><br><span class="line">&#123;</span><br><span class="line">#ifdef CONFIG_MMU</span><br><span class="line">    return do_fork(SIGCHLD, 0, 0, NULL, NULL);</span><br><span class="line">#else</span><br><span class="line">    /* can not support in nommu mode */</span><br><span class="line">    return -EINVAL;</span><br><span class="line">#endif</span><br><span class="line">&#125;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef __ARCH_WANT_SYS_VFORK</span><br><span class="line">SYSCALL_DEFINE0(vfork)</span><br><span class="line">&#123;</span><br><span class="line">    return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0, 0, NULL, NULL);</span><br><span class="line">&#125;</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#ifdef __ARCH_WANT_SYS_CLONE</span><br><span class="line">#ifdef CONFIG_CLONE_BACKWARDS</span><br><span class="line">SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,</span><br><span class="line">    int __user *, parent_tidptr,</span><br><span class="line">    int, tls_val,</span><br><span class="line">    int __user *, child_tidptr)</span><br><span class="line">#elif defined(CONFIG_CLONE_BACKWARDS2)</span><br><span class="line">SYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags,</span><br><span class="line">    int __user *, parent_tidptr,</span><br><span class="line">    int __user *, child_tidptr,</span><br><span class="line">    int, tls_val)</span><br><span class="line">#elif defined(CONFIG_CLONE_BACKWARDS3)</span><br><span class="line">SYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp,</span><br><span class="line">    int, stack_size,</span><br><span class="line">    int __user *, parent_tidptr,</span><br><span class="line">    int __user *, child_tidptr,</span><br><span class="line">    int, tls_val)</span><br><span class="line">#else</span><br><span class="line">SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,</span><br><span class="line">    int __user *, parent_tidptr,</span><br><span class="line">    int __user *, child_tidptr,</span><br><span class="line">    int, tls_val)</span><br><span class="line">#endif</span><br><span class="line">&#123;</span><br><span class="line">    return do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr);</span><br><span class="line">&#125;</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure></p><p>可以看到，无论是 fork() 还是 vfork()，最终都会调用到 do_fork() 函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">*  Ok, this is the main fork-routine.</span><br><span class="line">*</span><br><span class="line">* It copies the process, and if successful kick-starts</span><br><span class="line">* it and waits for it to finish using the VM if required.</span><br><span class="line">*/</span><br><span class="line">long do_fork(unsigned long clone_flags,</span><br><span class="line">unsigned long stack_start,</span><br><span class="line">unsigned long stack_size,</span><br><span class="line">int __user *parent_tidptr,</span><br><span class="line">int __user *child_tidptr)</span><br><span class="line">&#123;</span><br><span class="line">// 创建进程描述符指针</span><br><span class="line">    struct task_struct *p;</span><br><span class="line">    int trace = 0;</span><br><span class="line">    long nr;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">* Determine whether and which event to report to ptracer.  When</span><br><span class="line">* called from kernel_thread or CLONE_UNTRACED is explicitly</span><br><span class="line">* requested, no event is reported; otherwise, report if the event</span><br><span class="line">* for the type of forking is enabled.</span><br><span class="line">*/</span><br><span class="line">if (!(clone_flags &amp; CLONE_UNTRACED)) &#123;</span><br><span class="line">    if (clone_flags &amp; CLONE_VFORK)</span><br><span class="line">        trace = PTRACE_EVENT_VFORK;</span><br><span class="line">    else if ((clone_flags &amp; CSIGNAL) != SIGCHLD)</span><br><span class="line">        trace = PTRACE_EVENT_CLONE;</span><br><span class="line">    else</span><br><span class="line">        trace = PTRACE_EVENT_FORK;</span><br><span class="line"></span><br><span class="line">    if (likely(!ptrace_event_enabled(current, trace)))</span><br><span class="line">        trace = 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 复制进程描述符，返回值是 task_struct</span><br><span class="line">p = copy_process(clone_flags, stack_start, stack_size,</span><br><span class="line">child_tidptr, NULL, trace);</span><br><span class="line">/*</span><br><span class="line">* Do this prior waking up the new thread - the thread pointer</span><br><span class="line">* might get invalid after that point, if the thread exits quickly.</span><br><span class="line">*/</span><br><span class="line">if (!IS_ERR(p)) &#123;</span><br><span class="line">    struct completion vfork;</span><br><span class="line">    struct pid *pid;</span><br><span class="line"></span><br><span class="line">    trace_sched_process_fork(current, p);</span><br><span class="line"></span><br><span class="line">    // 得到新进程描述符的 pid</span><br><span class="line">    pid = get_task_pid(p, PIDTYPE_PID);</span><br><span class="line">    nr = pid_vnr(pid);</span><br><span class="line"></span><br><span class="line">    if (clone_flags &amp; CLONE_PARENT_SETTID)</span><br><span class="line">    put_user(nr, parent_tidptr);</span><br><span class="line"></span><br><span class="line">    // 调用 vfork() 方法，完成相关的初始化工作  </span><br><span class="line">    if (clone_flags &amp; CLONE_VFORK) &#123;</span><br><span class="line">    p-&gt;vfork_done = &amp;vfork;</span><br><span class="line">    init_completion(&amp;vfork);</span><br><span class="line">    get_task_struct(p);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 将新进程加入到调度器中，为其分配 CPU，准备执行</span><br><span class="line">    wake_up_new_task(p);</span><br><span class="line"></span><br><span class="line">    // fork() 完成，子进程开始运行，并让 ptrace 跟踪</span><br><span class="line">    /* forking complete and child started to run, tell ptracer */</span><br><span class="line">    if (unlikely(trace))</span><br><span class="line">    ptrace_event_pid(trace, pid);</span><br><span class="line"></span><br><span class="line">    // 如果是 vfork()，将父进程加入等待队列，等待子进程完成</span><br><span class="line">    if (clone_flags &amp; CLONE_VFORK) &#123;</span><br><span class="line">    if (!wait_for_vfork_done(p, &amp;vfork))</span><br><span class="line">    ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    put_pid(pid);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">    nr = PTR_ERR(p);</span><br><span class="line">&#125;</span><br><span class="line">    return nr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>do_fork() 首先调用 copy_process 将父进程信息复制给子进程，然后调用 vfork() 完成相关的初始化工作，接着调用 wake_up_new_task() 将进程加入调度器中，为之分配 CPU。最后，等待子进程退出。</p><p>copy_process():<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line">static struct task_struct *copy_process(unsigned long clone_flags,</span><br><span class="line">    unsigned long stack_start,</span><br><span class="line">    unsigned long stack_size,</span><br><span class="line">    int __user *child_tidptr,</span><br><span class="line">    struct pid *pid,</span><br><span class="line">    int trace)</span><br><span class="line">&#123;</span><br><span class="line">int retval;</span><br><span class="line">// 创建进程描述符指针</span><br><span class="line">struct task_struct *p;</span><br><span class="line"></span><br><span class="line">// 检查 clone flags 的合法性，比如 CLONE_NEWNS 与 CLONE_FS 是互斥的</span><br><span class="line">if ((clone_flags &amp; (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))</span><br><span class="line">return ERR_PTR(-EINVAL);</span><br><span class="line"></span><br><span class="line">if ((clone_flags &amp; (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))</span><br><span class="line">return ERR_PTR(-EINVAL);</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">* Thread groups must share signals as well, and detached threads</span><br><span class="line">* can only be started up within the thread group.</span><br><span class="line">*/</span><br><span class="line">if ((clone_flags &amp; CLONE_THREAD) &amp;&amp; !(clone_flags &amp; CLONE_SIGHAND))</span><br><span class="line">return ERR_PTR(-EINVAL);</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">* Shared signal handlers imply shared VM. By way of the above,</span><br><span class="line">* thread groups also imply shared VM. Blocking this case allows</span><br><span class="line">* for various simplifications in other code.</span><br><span class="line">*/</span><br><span class="line">if ((clone_flags &amp; CLONE_SIGHAND) &amp;&amp; !(clone_flags &amp; CLONE_VM))</span><br><span class="line">return ERR_PTR(-EINVAL);</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">* Siblings of global init remain as zombies on exit since they are</span><br><span class="line">* not reaped by their parent (swapper). To solve this and to avoid</span><br><span class="line">* multi-rooted process trees, prevent global and container-inits</span><br><span class="line">* from creating siblings.</span><br><span class="line">*/</span><br><span class="line">// 比如CLONE_PARENT时得检查当前signal flags是否为SIGNAL_UNKILLABLE，防止kill init进程。</span><br><span class="line">if ((clone_flags &amp; CLONE_PARENT) &amp;&amp;</span><br><span class="line">current-&gt;signal-&gt;flags &amp; SIGNAL_UNKILLABLE)</span><br><span class="line">return ERR_PTR(-EINVAL);</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">* If the new process will be in a different pid or user namespace</span><br><span class="line">* do not allow it to share a thread group or signal handlers or</span><br><span class="line">* parent with the forking task.</span><br><span class="line">*/</span><br><span class="line">if (clone_flags &amp; CLONE_SIGHAND) &#123;</span><br><span class="line">if ((clone_flags &amp; (CLONE_NEWUSER | CLONE_NEWPID)) ||</span><br><span class="line">(task_active_pid_ns(current) !=</span><br><span class="line">current-&gt;nsproxy-&gt;pid_ns_for_children))</span><br><span class="line">return ERR_PTR(-EINVAL);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">retval = security_task_create(clone_flags);</span><br><span class="line">if (retval)</span><br><span class="line">goto fork_out;</span><br><span class="line"></span><br><span class="line">retval = -ENOMEM;</span><br><span class="line">// 复制当前的 task_struct</span><br><span class="line">p = dup_task_struct(current);</span><br><span class="line">if (!p)</span><br><span class="line">goto fork_out;</span><br><span class="line"></span><br><span class="line">ftrace_graph_init_task(p);</span><br><span class="line"></span><br><span class="line">rt_mutex_init_task(p);</span><br><span class="line"></span><br><span class="line">#ifdef CONFIG_PROVE_LOCKING</span><br><span class="line">DEBUG_LOCKS_WARN_ON(!p-&gt;hardirqs_enabled);</span><br><span class="line">DEBUG_LOCKS_WARN_ON(!p-&gt;softirqs_enabled);</span><br><span class="line">#endif</span><br><span class="line">retval = -EAGAIN;</span><br><span class="line"></span><br><span class="line">// 检查进程是否超过限制，由 OS 定义</span><br><span class="line">if (atomic_read(&amp;p-&gt;real_cred-&gt;user-&gt;processes) &gt;=</span><br><span class="line">task_rlimit(p, RLIMIT_NPROC)) &#123;</span><br><span class="line">if (p-&gt;real_cred-&gt;user != INIT_USER &amp;&amp;</span><br><span class="line">!capable(CAP_SYS_RESOURCE) &amp;&amp; !capable(CAP_SYS_ADMIN))</span><br><span class="line">goto bad_fork_free;</span><br><span class="line">&#125;</span><br><span class="line">current-&gt;flags &amp;= ~PF_NPROC_EXCEEDED;</span><br><span class="line"></span><br><span class="line">retval = copy_creds(p, clone_flags);</span><br><span class="line">if (retval &lt; 0)</span><br><span class="line">goto bad_fork_free;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">* If multiple threads are within copy_process(), then this check</span><br><span class="line">* triggers too late. This doesn&apos;t hurt, the check is only there</span><br><span class="line">* to stop root fork bombs.</span><br><span class="line">*/</span><br><span class="line">retval = -EAGAIN;</span><br><span class="line">// 检查进程数是否超过 max_threads，由内存大小定义</span><br><span class="line">if (nr_threads &gt;= max_threads)</span><br><span class="line">goto bad_fork_cleanup_count;</span><br><span class="line"></span><br><span class="line">// ......</span><br><span class="line"></span><br><span class="line">// 初始化 io 计数器</span><br><span class="line">task_io_accounting_init(&amp;p-&gt;ioac);</span><br><span class="line">acct_clear_integrals(p);</span><br><span class="line"></span><br><span class="line">// 初始化 CPU 定时器</span><br><span class="line">posix_cpu_timers_init(p);</span><br><span class="line"></span><br><span class="line">// ......</span><br><span class="line"></span><br><span class="line">// 初始化进程数据结构，并为进程分配 CPU，进程状态设置为 TASK_RUNNING</span><br><span class="line">/* Perform scheduler related setup. Assign this task to a CPU. */</span><br><span class="line">retval = sched_fork(clone_flags, p);</span><br><span class="line"></span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_policy;</span><br><span class="line"></span><br><span class="line">retval = perf_event_init_task(p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_policy;</span><br><span class="line">retval = audit_alloc(p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_perf;</span><br><span class="line">/* copy all the process information */</span><br><span class="line">// 复制所有进程信息，包括文件系统，信号处理函数、信号、内存管理等</span><br><span class="line">shm_init_task(p);</span><br><span class="line">retval = copy_semundo(clone_flags, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_audit;</span><br><span class="line">retval = copy_files(clone_flags, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_semundo;</span><br><span class="line">retval = copy_fs(clone_flags, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_files;</span><br><span class="line">retval = copy_sighand(clone_flags, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_fs;</span><br><span class="line">retval = copy_signal(clone_flags, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_sighand;</span><br><span class="line">retval = copy_mm(clone_flags, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_signal;</span><br><span class="line">// !!! 复制 namespace</span><br><span class="line">retval = copy_namespaces(clone_flags, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_mm;</span><br><span class="line">retval = copy_io(clone_flags, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_namespaces;</span><br><span class="line">// 初始化子进程内核栈</span><br><span class="line">retval = copy_thread(clone_flags, stack_start, stack_size, p);</span><br><span class="line">if (retval)</span><br><span class="line">goto bad_fork_cleanup_io;</span><br><span class="line">// 为新进程分配新的 pid</span><br><span class="line">if (pid != &amp;init_struct_pid) &#123;</span><br><span class="line">pid = alloc_pid(p-&gt;nsproxy-&gt;pid_ns_for_children);</span><br><span class="line">if (IS_ERR(pid)) &#123;</span><br><span class="line">retval = PTR_ERR(pid);</span><br><span class="line">goto bad_fork_cleanup_io;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// ......</span><br><span class="line"></span><br><span class="line">// 返回新进程 p</span><br><span class="line">return p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>copy_process 主要分为三步：首先调用 dup_task_struct() 复制当前的进程描述符信息 task_struct，为新进程分配新的堆栈，第二步调用 sched_fork() 初始化进程数据结构，为其分配 CPU，把进程状态设置为 TASK_RUNNING，最后一步就是调用 copy_namespaces() 复制 namesapces。我们重点关注最后一步 copy_namespaces()：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">* called from clone.  This now handles copy for nsproxy and all</span><br><span class="line">* namespaces therein.</span><br><span class="line">*/</span><br><span class="line">int copy_namespaces(unsigned long flags, struct task_struct *tsk)</span><br><span class="line">&#123;</span><br><span class="line">struct nsproxy *old_ns = tsk-&gt;nsproxy;</span><br><span class="line">struct user_namespace *user_ns = task_cred_xxx(tsk, user_ns);</span><br><span class="line">struct nsproxy *new_ns;</span><br><span class="line"></span><br><span class="line">if (likely(!(flags &amp; (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |</span><br><span class="line">CLONE_NEWPID | CLONE_NEWNET)))) &#123;</span><br><span class="line">get_nsproxy(old_ns);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if (!ns_capable(user_ns, CAP_SYS_ADMIN))</span><br><span class="line">return -EPERM;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">* CLONE_NEWIPC must detach from the undolist: after switching</span><br><span class="line">* to a new ipc namespace, the semaphore arrays from the old</span><br><span class="line">* namespace are unreachable.  In clone parlance, CLONE_SYSVSEM</span><br><span class="line">* means share undolist with parent, so we must forbid using</span><br><span class="line">* it along with CLONE_NEWIPC.</span><br><span class="line">*/</span><br><span class="line">if ((flags &amp; (CLONE_NEWIPC | CLONE_SYSVSEM)) ==</span><br><span class="line">(CLONE_NEWIPC | CLONE_SYSVSEM)) </span><br><span class="line">return -EINVAL;</span><br><span class="line"></span><br><span class="line">new_ns = create_new_namespaces(flags, tsk, user_ns, tsk-&gt;fs);</span><br><span class="line">if (IS_ERR(new_ns))</span><br><span class="line">return  PTR_ERR(new_ns);</span><br><span class="line"></span><br><span class="line">tsk-&gt;nsproxy = new_ns;</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可见，copy_namespace() 主要基于“旧的” namespace 创建“新的” namespace，核心函数在于 create_new_namespaces：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">* Create new nsproxy and all of its the associated namespaces.</span><br><span class="line">* Return the newly created nsproxy.  Do not attach this to the task,</span><br><span class="line">* leave it to the caller to do proper locking and attach it to task.</span><br><span class="line">*/</span><br><span class="line">static struct nsproxy *create_new_namespaces(unsigned long flags,</span><br><span class="line">struct task_struct *tsk, struct user_namespace *user_ns,</span><br><span class="line">struct fs_struct *new_fs)</span><br><span class="line">&#123;</span><br><span class="line">struct nsproxy *new_nsp;</span><br><span class="line">int err;</span><br><span class="line"></span><br><span class="line">// 创建新的 nsproxy</span><br><span class="line">new_nsp = create_nsproxy();</span><br><span class="line">if (!new_nsp)</span><br><span class="line">return ERR_PTR(-ENOMEM);</span><br><span class="line"></span><br><span class="line">//创建 mnt namespace</span><br><span class="line">new_nsp-&gt;mnt_ns = copy_mnt_ns(flags, tsk-&gt;nsproxy-&gt;mnt_ns, user_ns, new_fs);</span><br><span class="line">if (IS_ERR(new_nsp-&gt;mnt_ns)) &#123;</span><br><span class="line">err = PTR_ERR(new_nsp-&gt;mnt_ns);</span><br><span class="line">goto out_ns;</span><br><span class="line">&#125;</span><br><span class="line">//创建 uts namespace</span><br><span class="line">new_nsp-&gt;uts_ns = copy_utsname(flags, user_ns, tsk-&gt;nsproxy-&gt;uts_ns);</span><br><span class="line">if (IS_ERR(new_nsp-&gt;uts_ns)) &#123;</span><br><span class="line">err = PTR_ERR(new_nsp-&gt;uts_ns);</span><br><span class="line">goto out_uts;</span><br><span class="line">&#125;</span><br><span class="line">//创建 ipc namespace</span><br><span class="line">new_nsp-&gt;ipc_ns = copy_ipcs(flags, user_ns, tsk-&gt;nsproxy-&gt;ipc_ns);</span><br><span class="line">if (IS_ERR(new_nsp-&gt;ipc_ns)) &#123;</span><br><span class="line">err = PTR_ERR(new_nsp-&gt;ipc_ns);</span><br><span class="line">goto out_ipc;</span><br><span class="line">&#125;</span><br><span class="line">//创建 pid namespace</span><br><span class="line">new_nsp-&gt;pid_ns_for_children =</span><br><span class="line">copy_pid_ns(flags, user_ns, tsk-&gt;nsproxy-&gt;pid_ns_for_children);</span><br><span class="line">if (IS_ERR(new_nsp-&gt;pid_ns_for_children)) &#123;</span><br><span class="line">err = PTR_ERR(new_nsp-&gt;pid_ns_for_children);</span><br><span class="line">goto out_pid;</span><br><span class="line">&#125;</span><br><span class="line">//创建 network namespace</span><br><span class="line">new_nsp-&gt;net_ns = copy_net_ns(flags, user_ns, tsk-&gt;nsproxy-&gt;net_ns);</span><br><span class="line">if (IS_ERR(new_nsp-&gt;net_ns)) &#123;</span><br><span class="line">err = PTR_ERR(new_nsp-&gt;net_ns);</span><br><span class="line">goto out_net;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return new_nsp;</span><br><span class="line">// 出错处理</span><br><span class="line">out_net:</span><br><span class="line">if (new_nsp-&gt;pid_ns_for_children)</span><br><span class="line">put_pid_ns(new_nsp-&gt;pid_ns_for_children);</span><br><span class="line">out_pid:</span><br><span class="line">if (new_nsp-&gt;ipc_ns)</span><br><span class="line">put_ipc_ns(new_nsp-&gt;ipc_ns);</span><br><span class="line">out_ipc:</span><br><span class="line">if (new_nsp-&gt;uts_ns)</span><br><span class="line">put_uts_ns(new_nsp-&gt;uts_ns);</span><br><span class="line">out_uts:</span><br><span class="line">if (new_nsp-&gt;mnt_ns)</span><br><span class="line">put_mnt_ns(new_nsp-&gt;mnt_ns);</span><br><span class="line">out_ns:</span><br><span class="line">kmem_cache_free(nsproxy_cachep, new_nsp);</span><br><span class="line">return ERR_PTR(err);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>在create_new_namespaces()中，分别调用 create_nsproxy(), create_utsname(), create_ipcs(), create_pid_ns(), create_net_ns(), create_mnt_ns() 来创建 nsproxy 结构，uts，ipcs，pid，mnt，net。</p><p>具体的函数我们就不再分析，基本到此为止，我们从子进程创建，到子进程相关的信息的初始化，包括文件系统，CPU，内存管理等，再到各个 namespace 的创建，都走了一遍，下面附上 namespace 创建的代码流程图。</p><center><img src="/images/docker/ns_call.jpg" alt=""></center><p>mnt namespace:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">mnt namespace:</span><br><span class="line">struct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,</span><br><span class="line">struct user_namespace *user_ns, struct fs_struct *new_fs)</span><br><span class="line">&#123;</span><br><span class="line">struct mnt_namespace *new_ns;</span><br><span class="line">struct vfsmount *rootmnt = NULL, *pwdmnt = NULL;</span><br><span class="line">struct mount *p, *q;</span><br><span class="line">struct mount *old;</span><br><span class="line">struct mount *new;</span><br><span class="line">int copy_flags;</span><br><span class="line"></span><br><span class="line">BUG_ON(!ns);</span><br><span class="line"></span><br><span class="line">if (likely(!(flags &amp; CLONE_NEWNS))) &#123;</span><br><span class="line">get_mnt_ns(ns);</span><br><span class="line">return ns;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">old = ns-&gt;root;</span><br><span class="line">// 分配新的 mnt namespace</span><br><span class="line">new_ns = alloc_mnt_ns(user_ns);</span><br><span class="line">if (IS_ERR(new_ns))</span><br><span class="line">return new_ns;</span><br><span class="line"></span><br><span class="line">namespace_lock();</span><br><span class="line">/* First pass: copy the tree topology */</span><br><span class="line">// 首先 copy root 路径</span><br><span class="line">copy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;</span><br><span class="line">if (user_ns != ns-&gt;user_ns)</span><br><span class="line">copy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;</span><br><span class="line">new = copy_tree(old, old-&gt;mnt.mnt_root, copy_flags);</span><br><span class="line">if (IS_ERR(new)) &#123;</span><br><span class="line">namespace_unlock();</span><br><span class="line">free_mnt_ns(new_ns);</span><br><span class="line">return ERR_CAST(new);</span><br><span class="line">&#125;</span><br><span class="line">new_ns-&gt;root = new;</span><br><span class="line">list_add_tail(&amp;new_ns-&gt;list, &amp;new-&gt;mnt_list);</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">* Second pass: switch the tsk-&gt;fs-&gt;* elements and mark new vfsmounts</span><br><span class="line">* as belonging to new namespace.  We have already acquired a private</span><br><span class="line">* fs_struct, so tsk-&gt;fs-&gt;lock is not needed.</span><br><span class="line">*/</span><br><span class="line">// 为新进程设置 fs 信息</span><br><span class="line">p = old;</span><br><span class="line">q = new;</span><br><span class="line">while (p) &#123;</span><br><span class="line">q-&gt;mnt_ns = new_ns;</span><br><span class="line">if (new_fs) &#123;</span><br><span class="line">if (&amp;p-&gt;mnt == new_fs-&gt;root.mnt) &#123;</span><br><span class="line">new_fs-&gt;root.mnt = mntget(&amp;q-&gt;mnt);</span><br><span class="line">rootmnt = &amp;p-&gt;mnt;</span><br><span class="line">&#125;</span><br><span class="line">if (&amp;p-&gt;mnt == new_fs-&gt;pwd.mnt) &#123;</span><br><span class="line">new_fs-&gt;pwd.mnt = mntget(&amp;q-&gt;mnt);</span><br><span class="line">pwdmnt = &amp;p-&gt;mnt;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">p = next_mnt(p, old);</span><br><span class="line">q = next_mnt(q, new);</span><br><span class="line">if (!q)</span><br><span class="line">break;</span><br><span class="line">while (p-&gt;mnt.mnt_root != q-&gt;mnt.mnt_root)</span><br><span class="line">p = next_mnt(p, old);</span><br><span class="line">&#125;</span><br><span class="line">namespace_unlock();</span><br><span class="line"></span><br><span class="line">if (rootmnt)</span><br><span class="line">mntput(rootmnt);</span><br><span class="line">if (pwdmnt)</span><br><span class="line">mntput(pwdmnt);</span><br><span class="line"></span><br><span class="line">return new_ns;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，mount namespace 在新建时会新建一个新的 namespace，然后将父进程的 namespace 拷贝过来，并将 mount-&gt;mnt_ns 指向新的 namespace。接着设置进程的 root 路径以及当前路径到新的 namespace，然后为新进程设置新的 vfs 等。从这里就可以看出，在子进程中进行 mount 操作不会影响到父进程中的 mount 信息。</p><p>uts namespace:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">static inline struct uts_namespace *copy_utsname(unsigned long flags,</span><br><span class="line">struct user_namespace *user_ns, struct uts_namespace *old_ns)</span><br><span class="line">&#123;</span><br><span class="line">if (flags &amp; CLONE_NEWUTS)</span><br><span class="line">return ERR_PTR(-EINVAL);</span><br><span class="line"></span><br><span class="line">return old_ns;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>uts namespace 直接返回父进程 namespace 信息。</p><p>ipc namespace:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">struct ipc_namespace *copy_ipcs(unsigned long flags,</span><br><span class="line">struct user_namespace *user_ns, struct ipc_namespace *ns)</span><br><span class="line">&#123;</span><br><span class="line">if (!(flags &amp; CLONE_NEWIPC))</span><br><span class="line">return get_ipc_ns(ns);</span><br><span class="line">return create_ipc_ns(user_ns, ns);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ipc namespace 如果是设置了参数 CLONE_NEWIPC，则直接返回父进程的 namespace，否则返回新创建的 namespace。</p><p>pid namespace:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">static inline struct pid_namespace *copy_pid_ns(unsigned long flags,</span><br><span class="line">struct user_namespace *user_ns, struct pid_namespace *ns)</span><br><span class="line">&#123;</span><br><span class="line">if (flags &amp; CLONE_NEWPID)</span><br><span class="line">ns = ERR_PTR(-EINVAL);</span><br><span class="line">return ns;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>pid namespace 直接返回父进程的 namespace。</p><p>net namespace</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">static inline struct net *copy_net_ns(unsigned long flags,</span><br><span class="line">struct user_namespace *user_ns, struct net *old_net)</span><br><span class="line">&#123;</span><br><span class="line">if (flags &amp; CLONE_NEWNET)</span><br><span class="line">return ERR_PTR(-EINVAL);</span><br><span class="line">return old_net;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>net namespace 也是直接返回父进程的 namespace。</p><p>OK，不知不觉写了这么多，但回头去看，这更像是代码走读，分析深度不够，更详细的大家可以参照源码，源码结构还是比较清晰的。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Namespace </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 基础技术之 Linux namespace 详解</title>
      <link href="/2018/03/08/tech/cloud/container/docker/Docker_%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E4%B9%8B_Linux_namespace_%E8%AF%A6%E8%A7%A3/"/>
      <url>/2018/03/08/tech/cloud/container/docker/Docker_%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E4%B9%8B_Linux_namespace_%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>Docker 是“新瓶装旧酒”的产物，依赖于 Linux 内核技术 chroot 、namespace 和 cgroup。本篇先来看 namespace 技术。</p><p>Docker 和虚拟机技术一样，从操作系统级上实现了资源的隔离，它本质上是宿主机上的进程（容器进程），所以资源隔离主要就是指进程资源的隔离。实现资源隔离的核心技术就是 Linux namespace。这技术和很多语言的命名空间的设计思想是一致的（如 C++ 的 namespace）。</p><p>隔离意味着可以抽象出多个轻量级的内核（容器进程），这些进程可以充分利用宿主机的资源，宿主机有的资源容器进程都可以享有，但彼此之间是隔离的，同样，不同容器进程之间使用资源也是隔离的，这样，彼此之间进行相同的操作，都不会互相干扰，安全性得到保障。</p><p>为了支持这些特性，Linux namespace 实现了 6 项资源隔离，基本上涵盖了一个小型操作系统的运行要素，包括主机名、用户权限、文件系统、网络、进程号、进程间通信。</p><center><img src="/images/docker/docker_ns.png" alt=""></center><p>这 6 项资源隔离分别对应 6 种系统调用，通过传入上表中的参数，调用 clone() 函数来完成。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg);</span><br></pre></td></tr></table></figure></p><p>clone() 函数相信大家都不陌生了，它是 fork() 函数更通用的实现方式，通过调用 clone()，并传入需要隔离资源对应的参数，就可以建立一个容器了（隔离什么我们自己控制）。</p><p>一个容器进程也可以再 clone() 出一个容器进程，这是容器的嵌套。</p><center><img src="/images/docker/docker_clone.jpg" alt=""></center><p>如果想要查看当前进程下有哪些 namespace 隔离，可以查看文件 /proc/[pid]/ns （注：该方法仅限于 3.8 版本以后的内核）。</p><center><img src="/images/docker/docker_shell.jpg" alt=""></center><p>可以看到，每一项 namespace 都附带一个编号，这是唯一标识 namespace 的，如果两个进程指向的 namespace 编号相同，则表示它们同在该 namespace 下。同时也注意到，多了一个 cgroup，这个 namespace 是 4.6 版本的内核才支持的。Docker 目前对它的支持普及度还不高。所以我们暂时先不考虑它。</p><p>下面通过简单的代码来实现 6 种 namespace 的隔离效果，让大家有个直观的印象。</p><h3 id="UTS-namespace"><a href="#UTS-namespace" class="headerlink" title="UTS namespace"></a>UTS namespace</h3><hr><p>UTS namespace 提供了主机名和域名的隔离，这样每个容器就拥有独立的主机名和域名了，在网络上就可以被视为一个独立的节点，在容器中对 hostname 的命名不会对宿主机造成任何影响。</p><p>首先，先看总体的代码骨架：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#define _GNU_SOURCE</span><br><span class="line">#include &lt;sys/types.h&gt;</span><br><span class="line">#include &lt;sys/wait.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;sched.h&gt;</span><br><span class="line">#include &lt;signal.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#define STACK_SIZE (1024 * 1024)</span><br><span class="line"></span><br><span class="line">static char container_stack[STACK_SIZE];</span><br><span class="line">char* const container_args[] = &#123;</span><br><span class="line">    &quot;/bin/bash&quot;,</span><br><span class="line">    NULL</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 容器进程运行的程序主函数</span><br><span class="line">int container_main(void *args)</span><br><span class="line">&#123;</span><br><span class="line">    printf(&quot;在容器进程中！\n&quot;);</span><br><span class="line">    execv(container_args[0], container_args); // 执行/bin/bash   return 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(int args, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">    printf(&quot;程序开始\n&quot;);</span><br><span class="line">    // clone 容器进程</span><br><span class="line">    int container_pid = clone(container_main, container_stack + STACK_SIZE, SIGCHLD, NULL);</span><br><span class="line">    // 等待容器进程结束</span><br><span class="line">    waitpid(container_pid, NULL, 0);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>该程序骨架调用 clone() 函数实现了子进程的创建工作，并定义子进程的执行函数，clone() 第二个参数指定了子进程运行的栈空间大小，第三个参数即为创建不同 namespace 隔离的关键。</p><p>对于 UTS namespace，传入 CLONE_NEWUTS，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int container_pid = clone(container_main, container_stack + STACK_SIZE, SIGCHLD | CLONE_NEWUTS, NULL);</span><br></pre></td></tr></table></figure></p><p>为了能够看出容器内和容器外主机名的变化，我们子进程执行函数中加入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sethostname(&quot;container&quot;, 9);</span><br></pre></td></tr></table></figure></p><p>最终运行可以看到效果如下：</p><center><img src="/images/docker/docker_show.png" alt=""></center><h3 id="IPC-namespace"><a href="#IPC-namespace" class="headerlink" title="IPC namespace"></a>IPC namespace</h3><hr><p>IPC namespace 实现了进程间通信的隔离，包括常见的几种进程间通信机制，如信号量，消息队列和共享内存。我们知道，要完成 IPC，需要申请一个全局唯一的标识符，即 IPC 标识符，所以 IPC 资源隔离主要完成的就是隔离 IPC 标识符。</p><p>同样，代码修改仅需要加入参数 CLONE_NEWIPC 即可，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int container_pid = clone(container_main, container_stack + STACK_SIZE, SIGCHLD | CLONE_NEWUTS | CLONE_NEWIPC, NULL);</span><br></pre></td></tr></table></figure></p><p>为了看出变化，首先在宿主机上建立一个消息队列：</p><center><img src="/images/docker/ns_ipc.png" alt=""></center><p>然后运行程序，进入容器查看 IPC，没有找到原先建立的 IPC 标识，达到了 IPC 隔离。</p><center><img src="/images/docker/ns_ipc1.png" alt=""></center><h3 id="PID-namespace"><a href="#PID-namespace" class="headerlink" title="PID namespace"></a>PID namespace</h3><hr><p>PID namespace 完成的是进程号的隔离，同样在 clone() 中加入 CLONE_NEWPID 参数，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int container_pid = clone(container_main, container_stack + STACK_SIZE, SIGCHLD | CLONE_NEWUTS | CLONE_NEWIPC | CLONE_NEWPID, NULL);</span><br></pre></td></tr></table></figure></p><p>效果如下，echo $$ 输出 shell 的 PID 号，发生了变化。</p><center><img src="/images/docker/ns_pid.png" alt=""></center><p>但是对于 ps/top 之类命令却没有改变：</p><center><img src="/images/docker/ns_pid1.jpg" alt=""></center><p>原因是 ps/top 之类的命令底层调用的是文件系统的 /proc 文件内容，由于 /proc 文件系统（procfs）还没有挂载到一个与原 /proc 不同的位置，自然在容器中显示的就是宿主机的进程。</p><p>我们可以通过在容器中重新挂载 /proc 即可实现隔离，如下：</p><center><img src="/images/docker/ns_proc.png" alt=""></center><p>这种方式会破坏 root namespace 中的文件系统，当退出容器时，如果 ps 会出现错误，只有再重新挂载一次 /proc 才能恢复。</p><center><img src="/images/docker/ns_proc1.jpg" alt=""></center><p>一劳永逸地解决这个问题最好的方法就是用接下来介绍的 mount namespace。</p><h3 id="mount-namespace"><a href="#mount-namespace" class="headerlink" title="mount namespace"></a>mount namespace</h3><hr><p>mount namespace 通过隔离文件系统的挂载点来达到对文件系统的隔离。我们依然在代码中加入 CLONE_NEWNS 参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int container_pid = clone(container_main, container_stack + STACK_SIZE, SIGCHLD | CLONE_NEWUTS | CLONE_NEWIPC | CLONE_NEWPID | CLONE_NEWNS, NULL);</span><br></pre></td></tr></table></figure></p><p>我验证的效果，当退出容器时，还是会有 mount 错误，这没道理，经多方查阅，没有找到问题的根源（有谁知道，可以留言指出）。</p><center><img src="/images/docker/ns_mount.jpg" alt=""></center><h3 id="Network-namespace"><a href="#Network-namespace" class="headerlink" title="Network namespace"></a>Network namespace</h3><hr><p>Network namespace 实现了网络资源的隔离，包括网络设备、IPv4 和 IPv6 协议栈，IP 路由表，防火墙，/proc/net 目录，/sys/class/net 目录，套接字等。</p><p>Network namespace 不同于其他 namespace 可以独立工作，要使得容器进程和宿主机或其他容器进程之间通信，需要某种“桥梁机制”来连接彼此（并没有真正的隔离），这是通过创建 veth pair （虚拟网络设备对，有两端，类似于管道，数据从一端传入能从另一端收到，反之亦然）来实现的。当建立 Network namespace 后，内核会首先建立一个 docker0 网桥，功能类似于 Bridge，用于建立各容器之间和宿主机之间的通信，具体就是分别将 veth pair 的两端分别绑定到 docker0 和新建的 namespace 中。</p><center><img src="/images/docker/docker_net.jpg" alt=""></center><p>和其他 namespace 一样，Network namespace 的创建也是加入 CLONE_NEWNET 参数即可。我们可以简单验证下 IP 地址的情况，如下，IP 被隔离了。</p><center><img src="/images/docker/ns_net.jpg" alt=""></center><h3 id="User-namespace"><a href="#User-namespace" class="headerlink" title="User namespace"></a>User namespace</h3><hr><p>User namespace 主要隔离了安全相关的标识符和属性，包括用户 ID、用户组 ID、root 目录、key 以及特殊权限。简单说，就是一个普通用户的进程通过 clone() 之后在新的 user namespace 中可以拥有不同的用户和用户组，比如可能是超级用户。</p><p>同样，可以加入 CLONE_NEWUSER 参数来创建一个 User namespace。然后再子进程执行函数中加入 getuid() 和 getpid() 得到 namespace 内部的 User ID，效果如下：</p><center><img src="/images/docker/ns_user.png" alt=""></center><p>可以看到，容器内部看到的 UID 和 GID 和外部不同了，默认显示为 65534。这是因为容器找不到其真正的 UID ，所以设置上了最大的UID（其设置定义在/proc/sys/kernel/overflowuid）。另外就是用户变为了 nobody，不再是 root，达到了隔离。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>以上就是对 6 种 namespace 从代码上简单直观地演示其实现，当然，真正的实现比这个要复杂得多，然后这 6 种 namespace 实际上也没有完全隔离 Linux 的资源，比如 SElinux、cgroup 以及 /sys 等目录下的资源没有隔离。目前，Docker 在很多方面已经做的很好，但相比虚拟机，仍然有许多安全性问题急需解决。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Namespace </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
            <tag> Cgroup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>容器生态系统</title>
      <link href="/2018/03/02/tech/cloud/container/%E5%AE%B9%E5%99%A8%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F/"/>
      <url>/2018/03/02/tech/cloud/container/%E5%AE%B9%E5%99%A8%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>说起生态，不禁让人想起贾跃亭的乐视，想当初我多次被它的生态布局给震撼到，一度相信它将要超越百度，坐拥互联网三大江山的宝座，但没过时日，各种劲爆的新闻就把它推到了风口浪尖上，现在想想也是让人唏嘘，但不管怎么说，愿它好吧，毕竟这种敢想敢做的精神还是值得敬佩的。</p><p>回到技术这个领域，不得不说，技术更新迭代的速度快得让人应接不暇，就容器技术这个领域来说，从 Docker 面世短短的 2-3 年时间里，就衍生出多种与之相关的技术框架，由此形成了一个小小的生态系统。</p><center><img src="/images/docker/docker_st.jpg" alt=""></center><p>一谈到容器，大家都会想到 Docker，本文也主要从 Docker 角度来讲容器生态系统。</p><h3 id="1-容器基础技术"><a href="#1-容器基础技术" class="headerlink" title="1 容器基础技术"></a>1 容器基础技术</h3><hr><p>Docker 的本质是利用 Linux 内核的 namespace 和 cgroups 机制，构建出一个隔离的进程（容器进程）。所以，容器的基础技术主要涉及到 Linux 内核的 namespace 和 cgroups 技术。</p><h3 id="2-容器核心技术"><a href="#2-容器核心技术" class="headerlink" title="2 容器核心技术"></a>2 容器核心技术</h3><hr><p>容器核心技术保证容器能够在主机上运行起来，包括容器规范、容器 runtime、容器管理工具、容器定义工具、Registry 和容器 OS。</p><center><img src="/images/docker/docker_core.jpg" alt=""></center><p>容器规范旨在将多种容器（如 OpenVZ，rkt，Docker 等）融合在一起，解决各种兼容问题，为此还专门成立了一个叫 OCI（Open Container Initiative）的组织来专门制定相关的容器规范。</p><p>容器 runtime 是容器真正运行的地方，一般需要依赖内核，也有运行在专门制定的容器 OS 上，关于容器 OS，下面会做介绍。lxc 、runc 和 rkt 是目前三种主流的 runtime。</p><blockquote><p>lxc 是 Linux 上老牌的容器 runtime。Docker 最初也是用 lxc 作为 runtime。<br>runc 是 Docker 自己开发的容器 runtime，符合 oci 规范，也是现在 Docker 的默认 runtime。<br>rkt 是 CoreOS 开发的容器 runtime，符合 oci 规范，因而能够运行 Docker 的容器。</p></blockquote><blockquote><p>容器管理工具是对外提供给用户的 CLI 接口，方便用户管理容器，对内与 runtime 交互。对应于不同的 runtime，分别有三种不同的管理工具：lxd、docker engine 和 rkt cli。</p></blockquote><p>容器定义工具允许用户定义容器的内容和属性，如容器需要什么镜像，装载什么应用等。常用有三种工具：docker images、Dockerfile 和 ACL（App Container Image）。</p><blockquote><p>docker images 是容器镜像，runtime 依据 docker images 创建容器。dockerfile 是包含若干命令的文本文件，可以通过这些命令创建出 docker images。ACI 与 docker images 类似，只不过它是由 CoreOS 开发的 rkt 容器的 images 格式。</p></blockquote><p>Registry 是存放容器镜像的仓库，包括 Docker Registry、Docker Hub 和 Quay.io，以及国内的 DaoCloud.io。企业可以用 Docker Registry 构建私有的 Registry。</p><p>容器 OS 不同于 runtime，是专门制定出来运行容器的操作系统，与常规 OS 相比，容器 OS 通常体积更小，启动更快。因为是为容器定制的 OS，通常它们运行容器的效率会更高。目前已经存在不少容器 OS，CoreOS、atomic 和 ubuntu core 是其中的杰出代表。</p><h3 id="3-容器平台技术"><a href="#3-容器平台技术" class="headerlink" title="3 容器平台技术"></a>3 容器平台技术</h3><hr><p>随着容器部署的增多，容器也逐步过渡到容器云，容器平台技术就是让容器作为集群在分布式的环境中运行，包括了容器编排引擎、容器管理平台和基于容器的 PaaS。</p><center><img src="/images/docker/docker_plat.jpg" alt=""></center><p>容器编排引擎就是管理、调度容器在集群中运行，以保障资源的合理利用。有名的三大编排引擎为 docker swarm、kubernetes 和 mesos。其中，kubernetes 这两年脱颖而出，成为其中的佼佼者。</p><p>容器管理平台是在编排引擎之上更为通用的一个平台，它抽象了编排引擎的底层实现细节，能够支持多种编排引擎，提供友好的接口给用户，极大方便了管理。Rancher 和 ContainerShip 是容器管理平台的典型代表。</p><p>基于容器的 PaaS 基于容器的 PaaS 为微服务应用开发人员和公司提供了开发、部署和管理应用的平台，使用户不必关心底层基础设施而专注于应用的开发。Deis、Flynn 和 Dokku 都是开源容器 PaaS 的代表。</p><h3 id="4-容器支持技术"><a href="#4-容器支持技术" class="headerlink" title="4 容器支持技术"></a>4 容器支持技术</h3><hr><p>容器的出现又重新让一些古老的技术焕发第二春，如监控、网络、数据管理、日志等技术，由于容器技术的不同，需要制定相应的符合容器规范的技术框架，由此有了容器支持技术，用于支持容器提供更丰富能力的基础设施。</p><center><img src="/images/docker/docker_zc.jpg" alt=""></center><p>其中包括容器网络、服务发现、监控、数据管理、日志管理和安全性。</p><p>容器网络主要用于解决容器与容器之间，容器与其他实体之间的连通性和隔离性。包括 Docker 原生的网络解决方案 docker network，以及第三方的网络解决方案，如  flannel、weave 和 calico。</p><p>服务发现保证容器使用过程中资源动态变化的感知性，如当负载增加时，集群会自动创建新的容器；负载减小，多余的容器会被销毁。容器也会根据 host 的资源使用情况在不同 host 中迁移，容器的 IP 和端口也会随之发生变化。在这种动态环境下，就需要有一种机制来感知这种变化，服务发现就是做这样的工作。etcd、consul 和 zookeeper 是服务发现的典型解决方案。</p><p>监控室保证容器健康运行，且让用户实时了解应用运行状态的工具，除了 Docker 原生的监控工具 docker ps/top/stats 之外，也有第三方的监控方案，如 sysdig、cAdvisor/Heapster 和 Weave Scope 。</p><p>数据管理保证容器在不同的 host 之间迁移时数据的动态迁移。有名的方案是 Flocker。</p><p>日志管理为问题排查和事件管理提供了重要依据。docker logs 是 Docker 原生的日志工具。而 logspout 对日志提供了路由功能，它可以收集不同容器的日志并转发给其他工具进行后处理。</p><p>容器安全性保证容器的安全，不被攻击，OpenSCAP 能够对容器镜像进行扫描，发现潜在的漏洞。</p><p>PS：本文借鉴了知名云计算博主 CloudMan 的博文：<br><a href="http://www.cnblogs.com/CloudMan6/p/6706546.html，感谢" target="_blank" rel="noopener">http://www.cnblogs.com/CloudMan6/p/6706546.html，感谢</a> CloudMan 呈现这么好的内容。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>容器进化史</title>
      <link href="/2018/02/28/tech/cloud/container/%E5%AE%B9%E5%99%A8%E8%BF%9B%E5%8C%96%E5%8F%B2/"/>
      <url>/2018/02/28/tech/cloud/container/%E5%AE%B9%E5%99%A8%E8%BF%9B%E5%8C%96%E5%8F%B2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>和虚拟机一样，容器技术也是一种资源隔离的虚拟化技术。我们追溯它的历史，会发现它的技术雏形早已有之。</p><h3 id="1-容器简史"><a href="#1-容器简史" class="headerlink" title="1 容器简史"></a>1 容器简史</h3><hr><p>容器概念始于 1979 年提出的 UNIX chroot，它是一个 UNIX 操作系统的系统调用，将一个进程及其子进程的根目录改变到文件系统中的一个新位置，让这些进程只能访问到这个新的位置，从而达到了进程隔离的目的。</p><center><img src="/imagesss/docker/docker_his.jpg" alt=""></center><p>2000 年的时候 FreeBSD 开发了一个类似于 chroot 的容器技术 Jails，这是最早期，也是功能最多的容器技术。Jails 英译过来是监狱的意思，这个“监狱”（用沙盒更为准确）包含了文件系统、用户、网络、进程等的隔离。</p><p>2001 Linux 也发布自己的容器技术 Linux VServer，2004 Solaris 也发布了 Solaris Containers，两者都将资源进行划分，形成一个个 zones，又叫做虚拟服务器。</p><p>2005 年推出 OpenVZ，它通过对 Linux 内核进行补丁来提供虚拟化的支持，每个 OpenVZ 容器完整支持了文件系统、用户及用户组、进程、网络、设备和 IPC 对象的隔离。</p><p>2007 年 Google 实现了 Control Groups( cgroups )，并加入到 Linux 内核中，这是划时代的，为后期容器的资源配额提供了技术保障。</p><p>2008 年基于 cgroups 和 linux namespace 推出了第一个最为完善的 Linux 容器 LXC。</p><p>2013 年推出到现在为止最为流行和使用最广泛的容器 Docker，相比其他早期的容器技术，Docker 引入了一整套容器管理的生态系统，包括分层的镜像模型，容器注册库，友好的 Rest API 等。</p><p>2014 年 CoreOS 也推出了一个类似于 Docker 的容器 Rocket，CoreOS 是一个更加轻量级的 Linux 操作系统，在安全性上比 Docker 更严格。</p><p>2016 年微软也在 Windows 上提供了容器的支持，Docker 可以以原生方式运行在 Windows 上，而不是需要使用 Linux 虚拟机。</p><p>基本上到这个时间节点，容器技术就已经很成熟了，再往后就是容器云的发展，由此也衍生出多种容器云的平台管理技术，其中以 kubernetes 最为出众，有了这样一些细粒度的容器集群管理技术，也为微服务的发展奠定了基石。因此，对于未来来说，应用的微服务化是一个较大的趋势。</p><h3 id="2-为什么需要容器"><a href="#2-为什么需要容器" class="headerlink" title="2 为什么需要容器"></a>2 为什么需要容器</h3><hr><p>其一，这是技术演进的一种创新结果，其二，这是人们追求高效生产活动的一种工具。</p><p>随着软件开发的发展，相比于早期的集中式应用部署方式，现在的应用基本都是采用分布式的部署方式，一个应用可能包含多种服务或多个模块，因此多种服务可能部署在多种环境中，如虚拟服务器、公有云、私有云等，由于多种服务之间存在一些依赖关系，所以可能存在应用在运行过程中的动态迁移问题，那这时如何保证不同服务在不同环境中都能平滑的适配，不需要根据环境的不同而去进行相应的定制，就显得尤为重要。</p><center><img src="/imagesss/docker/docker_con.jpg" alt=""></center><p>就像货物的运输问题一样，如何将不同的货物放在不同的运输机器上，减少因货物的不同而频繁进行货物的装载和卸载，浪费大量的人力物力。</p><p>为此人们发明了集装箱，将货物根据尺寸形状等的不同，用不同规格的集装箱装载，然后再放到运输机上运输，由于集装箱密封，只有货物到达目的地才需拆封，在运输过程能够在不同运输机上平滑过渡，避免了资源的浪费。</p><center><img src="/imagesss/docker/docker_con1.jpg" alt=""></center><p>因此集装箱被誉为是运输业与世界贸易最重要的发明。</p><p>Docker 容器的思想就是采用集装箱思想，为应用提供了一个基于容器的标准化运输系统。Docker 可以将任何应用及其依赖打包成一个轻量级、可移植、自包含的容器。容器可以运行在几乎所有的操作系统上。这样容器就可以跑在任何环境中，因此才有了那句话：</p><blockquote><p>Build Once, Run Anywhere</p></blockquote><p>这种集装箱的思想我们也能从 Docker 的 Logo 中看出来，这不就是一堆集装箱吗？</p><center><img src="/imagesss/docker/docker.jpg" alt=""></center><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/imagesss/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2018 云计算领域的技术趋势预测</title>
      <link href="/2018/02/20/tech/cloud/2018_%E4%BA%91%E8%AE%A1%E7%AE%97%E9%A2%86%E5%9F%9F%E7%9A%84%E6%8A%80%E6%9C%AF%E8%B6%8B%E5%8A%BF%E9%A2%84%E6%B5%8B/"/>
      <url>/2018/02/20/tech/cloud/2018_%E4%BA%91%E8%AE%A1%E7%AE%97%E9%A2%86%E5%9F%9F%E7%9A%84%E6%8A%80%E6%9C%AF%E8%B6%8B%E5%8A%BF%E9%A2%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><center><img src="/images/cloud/6.jpg" alt=""></center><p>作为一个技术人，保持对技术的敏感是必要的，尤其是在互联网这个行业，昨天还觉得移动互联网挺酷炫的，今天就被人工智能这波潮流给掩盖掉了锋芒，泯然众人矣。虽然我们不一定要赶时髦，踏踏实实做好技术才是好的，但保持这样一份好奇心，也许会让我们的技术之路走得更加顺畅一点。</p><p>对于云计算这个领域，虽然早已是陈词滥调，但新瓶装旧酒，总会玩出很多新鲜的花样。作为一个没什么经验的初学者，我斗胆预测一下 2018 年云计算领域的几大技术趋势，仅供参考。</p><h3 id="混合云"><a href="#混合云" class="headerlink" title="混合云"></a>混合云</h3><hr><p>混合云其实叫嚣了很多年，但一直没有得到真正的应用，现在大部分企业和个人还是部署公有云和私有云为主，随着市场的增长和用户需求的增加，尤其是对云安全这块的需求，相信在 2018 年会有众多客户将业务迁移到混合云上。公有云市场的一些大公司，像阿里云，腾讯云会大力布局其混合云市场，而私有云市场则会更多选择与公有云大公司合作来布局自己的混合云市场，另外，像 zstack 这样的混合云开源技术框架会得到较多青睐，兴许可以成为下一个 OpenStack。</p><h3 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a>微服务</h3><hr><p>2014 年，AWS 推出了 serverless （无服务器）计算服务  Lambda，微服务便是在这样一种模式下提出的一种架构设计思想。此后几年各大互联网公司都在引入微服务的设计思想，应用在自家的业务产品中。可以说，微服务架构设计得好，不管是开发测试，还是运维管理都会大大提升效率和增强产品的可维护性和可扩展性。2018 年，微服务将会引发一波小高潮。很多大公司会开放他们的微服务设计框架，及相关的开源项目。</p><h3 id="kubernetes-k8s-容器编排"><a href="#kubernetes-k8s-容器编排" class="headerlink" title="kubernetes( k8s ) 容器编排"></a>kubernetes( k8s ) 容器编排</h3><hr><p>容器技术现在基本上已经成为各大互联网公司的标配，随着部署的容器应用的增多，容器云的编排与管理已经成了不得不面对的问题。k8s 的强势崛起，在短短时间之内就刮起一阵旋风，令 docker 家族闻风丧胆，最终不得不缴械投降，2018 年，k8s 会得到井喷式应用。</p><h3 id="雾计算"><a href="#雾计算" class="headerlink" title="雾计算"></a>雾计算</h3><hr><p>雾计算，或者边缘计算，在学术界已经不算什么新鲜词了，但在工业界还少有耳闻，但其实 cisco 和一些物联网公司早已开发出自己的雾计算产品，并大量应用在网络边缘的计算设备中。据我所知的，智云这家公司早已推出自己的雾计算平台，并应用在很多物联网项目中，大公司中，百度在 2018 年年初也率先推出了自己的雾计算项目「智能边缘」，旨在通过提供 “IoT Edge SDK” 组件的方式进行数据的本地处理，结合 AI 技术，为用户打造实时响应，智能推断，安全可靠的服务体验。相信在 2018 年会有更多的雾计算产品出现，我们拭目以待吧。</p><p>以上就是我对 2018 年云计算这个领域技术趋势的大致预测，预测本身就代表着不确定性，但保持对新技术的敏感性，能让我们提前做一些准备，毕竟作为技术人，终身学习是最基本的职业素养。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 01 云计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 雾计算 </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> 混合云 </tag>
            
            <tag> 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vhost-user 简介</title>
      <link href="/2018/02/06/tech/cloud/virt/vhost_user_%E7%AE%80%E4%BB%8B/"/>
      <url>/2018/02/06/tech/cloud/virt/vhost_user_%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="1-什么是-vhost-user"><a href="#1-什么是-vhost-user" class="headerlink" title="1. 什么是 vhost-user"></a>1. 什么是 vhost-user</h3><p>在 vhost 的方案中，由于 vhost 实现在内核中，guest 与 vhost 的通信，相较于原生的 virtio 方式性能上有了一定程度的提升，从 guest 到 kvm.ko 的交互只有一次用户态的切换以及数据拷贝。这个方案对于不同 host 之间的通信，或者 guest 到 host nic 之间的通信是比较好的，但是对于某些用户态进程间的通信，比如数据面的通信方案，openvswitch 和与之类似的 SDN 的解决方案，guest 需要和 host 用户态的 vswitch 进行数据交换，如果采用 vhost 的方案，guest 和 host 之间又存在多次的上下文切换和数据拷贝，为了避免这种情况，业界就想出将 vhost 从内核态移到用户态。这就是 vhost-user 的实现。</p><h3 id="2-vhost-user-的实现"><a href="#2-vhost-user-的实现" class="headerlink" title="2 vhost-user 的实现"></a>2 vhost-user 的实现</h3><hr><p>vhost-user 和 vhost 的实现原理是一样，都是采用  vring 完成共享内存，eventfd 机制完成事件通知。不同在于 vhost 实现在内核中，而 vhost-user 实现在用户空间中，用于用户空间中两个进程之间的通信，其采用共享内存的通信方式。</p><center><img src="/images/virt/vhost_user.png" alt=""></center><p>vhost-user 基于 C/S 的模式，采用 UNIX 域套接字（UNIX domain socket）来完成进程间的事件通知和数据交互，相比 vhost 中采用 ioctl 的方式，vhost-user 采用 socket 的方式大大简化了操作。</p><p>vhost-user 基于 vring 这套通用的共享内存通信方案，只要 client 和 server 按照 vring 提供的接口实现所需功能即可，常见的实现方案是 client 实现在 guest OS 中，一般是集成在 virtio 驱动上，server 端实现在 qemu 中，也可以实现在各种数据面中，如 OVS，Snabbswitch 等虚拟交换机。</p><p>如果使用 qemu 作为 vhost-user 的 server 端实现，在启动 qemu 时，我们需要指定 -mem-path 和 -netdev 参数，如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ qemu -m 1024 -mem-path /hugetlbfs,prealloc=on,share=on \</span><br><span class="line">-netdev type=vhost-user,id=net0,file=/path/to/socket \</span><br><span class="line">-device virtio-net-pci,netdev=net0</span><br></pre></td></tr></table></figure><p>指定 -mem-path 意味着 qemu 会在 guest OS 的内存中创建一个文件，share=on 选项允许其他进程访问这个文件，也就意味着能访问 guest OS 内存，达到共享内存的目的。</p><p>-netdev type=vhost-user 指定通信方案，file=/path/to/socket 指定 socket 文件。</p><p>当 qemu 启动之后，首先会进行 vring 的初始化，并通过 socket 建立 C/S 的共享内存区域和事件机制，然后 client 通过 eventfd 将 virtio kick 事件通知到 server 端，server 端同样通过 eventfd 进行响应，完成整个数据交互。 </p><center><img src="/images/virt/vhost_user_flow.jpg" alt=""></center><h3 id="3-几个例子"><a href="#3-几个例子" class="headerlink" title="3 几个例子"></a>3 几个例子</h3><hr><p>开源社区中实现了一个项目 Vapp，主要是用来测试 vhost-user 的 C/S 模式的，github 地址如下：<br><a href="https://github.com/virtualopensystems/vapp.git" target="_blank" rel="noopener">https://github.com/virtualopensystems/vapp.git</a></p><p>使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/virtualopensystems/vapp.git</span><br><span class="line">$ cd vapp</span><br><span class="line">$ make</span><br><span class="line">// 运行 server 端</span><br><span class="line">$ ./vhost -s ./vhost.sock</span><br><span class="line">// 运行 client 端</span><br><span class="line">$ ./vhost -q ./vhost.sock</span><br></pre></td></tr></table></figure><p>通过以上步骤，就可以启动 vhost-user 的 C/S 模式。</p><p>另外还有例子就是集成在虚拟交换机 Snabbswitch 上的 vhost-user，通过以下方式获得 vhost-user 分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git clone -b vhostuser --recursive https://github.com/SnabbCo/snabbswitch.git</span><br><span class="line">$ cd snabbswitch</span><br><span class="line">$ make</span><br><span class="line">测试：</span><br><span class="line">$ sudo src/snabbswitch -t apps.vhost.vhost_user</span><br></pre></td></tr></table></figure><p>还有例子就是 qemu 上的实现，这也是最原早的实现，同样通过以下方式来获得使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git clone -b vhost-user-v5 https://github.com/virtualopensystems/qemu.git</span><br><span class="line">$ mkdir qemu/obj</span><br><span class="line">$ cd qemu/obj/</span><br><span class="line">$ ../configure --target-list=x86_64-softmmu</span><br><span class="line">$ make -j</span><br></pre></td></tr></table></figure><p>除此之外，还有很多的实现，如 OVS 和 DPDK 上都有实现，这实际上是集成了 vhost-user 的通用 API。</p><h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h3><hr><p>virtio，vhost，vhost-user 是基于场景和性能而提出的三种 guest 和 host 之间的通信方案，三种方案，各有优劣。<br>vhost-user 用在很多数据面之上的进程间通信，效率高。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 03 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 虚拟化 </tag>
            
            <tag> KVM </tag>
            
            <tag> Qemu </tag>
            
            <tag> vhost_user </tag>
            
            <tag> vhost </tag>
            
            <tag> virtio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vhost 简介</title>
      <link href="/2018/01/23/tech/cloud/virt/vhost_%E7%AE%80%E4%BB%8B/"/>
      <url>/2018/01/23/tech/cloud/virt/vhost_%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="1-什么是-vhost"><a href="#1-什么是-vhost" class="headerlink" title="1. 什么是 vhost"></a>1. 什么是 vhost</h3><hr><p>vhost 是 virtio 的一种后端实现方案，在 virtio 简介中，我们已经提到 virtio 是一种半虚拟化的实现方案，需要虚拟机端和主机端都提供驱动才能完成通信，通常，virtio 主机端的驱动是实现在用户空间的 qemu 中，而 vhost 是实现在内核中，是内核的一个模块 vhost-net.ko。为什么要实现在内核中，有什么好处呢，请接着往下看。</p><center><img src="/images/virt/vhost.jpg" alt=""></center><h3 id="2-为什么要用-vhost"><a href="#2-为什么要用-vhost" class="headerlink" title="2. 为什么要用 vhost"></a>2. 为什么要用 vhost</h3><hr><p>在 virtio 的机制中，guest 与 用户空间的 Hypervisor 通信，会造成多次的数据拷贝和 CPU 特权级的上下文切换。例如 guest 发包给外部网络，首先，guest 需要切换到 host kernel，然后 host kernel 会切换到 qemu 来处理 guest 的请求， Hypervisor 通过系统调用将数据包发送到外部网络后，会切换回 host kernel ， 最后再切换回 guest。这样漫长的路径无疑会带来性能上的损失。</p><p>vhost 正是在这样的背景下提出的一种改善方案，它是位于 host kernel 的一个模块，用于和 guest 直接通信，数据交换直接在 guest 和 host kernel 之间通过 virtqueue 来进行，qemu 不参与通信，但也没有完全退出舞台，它还要负责一些控制层面的事情，比如和 KVM 之间的控制指令的下发等。</p><h3 id="3-vhost-的数据流程"><a href="#3-vhost-的数据流程" class="headerlink" title="3. vhost 的数据流程"></a>3. vhost 的数据流程</h3><hr><p>下图左半部分是 vhost 负责将数据发往外部网络的过程， 右半部分是 vhost 大概的数据交互流程图。其中，qemu 还是需要负责 virtio 设备的适配模拟，负责用户空间某些管理控制事件的处理，而 vhost  实现较为纯净，以一个独立的模块完成 guest 和 host kernel 的数据交换过程。</p><center><img src="/images/virt/vhost_flow.png" alt=""></center><p>vhost 与 virtio 前端的通信主要采用一种事件驱动 eventfd 的机制来实现，guest 通知 vhost 的事件要借助 kvm.ko 模块来完成，vhost 初始化期间，会启动一个工作线程 work 来监听 eventfd，一旦 guest 发出对 vhost 的 kick event，kvm.ko 触发 ioeventfd 通知到 vhost，vhost 通过 virtqueue 的 avail ring 获取数据，并设置 used ring。同样，从 vhost 工作线程向 guest 通信时，也采用同样的机制，只不过这种情况发的是一个回调的 call envent，kvm.ko 触发 irqfd 通知 guest。</p><h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h3><hr><p>vhost 与 kvm 的事件通信通过 eventfd 机制来实现，主要包括两个方向的 event，一个是 guest 到 vhost 方向的 kick event，通过 ioeventfd 实现；另一个是 vhost 到 guest 方向的 call event，通过  irqfd 实现。</p><p>代码分析整个通信的流程：<br><a href="http://royluo.org/2014/08/22/vhost/" target="_blank" rel="noopener">http://royluo.org/2014/08/22/vhost/</a></p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 03 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 虚拟化 </tag>
            
            <tag> KVM </tag>
            
            <tag> Qemu </tag>
            
            <tag> vhost </tag>
            
            <tag> virtio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用户空间网络提升 NFV 的性能</title>
      <link href="/2018/01/21/tech/net/dpdk/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E7%BD%91%E7%BB%9C%E6%8F%90%E5%8D%87_NFV_%E7%9A%84%E6%80%A7%E8%83%BD/"/>
      <url>/2018/01/21/tech/net/dpdk/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E7%BD%91%E7%BB%9C%E6%8F%90%E5%8D%87_NFV_%E7%9A%84%E6%80%A7%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">本文是一篇翻译，翻译自：</span><br><span class="line">https://software.intel.com/en-us/blogs/2015/06/12/user-space-networking-fuels-nfv-performance，文章有点老了，15年写的，但是文章总结了一些用户态的协议栈，很有学习参考的意义。</span><br></pre></td></tr></table></figure><h3 id="用户态协议栈"><a href="#用户态协议栈" class="headerlink" title="用户态协议栈"></a>用户态协议栈</h3><hr><p>如今，作为一个网络空间的软件开发人员是非常激动人心的，因为工程师的角色随着这个世界的规则在逐渐发生改变。</p><p>过去这 15 年来，人们对高性能网络做了很多努力，网络模型也发生了很多改变，起初，数据包的收发都要推送到内核才能完成，现在，不用内核态的参与也可以完成。这种改变的背后主要是在解决以下的几个问题：1）用户态和内核态上下文切换的开销；2）软硬中断的开销；3）数据拷贝的开销等等。</p><center><img src="/images/sdn/mtcp.jpg" alt=""></center><p>最近，很多人在讨论 mTCP——一个实现了用户态协议栈的开源库，这种技术在很大程度上就颠覆了传统的网络模型，使得网络收发包的效率大大提升。它充分利用了 CPU 的亲和性，共享内存，批处理等技术来实现高效的 I/O 事件。与之类似的技术也相继被提出。</p><p>实验表明，mTCP 这种用户态协议栈，相较原生的内核协议栈，在处理多种流行的应用时的性能得到较大提升，如 SSLShader 提升了 33%，lighttpd 提升了 320%。</p><center><img src="/images/sdn/user_tcp_stack.png" alt=""></center><p>软件形态的改变离不开硬件的革新。由以前的单核系统到如今多核系统的性能扩展，直接导致了网络软件架构的改变。例如，以前内核所做的所有功能和处理，包括网络驱动程序，现在都可以被直接放到用户空间中来实现，应用程序可以直接访问底层的 NUMA 结构，利用 CPU 的亲和性，以及多核特性并行处理任务。这种设计避免了上下文之间的切换开销，可以显著降低数据传输的延迟和 CPU 使用，同时提高吞吐量和带宽。另外，提供一种运行到完成（run-to-completion）的模型能够让不同的核独立并行地完成不同的任务。</p><p>随着网络的发展，我们现在看到了大量的开源项目，都在将内核协议栈移到用户空间来做。它们的做法存在一些区别，像 mTCP，它的协议栈是从零开始开发的，而其他很多项目则是基于 FreeBSD 的来做，这主要是因为 FreeBSD 的协议栈具有 “最健壮的网络协议栈的声誉”，此外，很多存储解决方案也是采用的 FreeBSD 来作为其核心操作系统。当然，Linux 协议栈也是可以采用的。</p><p>这些用户态协议栈怎么做到绕过内核的，这就离不开 DPDK 的支持。利用 DPDK，用户态协议栈可以创建一个中断来将数据包从 NIC 的缓冲区直接映射到用户空间，然后利用协议栈的特性来管理 TCP/IP 数据包的处理和传输。</p><p>DPDK 还可以作为一些 vSwitch（虚拟交换机）的加速器，这些 vSwitch 包含 OpenFlow 协议的完整实现，以及与 OpenStack Neutron 的整合。</p><p>下面，我们收集了一些发现的开源项目，无论你决定使用一个 vSwitch 还是一个完整的网络协议栈，网络开发人员都有很多选择，可以将应用程序移到用户空间，并在多核系统上扩展性能。</p><h4 id="DPDK-Enabled-vSwitch："><a href="#DPDK-Enabled-vSwitch：" class="headerlink" title="DPDK-Enabled vSwitch："></a>DPDK-Enabled vSwitch：</h4><hr><p><strong>OVS</strong>      </p><p>Open vSwitch 是一个多核虚拟交换机平台，支持标准的管理接口和开放可扩展的可编程接口，支持第三方的控制接入。<br><a href="https://github.com/openvswitch/ovs" target="_blank" rel="noopener">https://github.com/openvswitch/ovs</a></p><p><strong>Lagopus</strong></p><p>Lagopus 是另一个多核虚拟交换的实现，功能和 OVS 差不多，支持多种网络协议，如 Ethernet，VLAN，QinQ，MAC-in-MAC，MPLS 和 PBB，以及隧道协议，如 GRE，VxLan 和 GTP。<br><a href="https://github.com/lagopus/lagopus/blob/master/QUICKSTART.md" target="_blank" rel="noopener">https://github.com/lagopus/lagopus/blob/master/QUICKSTART.md</a></p><p><strong>Snabb</strong></p><p>Snabb 是一个简单且快速的数据包处理工具箱。<br><a href="https://github.com/SnabbCo/snabbswitch/blob/master/README.md" target="_blank" rel="noopener">https://github.com/SnabbCo/snabbswitch/blob/master/README.md</a></p><p><strong>xDPd</strong></p><p>xDPd 是一个多平台，多 OpenFlow 版本支持的开源 datapath，主要专注在性能和可扩展性上。<br><a href="https://github.com/bisdn/xdpd/blob/stable/README" target="_blank" rel="noopener">https://github.com/bisdn/xdpd/blob/stable/README</a></p><h4 id="从零开发的用户空间协议栈套件"><a href="#从零开发的用户空间协议栈套件" class="headerlink" title="从零开发的用户空间协议栈套件"></a>从零开发的用户空间协议栈套件</h4><hr><p><strong>mTCP</strong></p><p>mTCP 是一个针对多核系统的高可扩展性的用户空间 TCP/IP 协议栈。<br><a href="https://github.com/eunyoung14/mtcp/blob/master/README" target="_blank" rel="noopener">https://github.com/eunyoung14/mtcp/blob/master/README</a></p><p><strong>Mirage-Tcpip</strong></p><p>mirage-tcpip 是一个针对 MirageOS 这种 “库操作系统” 而开发的一个用户态网络协议栈，开发的语言是 OCaml。<br><a href="https://github.com/mirage/mirage-tcpip" target="_blank" rel="noopener">https://github.com/mirage/mirage-tcpip</a></p><p><strong>IwIP</strong></p><p>IwIP 针对 RAM 平台的精简版的 TCP/IP 协议栈实现。<br><a href="http://git.savannah.gnu.org/cgit/lwip.git/tree/README" target="_blank" rel="noopener">http://git.savannah.gnu.org/cgit/lwip.git/tree/README</a></p><h4 id="移植版的用户空间协议栈套件"><a href="#移植版的用户空间协议栈套件" class="headerlink" title="移植版的用户空间协议栈套件"></a>移植版的用户空间协议栈套件</h4><hr><p><strong>Arrakis</strong>  </p><p>针对多核系统的用户空间 OS，移植于 IwIP。<br><a href="https://github.com/UWNetworksLab/arrakis/blob/master/README_ARRAKIS" target="_blank" rel="noopener">https://github.com/UWNetworksLab/arrakis/blob/master/README_ARRAKIS</a></p><p><strong>libuinet</strong></p><p>用户空间的 TCP/IP 协议栈，移植于 FreeBSD。<br><a href="https://github.com/pkelsey/libuinet/blob/master/README" target="_blank" rel="noopener">https://github.com/pkelsey/libuinet/blob/master/README</a></p><p><strong>NUSE (libos)</strong>  </p><p>一个基于 Linux 的库操作系统，移植于 Linux。<br><a href="https://github.com/libos-nuse/net-next-nuse/wiki/Quick-Start" target="_blank" rel="noopener">https://github.com/libos-nuse/net-next-nuse/wiki/Quick-Start</a></p><p><strong>OpenDP</strong> </p><p>一个针对 DPDK TCP/IP 协议栈的数据面，移植于 FreeBSD。<br><a href="https://github.com/opendp/dpdk-odp/wiki" target="_blank" rel="noopener">https://github.com/opendp/dpdk-odp/wiki</a></p><p><strong>OpenOnload</strong></p><p>一个高性能的用户态协议栈，移植于 IwIP。<br><a href="http://www.openonload.org/download/openonload-201205-README.txt" target="_blank" rel="noopener">http://www.openonload.org/download/openonload-201205-README.txt</a></p><p><strong>OSv</strong>  </p><p>一个针对虚拟机的开源操作系统。移植于 FreeBSD。<br><a href="https://github.com/cloudius-systems/osv/blob/master/README.md" target="_blank" rel="noopener">https://github.com/cloudius-systems/osv/blob/master/README.md</a></p><p><strong>Sandstorm</strong>  </p><p>一个针对个人服务器安全的开源网络平台，移植于 FreeBSD。<br><a href="https://github.com/sandstorm-io/sandstorm/blob/master/README.md" target="_blank" rel="noopener">https://github.com/sandstorm-io/sandstorm/blob/master/README.md</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>1、这篇文字的亮点在于总结了当前阶段业界出现的一些用户空间协议栈，对于文章标题提到的 NFV 在文中则只字未提，但其实意思也很明了了。用户空间的协议栈是随着硬件技术的发展，以及新鲜应用场景应运而生的，换句话说，对于像 NFV 这种对性能要求比较高的场景，采用用户态的协议栈是比较合适的。</p><p>2、文中是 2015 年写的，这意味着到现在为止，肯定出现了很多比上面总结还要多的方案，其中比较出名的有 SeaStar 和 腾讯开源的 F-Stack，后面找机会再进行详述，敬请期待吧。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> NFV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DPDK </tag>
            
            <tag> mTCP </tag>
            
            <tag> NFV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 网络体系结构全景分析</title>
      <link href="/2018/01/12/tech/net/Linux_%E7%BD%91%E7%BB%9C%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E5%85%A8%E6%99%AF%E5%88%86%E6%9E%90/"/>
      <url>/2018/01/12/tech/net/Linux_%E7%BD%91%E7%BB%9C%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E5%85%A8%E6%99%AF%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>Linux 是一个非常庞大的系统结构，模块众多，各个模块分工合作，共同运作着各自的任务，为 Linux 这个大家庭贡献着自己的力量。</p><h3 id="Linux-网络子系统"><a href="#Linux-网络子系统" class="headerlink" title="Linux 网络子系统"></a>Linux 网络子系统</h3><hr><p>其中，网络子系统是一个较为复杂的模块，如下图是 Linux 网络子系统的体系结构，自顶向下，可以分应用层、插口层（或协议无关层）、协议层和接口层（或设备驱动层），这个层级关系也分别对应着我们所熟悉的 OSI 七层模型（物理层、数据链路层、网络层、传输层、会话层、表示层和应用层）。</p><center><img src="/images/linux/linux_net_level.jpg" alt=""></center><p>Linux 所有用户态的应用程序要访问链接内核都要依赖系统调用，这是内核提供给用户态的调用接口，这通常是由标准的 C 库函数来实现的。对于网络应用程序，内核提供了一套通用的 socket 系统调用的接口。</p><p>插口层，也叫协议无关层，它屏蔽了协议相关的操作，不论是什么协议（UDP，TCP），都提供一组通用的函数接口，也就是 socket 的实现。</p><p>协议层，用于实现各种具体的网络协议族，包括 TCP/IP，OSI 和 Unix 域的实现，每个协议族都包含自己的内部结构，例如，对于 TCP/IP 协议族，IP 是最底层，TCP 和 UDP 层在 IP 层的上面。</p><p>接口层，包括了通用设备接口，和网络设备通信的设备驱动程序，其中，包串口使用的 SLIP 驱动程序以及以太网使用的以太网驱动程序，以及环回口使用的都是这一层的设备。它对上提供了协议与设备驱动通信的通用接口，对下也提供了一组通用函数供底层网络设备驱动程序使用。具体的细节后面再开文章详细讲述。</p><p>以上是从分层的体系结构来看，下面从数据传输的角度来说说，一个数据包是如何依赖这些层次关系被发送/接收的。</p><p>首先，数据的发送过程，应用层组织好待发送的数据包，执行系统调用进入内核协议栈，按照层次关系分别进行包头的封装，如到达传输层，封装 TCP/UDP 的包头，进入网络层封装 IP 包头，进入接口层封装 MAC 帧，最后借助驱动将数据包从网卡发送出去。</p><p>然后，数据的接收过程正好与发包过程相反，是一个拆包的过程。接口层借助网卡 DMA 硬件中断感知数据包的到来，拆解包头，识别数据，借助软中断机制告知上层进行相应的收包处理，最终用户态执行系统调用接收数据包。</p><center><img src="/images/linux/linux_net_flow.jpg" alt=""></center><p>继续细化这个过程，其内部在实现上都是通过具体的数据结构来完成每一层的过渡的，譬如说，应用层和内核之间通过 struct sock 结构实现协议无关的接口调用进行交互，传输层提供 struct proto 的结构来支持多种协议，网络层通过 struct sk_buff 来传递数据，接口层定义 struct net_device 来完成和驱动程序的交互。</p><center><img src="/images/linux/linux_net_struct.jpg" alt=""></center><p>Linux 网络体系结构还是比较博大精深的，尤其是这套自顶向下的分层设计方式对很多技术都有借鉴意义。更具体的函数调用和数据收发过程，后面的文章再进行讲述，敬请期待。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>Linux 网络子系统的分层模型，数据收发过程，以及核心数据结构。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux 下几种零拷贝的方法</title>
      <link href="/2018/01/05/tech/linux/Linux_%E4%B8%8B%E5%87%A0%E7%A7%8D%E9%9B%B6%E6%8B%B7%E8%B4%9D%E7%9A%84%E6%96%B9%E6%B3%95/"/>
      <url>/2018/01/05/tech/linux/Linux_%E4%B8%8B%E5%87%A0%E7%A7%8D%E9%9B%B6%E6%8B%B7%E8%B4%9D%E7%9A%84%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发于我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>本文讲解 Linux 的零拷贝技术，我在「云计算技能图谱」中说过，云计算是一门很庞大的技术学科，融合了很多技术，Linux 算是比较基础的技术，所以，学好 Linux 对于云计算的学习会有比较大的帮助。</p><p>本文借鉴并总结了几种比较常见的 Linux 下的零拷贝技术，相关的引用链接见文后，大家如果觉得本文总结得太抽象，可以转到链接看详细解释。</p><h3 id="为什么需要零拷贝"><a href="#为什么需要零拷贝" class="headerlink" title="为什么需要零拷贝"></a>为什么需要零拷贝</h3><hr><p>传统的 Linux 系统的标准 I/O 接口（read、write）是基于数据拷贝的，也就是数据都是 copy_to_user 或者 copy_from_user，这样做的好处是，通过中间缓存的机制，减少磁盘 I/O 的操作，但是坏处也很明显，大量数据的拷贝，用户态和内核态的频繁切换，会消耗大量的 CPU 资源，严重影响数据传输的性能，有数据表明，在Linux内核协议栈中，这个拷贝的耗时甚至占到了数据包整个处理流程的57.1%。</p><h3 id="什么是零拷贝"><a href="#什么是零拷贝" class="headerlink" title="什么是零拷贝"></a>什么是零拷贝</h3><hr><p>零拷贝就是这个问题的一个解决方案，通过尽量避免拷贝操作来缓解 CPU 的压力。Linux 下常见的零拷贝技术可以分为两大类：一是针对特定场景，去掉不必要的拷贝；二是去优化整个拷贝的过程。由此看来，零拷贝并没有真正做到“0”拷贝，它更多是一种思想，很多的零拷贝技术都是基于这个思想去做的优化。</p><center><img src="/imagessss/linux/copy_type.jpg" alt=""></center><h3 id="零拷贝的几种方法"><a href="#零拷贝的几种方法" class="headerlink" title="零拷贝的几种方法"></a>零拷贝的几种方法</h3><hr><h4 id="原始数据拷贝操作"><a href="#原始数据拷贝操作" class="headerlink" title="原始数据拷贝操作"></a>原始数据拷贝操作</h4><hr><p>在介绍之前，先看看 Linux 原始的数据拷贝操作是怎样的。如下图，假如一个应用需要从某个磁盘文件中读取内容通过网络发出去，像这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while((n = read(diskfd, buf, BUF_SIZE)) &gt; 0)</span><br><span class="line"></span><br><span class="line">write(sockfd, buf , n);</span><br></pre></td></tr></table></figure><p>那么整个过程就需要经历：1）read 将数据从磁盘文件通过 DMA 等方式拷贝到内核开辟的缓冲区；2）数据从内核缓冲区复制到用户态缓冲区；3）write 将数据从用户态缓冲区复制到内核协议栈开辟的 socket 缓冲区；4）数据从 socket 缓冲区通过 DMA 拷贝到网卡上发出去。</p><center><img src="/imagessss/linux/copy_tran.png" alt=""></center><p>可见，整个过程发生了至少四次数据拷贝，其中两次是 DMA 与硬件通讯来完成，CPU 不直接参与，去掉这两次，仍然有两次 CPU 数据拷贝操作。</p><h4 id="方法一：用户态直接-I-O"><a href="#方法一：用户态直接-I-O" class="headerlink" title="方法一：用户态直接 I/O"></a>方法一：用户态直接 I/O</h4><hr><p>这种方法可以使应用程序或者运行在用户态下的库函数直接访问硬件设备，数据直接跨过内核进行传输，内核在整个数据传输过程除了会进行必要的虚拟存储配置工作之外，不参与其他任何工作，这种方式能够直接绕过内核，极大提高了性能。</p><center><img src="/imagessss/linux/copy_dirtio.jpg" alt=""></center><p><strong>缺陷：</strong></p><p>1）这种方法只能适用于那些不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。</p><p>2）这种方法直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成资源的浪费，解决这个问题需要和异步 I/O 结合使用。</p><h4 id="方法二：mmap"><a href="#方法二：mmap" class="headerlink" title="方法二：mmap"></a>方法二：mmap</h4><hr><p>这种方法，使用 mmap 来代替 read，可以减少一次拷贝操作，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">buf = mmap(diskfd, len);</span><br><span class="line"></span><br><span class="line">write(sockfd, buf, len);</span><br></pre></td></tr></table></figure><p>应用程序调用 mmap ，磁盘文件中的数据通过 DMA 拷贝到内核缓冲区，接着操作系统会将这个缓冲区与应用程序共享，这样就不用往用户空间拷贝。应用程序调用write ，操作系统直接将数据从内核缓冲区拷贝到 socket 缓冲区，最后再通过 DMA 拷贝到网卡发出去。</p><center><img src="/imagessss/linux/copy_mmap.png" alt=""></center><p><strong>缺陷：</strong> </p><p>1）mmap 隐藏着一个陷阱，当 mmap 一个文件时，如果这个文件被另一个进程所截获，那么 write 系统调用会因为访问非法地址被 SIGBUS 信号终止，SIGBUS 默认会杀死进程并产生一个 coredump，如果服务器被这样终止了，那损失就可能不小了。</p><p>解决这个问题通常使用文件的租借锁：首先为文件申请一个租借锁，当其他进程想要截断这个文件时，内核会发送一个实时的 RT_SIGNAL_LEASE 信号，告诉当前进程有进程在试图破坏文件，这样 write 在被 SIGBUS 杀死之前，会被中断，返回已经写入的字节数，并设置 errno 为 success。</p><p>通常的做法是在 mmap 之前加锁，操作完之后解锁：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) == -1) &#123;</span><br><span class="line"></span><br><span class="line">perror(&quot;kernel lease set signal&quot;);</span><br><span class="line"></span><br><span class="line">return -1;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* l_type can be F_RDLCK F_WRLCK  加锁*/</span><br><span class="line"></span><br><span class="line">/* l_type can be  F_UNLCK 解锁*/</span><br><span class="line"></span><br><span class="line">if(fcntl(diskfd, F_SETLEASE, l_type))&#123;</span><br><span class="line"></span><br><span class="line">perror(&quot;kernel lease set type&quot;);</span><br><span class="line"></span><br><span class="line">return -1;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="方法三：sendfile"><a href="#方法三：sendfile" class="headerlink" title="方法三：sendfile"></a>方法三：sendfile</h4><hr><p>从Linux 2.1版内核开始，Linux引入了sendfile，也能减少一次拷贝。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;sys/sendfile.h&gt;</span><br><span class="line"></span><br><span class="line">ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);</span><br></pre></td></tr></table></figure><p>sendfile 是只发生在内核态的数据传输接口，没有用户态的参与，自然避免了用户态数据拷贝。它指定在 in_fd 和 out_fd 之间传输数据，其中，它规定 in_fd 指向的文件必须是可以 mmap 的，out_fd 必须指向一个套接字，也就是规定数据只能从文件传输到套接字，反之则不行。sendfile 不存在像 mmap 时文件被截获的情况，它自带异常处理机制。</p><center><img src="/imagessss/linux/copy_sendfile.jpg" alt=""></center><p><strong>缺陷：</strong></p><p>1）只能适用于那些不需要用户态处理的应用程序。</p><h4 id="方法四：DMA-辅助的-sendfile"><a href="#方法四：DMA-辅助的-sendfile" class="headerlink" title="方法四：DMA 辅助的 sendfile"></a>方法四：DMA 辅助的 sendfile</h4><hr><p>常规 sendfile 还有一次内核态的拷贝操作，能不能也把这次拷贝给去掉呢？</p><p>答案就是这种 DMA 辅助的 sendfile。</p><p>这种方法借助硬件的帮助，在数据从内核缓冲区到 socket 缓冲区这一步操作上，并不是拷贝数据，而是拷贝缓冲区描述符，待完成后，DMA 引擎直接将数据从内核缓冲区拷贝到协议引擎中去，避免了最后一次拷贝。</p><center><img src="/imagessss/linux/copy_dma_sendfile.jpg" alt=""></center><p><strong>缺陷：</strong></p><p>1）除了3.4 中的缺陷，还需要硬件以及驱动程序支持。</p><p>2）只适用于将数据从文件拷贝到套接字上。</p><h4 id="方法五：splice"><a href="#方法五：splice" class="headerlink" title="方法五：splice"></a>方法五：splice</h4><hr><p>splice 去掉 sendfile 的使用范围限制，可以用于任意两个文件描述符中传输数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#define _GNU_SOURCE         /* See feature_test_macros(7) */</span><br><span class="line"></span><br><span class="line">#include &lt;fcntl.h&gt;</span><br><span class="line"></span><br><span class="line">ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);</span><br></pre></td></tr></table></figure><p>但是 splice 也有局限，它使用了 Linux 的管道缓冲机制，所以，它的两个文件描述符参数中至少有一个必须是管道设备。</p><p>splice 提供了一种流控制的机制，通过预先定义的水印（watermark）来阻塞写请求，有实验表明，利用这种方法将数据从一个磁盘传输到另外一个磁盘会增加 30%-70% 的吞吐量，CPU负责也会减少一半。</p><p><strong>缺陷：</strong> </p><p>1）同样只适用于不需要用户态处理的程序</p><p>2）传输描述符至少有一个是管道设备。</p><h4 id="方法六：写时复制"><a href="#方法六：写时复制" class="headerlink" title="方法六：写时复制"></a>方法六：写时复制</h4><hr><p>在某些情况下，内核缓冲区可能被多个进程所共享，如果某个进程想要这个共享区进行 write 操作，由于 write 不提供任何的锁操作，那么就会对共享区中的数据造成破坏，写时复制就是 Linux 引入来保护数据的。</p><p>写时复制，就是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么就需要将其拷贝到自己的进程地址空间中，这样做并不影响其他进程对这块数据的操作，每个进程要修改的时候才会进行拷贝，所以叫写时拷贝。这种方法在某种程度上能够降低系统开销，如果某个进程永远不会对所访问的数据进行更改，那么也就永远不需要拷贝。</p><p><strong>缺陷：</strong></p><p>需要 MMU 的支持，MMU 需要知道进程地址空间中哪些页面是只读的，当需要往这些页面写数据时，发出一个异常给操作系统内核，内核会分配新的存储空间来供写入的需求。</p><h4 id="方法七：缓冲区共享"><a href="#方法七：缓冲区共享" class="headerlink" title="方法七：缓冲区共享"></a>方法七：缓冲区共享</h4><hr><p>这种方法完全改写 I/O 操作，因为传统 I/O 接口都是基于数据拷贝的，要避免拷贝，就去掉原先的那套接口，重新改写，所以这种方法是比较全面的零拷贝技术，目前比较成熟的一个方案是最先在 Solaris 上实现的 fbuf （Fast Buffer，快速缓冲区）。</p><p>Fbuf 的思想是每个进程都维护着一个缓冲区池，这个缓冲区池能被同时映射到程序地址空间和内核地址空间，内核和用户共享这个缓冲区池，这样就避免了拷贝。</p><center><img src="/imagessss/linux/copy_fbuf.jpg" alt=""></center><p><strong>缺陷：</strong></p><p>1）管理共享缓冲区池需要应用程序、网络软件、以及设备驱动程序之间的紧密合作</p><p>2）改写 API ，尚处于试验阶段。</p><h4 id="高性能网络-I-O-框架——netmap"><a href="#高性能网络-I-O-框架——netmap" class="headerlink" title="高性能网络 I/O 框架——netmap"></a>高性能网络 I/O 框架——netmap</h4><hr><p>Netmap 基于共享内存的思想，是一个高性能收发原始数据包的框架，由Luigi Rizzo 等人开发完成，其包含了内核模块以及用户态库函数。其目标是，不修改现有操作系统软件以及不需要特殊硬件支持，实现用户态和网卡之间数据包的高性能传递。</p><center><img src="/imagessss/linux/netmap.jpg" alt=""></center><p>在 Netmap 框架下，内核拥有数据包池，发送环\接收环上的数据包不需要动态申请，有数据到达网卡时，当有数据到达后，直接从数据包池中取出一个数据包，然后将数据放入此数据包中，再将数据包的描述符放入接收环中。内核中的数据包池，通过 mmap 技术映射到用户空间。用户态程序最终通过 netmap_if 获取接收发送环 netmap_ring，进行数据包的获取发送。</p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><hr><p>1、零拷贝本质上体现了一种优化的思想</p><p>2、直接 I/O，mmap，sendfile，DMA sendfile，splice，缓冲区共享，写时复制……</p><h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><hr><p>（1）Linux 中的零拷贝技术，第 1 部分<br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy1/index.html</a><br>（2）Linux 中的零拷贝技术，第 2 部分<br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/l-cn-zerocopy2/</a><br>（3）Netmap 原理<br><a href="http://www.tuicool.com/articles/MnIRbuU" target="_blank" rel="noopener">http://www.tuicool.com/articles/MnIRbuU</a></p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/imagessss/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 零拷贝 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OVS 总体架构、源码结构及数据流程全面解析</title>
      <link href="/2017/12/23/tech/net/ovs/OVS_%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84%E3%80%81%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90/"/>
      <url>/2017/12/23/tech/net/ovs/OVS_%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84%E3%80%81%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B%E5%85%A8%E9%9D%A2%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>在前文「从 Bridge 到 OVS」中，我们已经对 OVS 进行了一番探索。本文决定从 OVS 的整体架构到各个组件都进行一个详细的介绍。</p><h3 id="OVS-架构"><a href="#OVS-架构" class="headerlink" title="OVS 架构"></a>OVS 架构</h3><hr><p>OVS 是产品级的虚拟交换机，大量应用在生产环境中，支撑整个数据中心虚拟网络的运转。OVS 基于 SDN 的思想，将整个核心架构分为控制面和数据面，数据面负责数据的交换工作，控制面实现交换策略，指导数据面工作。</p><center><img src="/images/virt/ovs_arch.jpg" alt=""></center><p>从整体上看，OVS 可以划分为三大块，管理面、数据面和控制面。</p><p>数据面就是以用户态的 ovs-vswitchd 和内核态的 datapath 为主的转发模块，以及与之相关联的数据库模块 ovsdb-server，控制面主要是由 ovs-ofctl 模块负责，基于 OpenFlow 协议与数据面进行交互。而管理面则是由 OVS 提供的各种工具来负责，这些工具的提供也是为了方便用户对底层各个模块的控制管理，提高用户体验。下面就对这些工具进行一个逐一的阐述。</p><p><strong>ovs-ofctl：</strong> 这个是控制面的模块，但本质上它也是一个管理工具，主要是基于 OpenFlow 协议对 OpenFlow 交换机进行监控和管理，通过它可以显示一个 OpenFlow 交换机的当前状态，包括功能、配置和表中的项。使用时，有很多参数，我们可以通过 ovs-ofctl –help 查看。</p><p>常用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ovs-ofctl show switch-name ：输出交换机信息，包括其流量表和端口信息。</span><br><span class="line"></span><br><span class="line">ovs-ofctl dump-ports switch-name：输出交换机的端口统计信息，包括收发包、丢包、错误包等数量。</span><br><span class="line"></span><br><span class="line">ovs-ofctl add-flow switch-name：为交换机配置流策略。</span><br></pre></td></tr></table></figure><p><strong>ovs-dpctl：</strong> 用来配置交换机的内核模块 datapath，它可以创建，修改和删除 datapath，一般，单个机器上的 datapath 有 256 条（0-255）。一条 datapath 对应一个虚拟网络设备。该工具还可以统计每条 datapath 上的设备通过的流量，打印流的信息等，更过参数通过 ovs-dpctl –help 查看。</p><p>常用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ovs-dpctl show ：显示所有 datapath 的基本信息。</span><br><span class="line"></span><br><span class="line">ovs-dpctl dump-dps ：显示所有 datapath 的名字。</span><br><span class="line"></span><br><span class="line">ovs-dpctl dump-flows DP ：显示一条 datapath DP 上的流信息。</span><br></pre></td></tr></table></figure><p><strong>ovs-appctl：</strong> 查询和控制运行中的 OVS 守护进程，包括 ovs-switchd，datapath，OpenFlow 控制器等，兼具 ovs-ofctl、ovs-dpctl 的功能，是一个非常强大的命令。ovs-vswitchd 等进程启动之后就以一个守护进程的形式运行，为了能够很好的让用户控制这些进程，就有了这个命令。详细可以 ovs-appctl –help 查看。</p><p><strong>ovs-vsctl：</strong> 查询和更新 ovs-vswitchd 的配置，这也是一个很强大的命令，网桥、端口、协议等相关的命令都由它来完成。此外，还负责和 ovsdb-server 相关的数据库操作。</p><p>常用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ovs-vsctl show ：显示主机上已有的网桥及端口信息。</span><br><span class="line"></span><br><span class="line">ovs-vsctl add-br br0：添加网桥 br0。</span><br></pre></td></tr></table></figure><p><strong>ovsdb-client：</strong> 访问 ovsdb-server 的客户端程序，通过 ovsdb-server 执行一些数据库操作。</p><p>常用命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ovsdb-client dump：用来查看ovsdb内容。</span><br><span class="line"></span><br><span class="line">ovsdb-client transact ：用来执行一条类 sql。</span><br></pre></td></tr></table></figure></p><p><strong>ovsdb-tool：</strong> 和 ovsdb-client 要借助 ovsdb-server 才能进行相关数据库操作不同，ovsdb-tool 可以直接操作数据库。</p><h3 id="OVS-源码结构"><a href="#OVS-源码结构" class="headerlink" title="OVS 源码结构"></a>OVS 源码结构</h3><hr><p>OVS 源码结构中，主要包含以下几个主要的模块，数据交换逻辑在 vswitchd 和 datapath 中实现，vswitchd 是最核心的模块，OpenFlow 的相关逻辑都在 vswitchd 中实现，datapath 则不是必须的模块。ovsdb 用于存储 vswitch 本身的配置信息，如端口、拓扑、规则等。控制面部分采用的是 OVS 自家实现的 OVN，和其他控制器相比，OVN 对 OVS 和 OpenStack 有更好的兼容性和性能。</p><center><img src="/images/virt/ovs_module.jpg" alt=""></center><p>从图中可以看出 OVS 的分层结构，最上层 vswitchd 主要与 ovsdb 通信，做配置下发和更新等，中间层是 ofproto ，用于和 OpenFlow 控制器通信，并基于下层的 ofproto provider 提供的接口，完成具体的设备操作和流表操作等工作。</p><p>dpif 层实现对流表的操作。</p><p>netdev 层实现了对网络设备（如 Ethernet）的抽象，基于 netdev provider 接口实现多种不同平台的设备，如 Linux 内核的 system, tap, internal 等，dpdk 系的 vhost, vhost-user 等，以及隧道相关的 gre, vxlan 等。</p><h3 id="数据转发流程"><a href="#数据转发流程" class="headerlink" title="数据转发流程"></a>数据转发流程</h3><hr><p>通过一个例子来看看 OVS 中数据包是如何进行转发的。</p><center><img src="/images/virt/ovs_dataflow.jpg" alt=""></center><p>1）ovs 的 datapath 接收到从 ovs 连接的某个网络端口发来的数据包，从数据包中提取源/目的 IP、源/目的 MAC、端口等信息。</p><p>2）ovs 在内核态查看流表结构（通过 hash），如果命中，则快速转发。</p><p>3）如果没有命中，内核态不知道如何处置这个数据包，所以，通过 netlink upcall 机制从内核态通知用户态，发送给 ovs-vswitchd 组件处理。</p><p>4）ovs-vswitchd 查询用户态精确流表和模糊流表，如果还不命中，在 SDN 控制器接入的情况下，经过 OpenFlow 协议，通告给控制器，由控制器处理。</p><p>5）如果模糊命中， ovs-vswitchd 会同时刷新用户态精确流表和内核态精确流表，如果精确命中，则只更新内核态流表。</p><p>6）刷新后，重新把该数据包注入给内核态 datapath 模块处理。</p><p>7）datapath 重新发起选路，查询内核流表，匹配；报文转发，结束。<br>总结</p><p>OVS 为了方便用户操作，提供了很多管理工具，我们平常在使用过程中只需记住每个工具的作用，具体的命令可以使用 -h 或 –help 查看。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> OVS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
            <tag> OVS </tag>
            
            <tag> OpenFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从 Bridge 到 OVS，探索虚拟交换机</title>
      <link href="/2017/12/17/tech/net/ovs/%E4%BB%8E_Bridge_%E5%88%B0_OVS%EF%BC%8C%E6%8E%A2%E7%B4%A2%E8%99%9A%E6%8B%9F%E4%BA%A4%E6%8D%A2%E6%9C%BA/"/>
      <url>/2017/12/17/tech/net/ovs/%E4%BB%8E_Bridge_%E5%88%B0_OVS%EF%BC%8C%E6%8E%A2%E7%B4%A2%E8%99%9A%E6%8B%9F%E4%BA%A4%E6%8D%A2%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>和物理网络一样，虚拟网络要通信，必须借助一些交换设备来转发数据。因此，对于网络虚拟化来说，交换设备的虚拟化是很关键的一环。</p><p>上文「网络虚拟化」已经大致介绍了 Linux 内核为了满足网络虚拟化的要求，实现了一套虚拟交换设备——Bridge。本文重点介绍下 Bridge 的加强版——Open vSwitch（OVS），并从 Bridge 过渡到 OVS 的缘由讲起，让大家有个全面的认识。</p><p>借助 Linux Bridge 功能，同主机或跨主机的虚拟机之间能够轻松实现通信，也能够让虚拟机访问到外网，这就是我们所熟知的桥接模式，一般在装 VMware 虚拟机或者 VirtualBox 虚拟机的时候，都会提示我们要选择哪种模式，常用的两种模式是桥接和 NAT。</p><p>NAT 也很好理解，可以简单理解为当虚拟机启用了 NAT 模式之后，宿主机便通过 DHCP 为其生成可以访问外网的 IP，当 VM 访问外网的时候，就可以用该 IP 访问，其实就是宿主机为其做了地址转换。更详细的内容请自行搜索了解。</p><p>物理交换机有个重要的功能，就是虚拟局域网（VLAN），是对局域网（LAN）的软件化升级。一般，两台计算机通过一台交换机连接在一起就构成了一个 LAN。</p><p>一个 LAN 表示一个广播域，这意味着这个 LAN 中的任何节点发的数据包，其他节点都能收到，这有两个问题，一个是容易形成广播风暴，造成网络拥塞，另一个是广播包无法隔离，比如节点 B 不想接收节点 A 的包，但节点 A 强行要发，这就有点说不过去了。</p><p>解决这个问题的方案就是 VLAN，VLAN 能够对广播包进行有效隔离，它的做法是从软件上将交换机的端口虚拟出多个子端口，用 tag 来标记，相当于将交换机的端口划分多个 LAN，同一个 LAN 中的节点发出的数据包打上本 LAN 的 tag，这样，其他 LAN 中的节点就无法收到包，达到隔离的目的。</p><p>Bridge 本身是支持 VLAN 功能的，如下图所示，通过配置，Bridge 可以将一个物理网卡设备 eth0 划分成两个子设备 eth0.10，eth0.20，分别挂到 Bridge 虚拟出的两个 VLAN 上，VLAN id 分别为 VLAN 10 和 VLAN 20。同样，两个 VM 的虚拟网卡设备 vnet0 和 vnet 1 也分别挂到相应的 VLAN 上。这样配好的最终效果就是 VM1 不能和 VM2 通信了，达到了隔离。</p><center><img src="/images/virt/net_vlan.png" alt=""></center><p>Linux Bridge + VLAN 便可以构成一个和物理交换机具备相同功能的虚拟交换机了。对于网络虚拟化来说，Bridge 已经能够很好地充当交换设备的角色了。</p><p>但是为什么还有很多厂商都在做自己的虚拟交换机，比如比较流行的有 VMware virtual switch、Cisco Nexus 1000V，以及 Open vSwitch。究其原因，主要有以下几点（我们重点关注 OVS）：</p><p><strong>1）</strong> 方便网络管理与监控。OVS 的引入，可以方便管理员对整套云环境中的网络状态和数据流量进行监控，比如可以分析网络中流淌的数据包是来自哪个 VM、哪个 OS 及哪个用户，这些都可以借助 OVS 提供的工具来达到。</p><p><strong>2）</strong> 加速数据包的寻路与转发。相比 Bridge 单纯的基于 MAC 地址学习的转发规则，OVS 引入流缓存的机制，可以加快数据包的转发效率。</p><p><strong>3）</strong> 基于 SDN 控制面与数据面分离的思想。上面两点其实都跟这一点有关，OVS 控制面负责流表的学习与下发，具体的转发动作则有数据面来完成。可扩展性强。</p><p><strong>4）</strong> 隧道协议支持。Bridge 只支持 VxLAN，OVS 支持 gre/vxlan/IPsec 等。</p><p><strong>5）</strong> 适用于 Xen、KVM、VirtualBox、VMware 等多种 Hypervisors。</p><p>……</p><p>除此之外，OVS 还有很多高级特性，详情可以查阅官网自行了解。</p><p>下面简单看下 OVS 的整体架构，如下图所示，OVS 在 Linux 用户态和内核态都实现了相应的模块，用户态主要组件有数据库服务 ovsdb-server 和守护进程 ovs-vswitchd。内核态中实现了 datapath 模块。</p><center><img src="/images/virt/ovs_path.jpg" alt=""></center><p>其中， ovs-vswitchd 和 datapath 共同构成了 OVS 的数据面，控制面由 controller 模块来完成，controller 一般表示的是 OpenFlow 控制器，在 OVS 中，它可以借由第三方来完成，只要支持 OpenFlow 协议即可。</p><p>这里额外提一点，很多的一些产品级的虚拟交换机都是自身集成了控制器，比如 Cisco 1000V 的 Virtual Supervisor Manager(VSM)，VMware 的分布式交换机中的 vCenter，而 OVS 是把这个事交由第三方去做，这么做的意义还是比较大的，可以让自己的产品很好地融入到各种解决方案中。</p><h4 id="OpenFlow"><a href="#OpenFlow" class="headerlink" title="OpenFlow"></a><strong>OpenFlow</strong></h4><hr><p>OpenFlow 是控制面和数据面通信的一套协议，我们常常把支持 OpenFlow 协议的交换机称为 OpenFlow 交换机，控制器称为 OpenFlow 控制器，业界比较知名的 OpenFlow 控制器有 OpenDaylight、ONOS 等。</p><p>OpenFlow 是一个独立的完整的流表协议，不依赖于 OVS，OVS 只是支持 OpenFlow 协议，有了支持，就可以使用 OpenFlow 控制器来管理 OVS 中的流表。OpenFlow 不仅仅支持虚拟交换机，某些硬件交换机也支持 OpenFlow 协议。</p><h4 id="ovs-vswitchd"><a href="#ovs-vswitchd" class="headerlink" title="ovs-vswitchd"></a><strong>ovs-vswitchd</strong></h4><hr><p>ovs-vswitchd 是 OVS 的核心组件，它和内核模块 datapath 共同构成了 OVS 的数据面。它使用 OpenFlow 协议与 OpenFlow 控制器通信，使用 OVSDB 协议与 ovsdb-server 通信，使用 netlink 和 datapath 内核模块通信。</p><h4 id="ovsdb-server"><a href="#ovsdb-server" class="headerlink" title="ovsdb-server"></a><strong>ovsdb-server</strong></h4><hr><p>ovsdb-server 是 OVS 轻量级的数据库服务，用于整个 OVS 的配置信息，包括接口、交换内容、VLAN 等，ovs-vswitchd 根据这些配置信息工作。</p><h4 id="OpenFlow-控制器"><a href="#OpenFlow-控制器" class="headerlink" title="OpenFlow 控制器"></a><strong>OpenFlow 控制器</strong></h4><hr><p>OpenFlow 控制器可以通过 OpenFlow 协议连接到任何支持 OpenFlow 的交换机，比如 OVS 。控制器通过向交换机下发流表规则来控制数据流向。</p><h4 id="Kernel-Datapath"><a href="#Kernel-Datapath" class="headerlink" title="Kernel Datapath"></a><strong>Kernel Datapath</strong></h4><hr><p>datapath 内核模块和 ovs-vswitchd 是相互协作工作的，datapath 负责具体的收发包，而 ovs-vswitchd 通过 controller 下发的流表规则指导 datapath 如何转发包。</p><p>举个例子，datapath 从主机物理网卡 NIC 或者 VM 的 虚拟网卡 vNIC 收到包，如果是第一次收到包，datapath 不知道怎么处理这个包，于是将其丢给  ovs-vswitchd ， ovs-vswitchd 决定该如何处理这个包之后又丢给 datapath，datapath 根据 ovs-vswitchd 的指示执行相应的动作，是丢弃还是从哪个口传出去。同时，ovs-vswitchd 会让 datapath 缓存好这个包的动作，下次再来就可以直接执行动作。</p><p>如果不是第一次收到包，就是按照之前缓存好的动作执行，这样极大地提高了数据处理的速度。</p><p>本文先对 OVS 有个初步印象，下文再详细介绍 OVS 的其他组件。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> OVS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
            <tag> OVS </tag>
            
            <tag> Bridge </tag>
            
            <tag> OpenFlow </tag>
            
            <tag> VLAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文搞懂网络虚拟化</title>
      <link href="/2017/12/14/tech/cloud/virt/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
      <url>/2017/12/14/tech/cloud/virt/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E7%BD%91%E7%BB%9C%E8%99%9A%E6%8B%9F%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>网络虚拟化相对计算、存储虚拟化来说是比较抽象的，以我们在学校书本上学的那点网络知识来理解网络虚拟化可能是不够的。</p><p>在我们的印象中，网络就是由各种网络设备（如交换机、路由器）相连组成的一个网状结构，世界上的任何两个人都可以通过网络建立起连接。</p><p>带着这样一种思路去理解网络虚拟化可能会感觉云里雾里——这样一个庞大的网络如何实现虚拟化？</p><p>其实，网络虚拟化更多关注的是数据中心网络、主机网络这样比较「细粒度」的网络，所谓细粒度，是相对来说的，是深入到某一台物理主机之上的网络结构来谈的。</p><p>如果把传统的网络看作「宏观网络」的话，那网络虚拟化关注的就是「微观网络」。网络虚拟化的目的，是要节省物理主机的网卡设备资源。从资源这个角度去理解，可能会比较好理解一点。</p><h3 id="传统网络架构"><a href="#传统网络架构" class="headerlink" title="传统网络架构"></a>传统网络架构</h3><hr><p>在传统网络环境中，一台物理主机包含一个或多个网卡（NIC），要实现与其他物理主机之间的通信，需要通过自身的 NIC 连接到外部的网络设施，如交换机上，如下图所示。</p><center><img src="/images/virt/net_tran.jpg" alt=""><br>传统网络（图片来源于网络，侵权必删）<br></center><p>这种架构下，为了对应用进行隔离，往往是将一个应用部署在一台物理设备上，这样会存在两个问题，1）是某些应用大部分情况可能处于空闲状态，2）是当应用增多的时候，只能通过增加物理设备来解决扩展性问题。不管怎么样，这种架构都会对物理资源造成极大的浪费。</p><h3 id="虚拟化网络架构"><a href="#虚拟化网络架构" class="headerlink" title="虚拟化网络架构"></a>虚拟化网络架构</h3><hr><p>为了解决这个问题，可以借助虚拟化技术对一台物理资源进行抽象，将一张物理网卡虚拟成多张虚拟网卡（vNIC），通过虚拟机来隔离不同的应用。</p><p>这样对于上面的问题 1），可以利用虚拟化层 Hypervisor 的调度技术，将资源从空闲的应用上调度到繁忙的应用上，达到资源的合理利用；针对问题 2），可以根据物理设备的资源使用情况进行横向扩容，除非设备资源已经用尽，否则没有必要新增设备。这种架构如下所示。</p><center><img src="/images/virt/net_virt.jpg" alt=""><br>虚拟化网络（图片来源于网络，侵权必删）<br></center><p>其中虚拟机与虚拟机之间的通信，由虚拟交换机完成，虚拟网卡和虚拟交换机之间的链路也是虚拟的链路，整个主机内部构成了一个虚拟的网络，如果虚拟机之间涉及到三层的网络包转发，则又由另外一个角色——虚拟路由器来完成。</p><p>一般，这一整套虚拟网络的模块都可以独立出去，由第三方来完成，如其中比较出名的一个解决方案就是 Open vSwitch（OVS）。</p><p>OVS 的优势在于它基于 SDN 的设计原则，方便虚拟机集群的控制与管理，另外就是它分布式的特性，可以「透明」地实现跨主机之间的虚拟机通信，如下是跨主机启用 OVS 通信的图示。</p><center><img src="/images/virt/net_dis.jpg" alt=""><br>分布式虚拟交换机（图片来源于网络，侵权必删）<br></center><p>总结下来，网络虚拟化主要解决的是虚拟机构成的网络通信问题，完成的是各种网络设备的虚拟化，如网卡、交换设备、路由设备等。</p><h3 id="Linux-下网络设备虚拟化的几种形式"><a href="#Linux-下网络设备虚拟化的几种形式" class="headerlink" title="Linux 下网络设备虚拟化的几种形式"></a>Linux 下网络设备虚拟化的几种形式</h3><hr><p>为了完成虚拟机在同主机和跨主机之间的通信，需要借助某种“桥梁”来完成用户态到内核态（Guest 到 Host）的数据传输，这种桥梁的角色就是由虚拟的网络设备来完成，上面介绍了一个第三方的开源方案——OVS，它其实是一个融合了各种虚拟网络设备的集大成者，是一个产品级的解决方案。</p><p>但 Linux 本身由于虚拟化技术的演进，也集成了一些虚拟网络设备的解决方案，主要有以下几种：</p><h4 id="（1）TAP-TUN-VETH"><a href="#（1）TAP-TUN-VETH" class="headerlink" title="（1）TAP/TUN/VETH"></a>（1）TAP/TUN/VETH</h4><hr><p>TAP/TUN 是 Linux 内核实现的一对虚拟网络设备，TAP 工作在二层，TUN 工作在三层。Linux 内核通过 TAP/TUN 设备向绑定该设备的用户空间程序发送数据，反之，用户空间程序也可以像操作物理网络设备那样，向 TAP/TUN 设备发送数据。</p><p>基于 TAP 驱动，即可实现虚拟机 vNIC 的功能，虚拟机的每个 vNIC 都与一个 TAP 设备相连，vNIC 之于 TAP 就如同 NIC 之于 eth。</p><p>当一个 TAP 设备被创建时，在 Linux 设备文件目录下会生成一个对应的字符设备文件，用户程序可以像打开一个普通文件一样对这个文件进行读写。</p><p>比如，当对这个 TAP 文件执行 write 操作时，相当于 TAP 设备收到了数据，并请求内核接受它，内核收到数据后将根据网络配置进行后续处理，处理过程类似于普通物理网卡从外界收到数据。当用户程序执行 read 请求时，相当于向内核查询 TAP 设备是否有数据要发送，有的话则发送，从而完成 TAP 设备的数据发送。</p><p>TUN 则属于网络中三层的概念，数据收发过程和 TAP 是类似的，只不过它要指定一段 IPv4 地址或 IPv6 地址，并描述其相关的配置信息，其数据处理过程也是类似于普通物理网卡收到三层 IP 报文数据。</p><p>VETH 设备总是成对出现，一端连着内核协议栈，另一端连着另一个设备，一个设备收到内核发送的数据后，会发送到另一个设备上去，这种设备通常用于容器中两个 namespace 之间的通信。</p><h4 id="（2）Bridge"><a href="#（2）Bridge" class="headerlink" title="（2）Bridge"></a>（2）Bridge</h4><hr><p>Bridge 也是 Linux 内核实现的一个工作在二层的虚拟网络设备，但不同于 TAP/TUN 这种单端口的设备，Bridge 实现为多端口，本质上是一个虚拟交换机，具备和物理交换机类似的功能。</p><p>Bridge 可以绑定其他 Linux 网络设备作为从设备，并将这些从设备虚拟化为端口，当一个从设备被绑定到 Bridge 上时，就相当于真实网络中的交换机端口上插入了一根连有终端的网线。</p><p>如下图所示，Bridge 设备 br0 绑定了实际设备 eth0 和 虚拟设备设备 tap0/tap1，当这些从设备接收到数据时，会发送给 br0 ，br0 会根据 MAC 地址与端口的映射关系进行转发。</p><center><img src="/images/virt/net_br.png" alt=""><br>Bridge 与 TAP/TUN 的关系<br></center><p>因为 Bridge 工作在二层，所以绑定到它上面的从设备 eth0、tap0、tap1 均不需要设 IP，但是需要为 br0 设置 IP，因为对于上层路由器来说，这些设备位于同一个子网，需要一个统一的 IP 将其加入路由表中。</p><p>这里有人可能会有疑问，Bridge 不是工作在二层吗，为什么会有 IP 的说法？其实 Bridge 虽然工作在二层，但它只是 Linux 网络设备抽象的一种，能设 IP 也不足为奇。</p><p>对于实际设备 eth0 来说，本来它是有自己的 IP 的，但是绑定到 br0 之后，其 IP 就生效了，就和 br0 共享一个 IP 网段了，在设路由表的时候，就需要将 br0 设为目标网段的地址。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>传统网络架构到虚拟化的网络架构，可以看作是宏观网络到微观网络的过渡</p><p>TAP/TUN/VETH、Bridge 这些虚拟的网络设备是 Linux 为了实现网络虚拟化而实现的网络设备模块，很多的云开源项目的网络功能都是基于这些技术做的，比如 Neutron、Docker network 等。</p><p>OVS 是一个开源的成熟的产品级分布式虚拟交换机，基于 SDN 的思想，被大量应用在生产环境中。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 03 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
            <tag> OVS </tag>
            
            <tag> Bridge </tag>
            
            <tag> tap </tag>
            
            <tag> tun </tag>
            
            <tag> veth-pair </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2017 年除了人工智能，这门技术也在茁壮生长</title>
      <link href="/2017/12/10/tech/cloud/2017_%E5%B9%B4%E9%99%A4%E4%BA%86%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%8C%E8%BF%99%E9%97%A8%E6%8A%80%E6%9C%AF%E4%B9%9F%E5%9C%A8%E8%8C%81%E5%A3%AE%E7%94%9F%E9%95%BF/"/>
      <url>/2017/12/10/tech/cloud/2017_%E5%B9%B4%E9%99%A4%E4%BA%86%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%8C%E8%BF%99%E9%97%A8%E6%8A%80%E6%9C%AF%E4%B9%9F%E5%9C%A8%E8%8C%81%E5%A3%AE%E7%94%9F%E9%95%BF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>没错，这个标题就是个标题党，目的就是为了让你点进来看看。</p><p>2017 年是人工智能元年，我们也能看到各大互联网公司对于人工智能的大布局。但人工智能再怎么牛逼，别忘了它的底层基础设施是什么，没错，就是云计算，少了云计算的底层支撑，人工智能也只能活在书本里。虽然说云计算经过这么多年的沉淀，技术总体上来说已经比较成熟，但有一门技术仍然呈现欣欣向荣之势，未来发展仍有无限可能。</p><p>它就是以 docker 为首的容器技术。</p><p>不知道大家关注最近这两天的 KubeCon 2017 北美峰会没有，会上提得最多的也是容器技术，其中最大的新闻莫过于 OpenStack 基金会发布了最新开源容器项目 Kata Containers。看到这个消息，我的第一反应是「为啥最近容器圈的动作这么多」。</p><p>我们把时间拉回到 11 月底的中国开源年会上，当时阿里正式开源了自家自研容器项目 Pouch。再把时间拉到差不多两个月前（10月17日）的 DockerCon EU 2017 大会上，彼时 docker 官方宣布全面支持 kubernates。类似这样大大小小的动作，在 2017 年还发生了很多，这一切的动作都在说明容器的重要性，未来的可期待指数可以说不亚于人工智能。</p><p>下面简单介绍下 Kata 和阿里的 Pouch 到底是个怎样的角色，我也仅限于网络上各种资讯的了解，信息传达难免会有失误，如果大家觉得有问题可以留言指出。</p><h3 id="Kata-Containers-是什么"><a href="#Kata-Containers-是什么" class="headerlink" title="Kata Containers 是什么"></a>Kata Containers 是什么</h3><hr><p>Kata 官方宣称这是同时兼具容器的速度和虚拟机安全的全新容器解决方案，旨在将虚拟机的安全优势与容器的速度和可管理性统一起来，其建立在 Intel 的 Clear Containers 技术和 Hyper 的 runV 虚拟机管理程序运行时基础之上。</p><center><img src="/images/cloud/kata.jpg" alt=""></center><p>Kata 的特点是什么？总体来讲，Kata 解决的是容器的安全问题。众所周知，当前容器技术旨在实现在一个虚拟机之上运行多个用户的、多个应用的容器实例，不同实例之间共享同一个虚拟机操作系统内核并采用 Namespaces 来隔离，但这种方式很难保证各实例彼此之间的完全隔离，存在安全隐患。</p><p>Kata 的解决方案是意图为每个容器实例提供一个专属的、高度轻量化的虚拟机操作系统内核来解决这个问题。让某一个用户的、一个应用的一个或多个容器实例单独跑在这个专属的虚拟机内核之上，这样不同用户、不同应用之间都是使用独占的虚拟机，不会共享同一个操作系统内核，这样就确保了安全性。</p><center><img src="/images/cloud/kata_hyper.png" alt=""></center><p>另外还有一点值得注意的是，Kata 的设计初衷强调了能够无缝、便捷的与 OpenStack 和 Kubernetes 集成的能力，这为 OpenStack 、Kubernetes 和 Container 更好的融合铺平了道路。</p><center><img src="/images/cloud/kata_arch.jpg" alt=""></center><p>更详细的内容可以访问：</p><p><a href="http://www/katacontainers.io/" target="_blank" rel="noopener">http://www/katacontainers.io/</a><br><a href="https://github.com/hyperhq/runv" target="_blank" rel="noopener">https://github.com/hyperhq/runv</a></p><h3 id="Pouch-是什么"><a href="#Pouch-是什么" class="headerlink" title="Pouch 是什么"></a>Pouch 是什么</h3><hr><p>相比 Kata，阿里的 Pouch 就没那么新鲜了，只不过是换了个马甲而已。</p><p>为什么这么说，因为 Pouch 并不是全新的容器解决方案，而是已经在阿里内部经过千锤百炼的老牌容器技术 t4。2011 年，Linux 内核的 namespace、cgroup 等技术开始成熟，LXC 等容器运行时技术也在同期诞生，阿里作为一家技术公司，在当时便基于 LXC 自研了自己的容器技术 t4，并以产品的形式给内部提供服务。</p><p>t4 就是 Pouch 的前身，从时间节点上看，t4 面世比 docker 要早两年，但 t4 有很多问题没有解决，譬如说没有镜像机制。2013 年，docker 横空出世，其带有镜像创新的容器技术，似一阵飓风，所到之处，国内外无不叫好，阿里也不例外，便在现有技术体系结构的基础上融入了 docker 的镜像技术，慢慢打磨，演变成今天的 Pouch。</p><p>Pouch 针对自身的业务场景对镜像的下载和分发进行了创新。由于阿里的业务体量庞大，集群规模数以万计，这就会存在一个问题就是镜像的下载和分发效率会很低，所以针对此，阿里在 Pouch 中集成了一个镜像分发工具蜻蜓（Dragonfly），蜻蜓基于智能 P2P 技术的文件分发系统，解决了大规模文件分发场景下分发耗时、成功率低、带宽浪费等难题。</p><center><img src="/images/cloud/pouch.jpg" alt=""></center><p>Pouch 的架构主要考虑到两个方面，一方面是如何对接容器编排系统，另一方面是如何加强容器运行时，第一点让 Pouch 有了对外可扩展的能力，譬如可以原生支持 Kubernetes 等编排系统。第二点可以增加 Pouch 对虚拟机和容器的统一管理，让其适应更多的业务场景。</p><center><img src="/images/cloud/pouch_arch.jpg" alt=""></center><p>更详细的内容可以访问：</p><p><a href="https://github.com/alibaba/pouch" target="_blank" rel="noopener">https://github.com/alibaba/pouch</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>从几种举动中，我们可以看出，未来容器发展具有两大重要方向，分别是容器编排技术和容器的安全加强。这些都是在寻求一种更好的、更有效率的方式来为上层的业务提供更可靠、更安全的支撑。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 01 云计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 容器 </tag>
            
            <tag> Docker </tag>
            
            <tag> Kata </tag>
            
            <tag> Pouch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存虚拟化</title>
      <link href="/2017/11/27/tech/cloud/virt/%E5%86%85%E5%AD%98%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
      <url>/2017/11/27/tech/cloud/virt/%E5%86%85%E5%AD%98%E8%99%9A%E6%8B%9F%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><hr><p>我们知道，早期的计算机内存，只有物理内存，而且空间是极其有限的，每个应用或进程在使用内存时都得小心翼翼，不能覆盖别的进程的内存区。</p><p>为了避免这些问题，就提出了虚拟内存的概念，其抽象了物理内存，相当于对物理内存进行了虚拟化，保证每个进程都被赋予一块连续的，超大的（根据系统结构来定，32 位系统寻址空间为 2^32，64 位系统为 2^64）虚拟内存空间，进程可以毫无顾忌地使用内存，不用担心申请内存会和别的进程冲突，因为底层有机制帮忙处理这种冲突，能够将虚拟地址根据一个页表映射成相应的物理地址。</p><p>这种机制正是虚拟化软件做的事，也就是 MMU 内存管理单元。</p><center><img src="/images/virt/mem_virt.png" alt=""></center><p>本文要说的不是这种虚拟内存，而是基于虚拟机的内存虚拟化，它们本质上是一样的，通过对虚拟内存的理解，再去理解内存虚拟化就比较容易了。</p><p>结合前面的文章，我们知道，虚拟化分为软件虚拟化和硬件虚拟化，而且遵循 intercept 和 virtualize 的规律。</p><p>内存虚拟化也分为基于软件的内存虚拟化和硬件辅助的内存虚拟化，其中，常用的基于软件的内存虚拟化技术为「影子页表」技术，硬件辅助内存虚拟化技术为 Intel 的 EPT（Extend Page Table，扩展页表）技术。</p><p>为了讲清楚这两门技术，我们从简易到复杂，循序渐进，逐步揭开其神秘面纱。</p><h3 id="常规软件内存虚拟化"><a href="#常规软件内存虚拟化" class="headerlink" title="常规软件内存虚拟化"></a>常规软件内存虚拟化</h3><hr><p>虚拟机本质上是 Host 机上的一个进程，按理说应该可以使用 Host 机的虚拟地址空间，但由于在虚拟化模式下，虚拟机处于非 Root 模式，无法直接访问 Root 模式下的 Host 机上的内存。</p><p>这个时候就需要 VMM 的介入，VMM 需要 intercept （截获）虚拟机的内存访问指令，然后 virtualize（模拟）Host 上的内存，相当于 VMM 在虚拟机的虚拟地址空间和 Host 机的虚拟地址空间中间增加了一层，即虚拟机的物理地址空间，也可以看作是 Qemu 的虚拟地址空间（稍微有点绕，但记住一点，虚拟机是由 Qemu 模拟生成的就比较清楚了）。</p><p>所以，内存软件虚拟化的目标就是要将虚拟机的虚拟地址（Guest Virtual Address, GVA）转化为 Host 的物理地址（Host Physical Address, HPA），中间要经过虚拟机的物理地址（Guest Physical Address, GPA）和 Host 虚拟地址（Host Virtual Address）的转化，即：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GVA -&gt; GPA -&gt; HVA -&gt; HPA</span><br></pre></td></tr></table></figure></p><p>其中前两步由虚拟机的系统页表完成，中间两步由 VMM 定义的映射表（由数据结构 kvm_memory_slot 记录）完成，它可以将连续的虚拟机物理地址映射成非连续的 Host 机虚拟地址，后面两步则由 Host 机的系统页表完成。如下图所示。</p><center><img src="/images/virt/mem_shadow.png" alt=""></center><p>这样做得目的有两个：</p><ol><li><p>提供给虚拟机一个从零开始的连续的物理内存空间。</p></li><li><p>在各虚拟机之间有效隔离、调度以及共享内存资源。</p></li></ol><h3 id="影子页表技术"><a href="#影子页表技术" class="headerlink" title="影子页表技术"></a>影子页表技术</h3><hr><p>接上图，我们可以看到，传统的内存虚拟化方式，虚拟机的每次内存访问都需要 VMM 介入，并由软件进行多次地址转换，其效率是非常低的。因此才有了影子页表技术和 EPT 技术。</p><p>影子页表简化了地址转换的过程，实现了 Guest 虚拟地址空间到 Host 物理地址空间的直接映射。</p><p>要实现这样的映射，必须为 Guest 的系统页表设计一套对应的影子页表，然后将影子页表装入 Host 的 MMU 中，这样当 Guest 访问 Host 内存时，就可以根据 MMU 中的影子页表映射关系，完成 GVA 到 HPA 的直接映射。而维护这套影子页表的工作则由 VMM 来完成。</p><p>由于 Guest 中的每个进程都有自己的虚拟地址空间，这就意味着 VMM 要为 Guest 中的每个进程页表都维护一套对应的影子页表，当 Guest 进程访问内存时，才将该进程的影子页表装入 Host 的 MMU 中，完成地址转换。</p><p>我们也看到，这种方式虽然减少了地址转换的次数，但本质上还是纯软件实现的，效率还是不高，而且 VMM 承担了太多影子页表的维护工作，设计不好。</p><p>为了改善这个问题，就提出了基于硬件的内存虚拟化方式，将这些繁琐的工作都交给硬件来完成，从而大大提高了效率。</p><h3 id="EPT-技术"><a href="#EPT-技术" class="headerlink" title="EPT 技术"></a>EPT 技术</h3><hr><p>这方面 Intel 和 AMD 走在了最前面，Intel 的 EPT 和 AMD 的 NPT 是硬件辅助内存虚拟化的代表，两者在原理上类似，本文重点介绍一下 EPT 技术。</p><p>如下图是 EPT 的基本原理图示，EPT 在原有 CR3 页表地址映射的基础上，引入了 EPT 页表来实现另一层映射，这样，GVA-&gt;GPA-&gt;HPA 的两次地址转换都由硬件来完成。</p><center><img src="/images/virt/ept.png" alt=""></center><p>这里举一个小例子来说明整个地址转换的过程。假设现在 Guest 中某个进程需要访问内存，CPU 首先会访问 Guest 中的 CR3 页表来完成 GVA 到 GPA 的转换，如果 GPA 不为空，则 CPU 接着通过 EPT 页表来实现 GPA 到 HPA 的转换（实际上，CPU 会首先查看硬件 EPT TLB 或者缓存，如果没有对应的转换，才会进一步查看 EPT 页表），如果 HPA 为空呢，则 CPU 会抛出 EPT Violation 异常由 VMM 来处理。</p><p>如果 GPA 地址为空，即缺页，则 CPU 产生缺页异常，注意，这里，如果是软件实现的方式，则会产生 VM-exit，但是硬件实现方式，并不会发生 VM-exit，而是按照一般的缺页中断处理，这种情况下，也就是交给 Guest 内核的中断处理程序处理。</p><p>在中断处理程序中会产生 EXIT_REASON_EPT_VIOLATION，Guest  退出，VMM 截获到该异常后，分配物理地址并建立 GVA 到 HPA 的映射，并保存到 EPT 中，这样在下次访问的时候就可以完成从 GVA 到 HPA 的转换了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr><p>内存虚拟化经历从虚拟内存，到传统软件辅助虚拟化，影子页表，再到硬件辅助虚拟化，EPT 技术的进化，效率越来越高。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 03 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
            <tag> 内存 </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CPU 虚拟化</title>
      <link href="/2017/11/26/tech/cloud/virt/CPU_%E8%99%9A%E6%8B%9F%E5%8C%96/"/>
      <url>/2017/11/26/tech/cloud/virt/CPU_%E8%99%9A%E6%8B%9F%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>前面「虚拟化技术总览」中从虚拟平台 VMM 的角度，将虚拟化分为 Hypervisor 模型和宿主模型，如果根据虚拟的对象（资源类型）来划分，虚拟化又可以分为计算虚拟化、存储虚拟化和网络虚拟化，再细一些，又有中断虚拟化，内存虚拟化，字符/块设备虚拟化，网络功能虚拟化等。</p><p>我会将此作为一个系列来写，本文先看 CPU 虚拟化。在这之前，我们先来笼统看下虚拟化的本质是什么，它到底是如何做到将 Host 的硬件资源虚拟化给 Guest 用，我这里用两个词来定义，<strong>intercept</strong> 和 <strong>virtualize</strong>，中文翻译成截获和模拟比较恰当一点，这两个词基本上是虚拟化的终极定义了，带着这两个词去看每一种虚拟化类型，会发现很容易理解和记忆。</p><h3 id="CPU-软件虚拟化"><a href="#CPU-软件虚拟化" class="headerlink" title="CPU 软件虚拟化"></a>CPU 软件虚拟化</h3><hr><p>基于软件的 CPU 虚拟化，故名思议，就是通过软件的形式来模拟每一条指令。通过前面的文章我们知道常用的软件虚拟化技术有两种：优先级压缩和二进制代码翻译。这两种是通用技术，可以用在所有虚拟化类型中。我们就结合 intercept 和 virtualize 来看看 CPU 软件虚拟化是怎么做的。</p><p>首先，一些必须的硬件知识要知道，X86 体系架构为了让上层的软件（操作系统、应用程序）能够访问硬件，提供了四个 CPU 特权级别，Ring 0 是最高级别，Ring 1 次之，Ring 2 更次之，Ring 3 是最低级别。</p><p>一般，操作系统由于要直接访问硬件和内存，因此它的代码需要运行在最高级别 Ring 0 上，而应用程序的代码运行在最低级别 Ring 3 上，如果要访问硬件和内存，比如设备访问，写文件等，就要执行相关的系统调用，CPU 的运行级别发生从 Ring 3 到 Ring 0 的切换，当完成之后，再切换回去，我们熟悉的用户态和内核态切换的本质就来自这里。</p><p>虚拟化的实现也是基于这个思想，VMM 本质上是个 Host OS，运行在 Ring 0 上，Guest OS 运行在 Ring 1 上，再往上是相应层次的应用程序运行在 Ring 2 和 Ring 3 上。</p><p>当 Guest OS 或上层应用在执行相关的特权指令时，就会发生越权访问，触发异常，这个时候 VMM 就截获（intercept）这个指令，然后模拟（virtualize）这个指令，返回给 Guest OS，让其以为自己的特权指令可以正常工作，继续运行。整个过程其实就是优先级压缩和二进制代码翻译的体现。</p><center><img src="/images/virt/cpu_soft.jpg" alt=""></center><h3 id="CPU-硬件虚拟化"><a href="#CPU-硬件虚拟化" class="headerlink" title="CPU 硬件虚拟化"></a>CPU 硬件虚拟化</h3><hr><p>上面的这种截获再模拟的纯软件的虚拟化方式，势必是性能非常低的。那怎么样提高性能呢，有一种改进的方式是修改 Guest OS 中关于特权指令的相关操作，将其改为一种函数调用的方式，让 VMM 直接执行，而不是截获和模拟，这样就能在一定程度上提高性能。</p><p>但这种方式并不通用，要去改 Guest OS 的代码，只能看作是一种定制。为了能够通用，又能够提高性能，就只能从硬件上去做文章了。所以，后来，以 Intel 的 VT-x 和 AMD 的 AMD-V 为主的硬件辅助的 CPU 虚拟化就被提出来（Intel VT 包括 VT-x （支持 CPU 虚拟化）、EPT（支持内存虚拟化）和 VT-d（支持 I/O 虚拟化））。</p><center><img src="/images/virt/cpu_hard.jpg" alt=""></center><p>CPU 硬件辅助虚拟化在 Ring 模式的基础上引入了一种新的模式，叫 VMX 模式。它包括根操作模式（VMX Root Operation）和非根操作模式（VMX Non-Root Operation）。</p><p>这两种模式都有 Ring 0 - Ring 3 的特权级。所以，在描述某个应用程序时，除了描述其属于哪个特权级，还要指明其处于根模式还是非根模式。</p><p>引入这种模式的好处就在于，Guest OS 运行在 Ring 0 上，就意味着它的核心指令可以直接下达到硬件层去执行，而特权指令等敏感指令的执行则是由硬件辅助，直接切换到 VMM 执行，这是自动执行的，应用程序是感知不到的，性能自然就提高了。</p><p>这种切换 VT-x 定义了一套机制，称为 VM-entry 和 VM-exit。从非根模式切换到根模式，也就是从 Guest 切换到 Host VMM，称为 VM-exit，反之称为 VM-entry。</p><ul><li><p>VM-exit ： 如果 Guest OS 运行过程中遇到需要 VMM 处理的事件，比如中断或缺页异常，或者主动调用 VMCALL 指令调用 VMM 服务的时候（类似于系统调用），硬件自动挂起 Guest OS，切换到根模式，VMM 开始执行。</p></li><li><p>VM-entry： VMM 通过显示调用 VMLAUNCH 或 VMRESUME 指令切换到非根模式，硬件自动加载 Guest OS 的上下文，Guest OS 开始执行。</p></li></ul><center><img src="/images/virt/vm_exit.png" alt=""></center><h3 id="KVM-CPU-虚拟化"><a href="#KVM-CPU-虚拟化" class="headerlink" title="KVM CPU 虚拟化"></a>KVM CPU 虚拟化</h3><hr><p>KVM 是一种硬件辅助的虚拟化技术，支持 Intel VT-x 和 AMD-v 技术，怎么知道 CPU 是否支持 KVM 虚拟化呢？可以通过如下命令查看：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># grep -E &apos;(vmx|svm)&apos; /proc/cpuinfo</span><br></pre></td></tr></table></figure></p><p>如果输出是 vmx 或 svm，则表明当前 CPU 支持 KVM，Intel 是 vmx，AMD 是svm。</p><p>从本质上看，一个 KVM 虚拟机对应 Host 上的一个 qemu-kvm 进程，它和其他 Linux 进程一样被调度，而 qemu-kvm 进程中的一个线程就对应虚拟机的虚拟 CPU （vCPU），虚拟机中的任务线程就被 vCPU 所调度。</p><p>比如下面这个例子，Host 机有两个物理 CPU，上面起了两个虚拟机 VM1 和 VM2，VM1 有两个 vCPU，VM2 有 3 个 vCPU，VM1 和 VM2 分别有 2 个 和 3 个线程在 2 个物理 CPU 上调度。VM1 和 VM2 中又分别有 3 个任务线程在被 vCPU 调度。</p><p>所以，这里有两级的 CPU 调度，Guest OS 中的 vCPU 负责一级调度，Host VMM 负责另一级调度，即 vCPU 在物理 CPU 上的调度。</p><center><img src="/images/virt/cpu_over.png" alt=""></center><p>我们也可以看到，vCPU 的个数，可以超过物理 CPU 的个数，这个叫 CPU 「超配」，这正是 CPU 虚拟化的优势所在，这表明了虚拟机能够充分利用 Host 的 CPU 资源，进行相应的业务处理，运维人员也可以据此控制 CPU 资源使用，达到灵活调度。</p><p>OK，CPU 虚拟化就到这里，下篇文章将讲述内存虚拟化。觉得写得凑合可以给个赞，谢谢大家的支持。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 03 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
            <tag> CPU </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KVM 初探</title>
      <link href="/2017/11/20/tech/cloud/virt/KVM_%E5%88%9D%E6%8E%A2/"/>
      <url>/2017/11/20/tech/cloud/virt/KVM_%E5%88%9D%E6%8E%A2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>KVM 是业界最为流行的 Hypervisor，全称是 Kernel-based Virtual Machine。它是作为 Linux kernel 中的一个内核模块而存在，模块名为 kvm.ko，也可以看作是一个进程，被内核调度并管理，从 Linux 2.6.20 版本开始被完全正式加入到内核的主干开发和正式发布代码中。 KVM 主要用于管理 CPU 和内存的虚拟化，IO 设备的虚拟化则是由 Qemu 来完成。为什么会有这样的分工，请继续往下看。</p><h3 id="KVM-与-Qemu-的前世今生"><a href="#KVM-与-Qemu-的前世今生" class="headerlink" title="KVM 与 Qemu 的前世今生"></a>KVM 与 Qemu 的前世今生</h3><hr><p>Qemu 是一个纯软件实现的开源「模拟」软件，它能够模拟整套虚拟机的实现，包括 CPU、内存、各种 IO 设备、鼠标、键盘、USB 、网卡、声卡等等，基本上没有它不能模拟的。有人可能会比较疑惑它跟 KVM 之间到底有何关系，我们可以把它们看成是合作关系，好基友，谁都离不开彼此。</p><p>KVM 离不开 Qemu。KVM 实现初期，为了简化开发和代码重用，在 Qemu 的基础上进行了修改，主要是将比较耗性能的 CPU 虚拟化和内存虚拟化部分移到了内核中实现，保留 IO 虚拟化模块在用户空间实现。这样的做法主要是考虑到性能的原因，CPU 和 内存虚拟化是非常复杂的虚拟化模块，而且使用非常频繁，如果实现在用户空间的话，用户态和内核态的频繁切换势必会对性能造成很大的影响。那为什么要单独保留 IO 虚拟化在用户空间呢，这个也是权衡之下的结果，首先 IO 设备太多了，其次 IO 虚拟化相对其他两个模块使用不是很频繁，开销会小一些，所以，为了尽可能保持内核的纯净性，才有了这样的分配。</p><p>Qemu 离不开 KVM。上面也说了，Qemu 是一个纯软件的实现，运行在用户空间，性能非常低下，所以，从 Qemu 的角度，可以说是 Qemu 使用了 KVM 的虚拟化功能，为自身虚拟机提供加速。</p><p>早期两者还没有区分（没有同居），KVM 修改的模块叫 qemu-kvm，到 Qemu1.3 版本之后，两者就合二为一了（同居啦），如果我们在用 Qemu 创建虚拟机时，要加载 KVM 模块，需要为其指定参数 <code>--enable-kvm</code>。</p><center><img src="/imagesss/virt/kvm_qemu.png" alt=""><br>KVM 与 Qemu 的关系（图片来源于网络，侵权必删）</center><h3 id="KVM-架构"><a href="#KVM-架构" class="headerlink" title="KVM 架构"></a>KVM 架构</h3><hr><p>KVM 是基于硬件虚拟化（Intel VT 或 AMD-V）实现的一套虚拟化解决方案，通过以上一个与 Qemu 关系的分析，我们基本上知道它在虚拟化领域处在一个什么样的地位。它其实只负责 CPU 和内存的虚拟化，不负责任何设备的模拟，而是提供接口给用户空间的 Qemu 来模拟。这个接口是 /dev/kvm，<br>Qemu 通过 /dev/kvm 接口设置一个虚拟机的地址空间，然后向它提供模拟好的 I/O 设备，并将相关的设备回显操作映射到宿主机，完成整个 I/O 设备的虚拟化操作。</p><center><img src="/imagesss/virt/kvm_arch.png" alt=""><br>KVM 架构</center><p>/dev/kvm 接口是 Qemu 和 KVM 交互的“桥梁”，基本的原理是：/dev/kvm 本身是一个设备文件，这就意味着可以通过 ioctl 函数来对该文件进行控制和管理，从而可以完成用户空间与内核空间的数据交互。在 KVM 与 Qemu 的通信过程主要就是一系列针对该设备文件的 ioctl 调用。</p><p>我就拿创建虚拟机举个例子，虚拟机本质上是宿主机的一个进程，包括用户态数据结构和内核态数据结构，用户态部分由 Qemu 创建并初始化，内核态部分则由 KVM 来完成，完成后会返回一个文件句柄来代表所创建的虚拟机，针对该文件句柄的 ioctl 调用就可以对虚拟机进行相应的管理，比如建立虚拟机地址空间和宿主机地址空间的映射关系，创建多个线程（虚拟处理器，vCPU）来供虚拟机使用等，对于创建出的 vCPU，也会生成相应的文件句柄，同样，对 vCPU 的文件句柄的 ioctl 调用就可以对 vCPU 进行管理。</p><p>关于这块的具体细节，后面会有文章来专门讨论。</p><h3 id="VMM-管理工具-——-libvirt"><a href="#VMM-管理工具-——-libvirt" class="headerlink" title="VMM 管理工具 —— libvirt"></a>VMM 管理工具 —— libvirt</h3><hr><p>目前，虚拟化这个领域可以说是百花齐放，针对不同的场景提出了很多的虚拟化解决方案，KVM、Xen、VMware、VirtualBox、Hyper-V 等等，具体的这些方案有什么特点，可以看前文「虚拟化技术总览」。这么多方案势必有很多通用的模块，不同之处可能在于，与不同硬件厂商的适配上，为了支持更多厂商，以及应用更多的领域，有很多 IaaS 解决方案需要融合多种虚拟化技术。这个时候如果有一个平台类的管理工具就会非常方便，libvirt 就是这样一个工具。</p><center><img src="/imagesss/virt/kvm_qemu.png" alt=""><br>libvirt 架构（图片来源于网络，侵权必删）</center><p>libvirt 除了能够支持多种虚拟化方案之外，还支持 OpenVZ、LXC 等容器虚拟化系统。它提供一套完善的虚拟机管理工具，支持 GUI 和命令行的形式，如 virsh、virt-install、virt-manager。由于它的通用性和易管理，很多云计算框架平台都在底层使用 libvirt 的 API 来管理虚拟机，比如 OpenStack、OpenNebula、Eucalyptus 等。这个工具我们仅仅提一下，有兴趣的可以装个玩玩。</p><p>下面给出 KVM 和 Qemu 的 git 路径，有兴趣的可以把源码下下来研究下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kvm.git：</span><br><span class="line">git clone git://git.kernel.org/pub/scm/virt/kvm/kvm.git</span><br><span class="line">qemu.git（包括了 kvm）:</span><br><span class="line">git clone git://git.qemu-project.org/qemu.git</span><br></pre></td></tr></table></figure></p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/imagesss/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 03 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
            <tag> KVM </tag>
            
            <tag> Qemu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一文读懂 Qemu 模拟器</title>
      <link href="/2017/11/19/tech/cloud/virt/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82_Qemu_%E6%A8%A1%E6%8B%9F%E5%99%A8/"/>
      <url>/2017/11/19/tech/cloud/virt/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82_Qemu_%E6%A8%A1%E6%8B%9F%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="Qemu-架构"><a href="#Qemu-架构" class="headerlink" title="Qemu 架构"></a>Qemu 架构</h3><hr><p>Qemu  是纯软件实现的虚拟化模拟器，几乎可以模拟任何硬件设备，我们最熟悉的就是能够模拟一台能够独立运行操作系统的虚拟机，虚拟机认为自己和硬件打交道，但其实是和 Qemu 模拟出来的硬件打交道，Qemu 将这些指令转译给真正的硬件。</p><p>正因为 Qemu 是纯软件实现的，所有的指令都要经 Qemu 过一手，性能非常低，所以，在生产环境中，大多数的做法都是配合 KVM 来完成虚拟化工作，因为 KVM 是硬件辅助的虚拟化技术，主要负责 比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化，两者合作各自发挥自身的优势，相得益彰。</p><center><img src="/images/virt/qemu.png" alt=""><br>Qemu 总结结构</center><p>从本质上看，虚拟出的每个虚拟机对应 host 上的一个 Qemu 进程，而虚拟机的执行线程（如 CPU 线程、I/O 线程等）对应 Qemu 进程的一个线程。下面通过一个虚拟机启动过程看看 Qemu 是如何与 KVM 交互的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">// 第一步，获取到 KVM 句柄</span><br><span class="line">kvmfd = open(&quot;/dev/kvm&quot;, O_RDWR);</span><br><span class="line">// 第二步，创建虚拟机，获取到虚拟机句柄。</span><br><span class="line">vmfd = ioctl(kvmfd, KVM_CREATE_VM, 0);</span><br><span class="line">// 第三步，为虚拟机映射内存，还有其他的 PCI，信号处理的初始化。</span><br><span class="line">ioctl(kvmfd, KVM_SET_USER_MEMORY_REGION, &amp;mem);</span><br><span class="line">// 第四步，将虚拟机镜像映射到内存，相当于物理机的 boot 过程，把镜像映射到内存。</span><br><span class="line">// 第五步，创建 vCPU，并为 vCPU 分配内存空间。</span><br><span class="line">ioctl(kvmfd, KVM_CREATE_VCPU, vcpuid);</span><br><span class="line">vcpu-&gt;kvm_run_mmap_size = ioctl(kvm-&gt;dev_fd, KVM_GET_VCPU_MMAP_SIZE, 0);</span><br><span class="line">// 第五步，创建 vCPU 个数的线程并运行虚拟机。</span><br><span class="line">ioctl(kvm-&gt;vcpus-&gt;vcpu_fd, KVM_RUN, 0);</span><br><span class="line">// 第六步，线程进入循环，并捕获虚拟机退出原因，做相应的处理。</span><br><span class="line">for (;;) &#123;</span><br><span class="line">ioctl(KVM_RUN)</span><br><span class="line">switch (exit_reason) &#123;</span><br><span class="line">case KVM_EXIT_IO:  /* ... */</span><br><span class="line">case KVM_EXIT_HLT: /* ... */</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">// 这里的退出并不一定是虚拟机关机，</span><br><span class="line">// 虚拟机如果遇到 I/O 操作，访问硬件设备，缺页中断等都会退出执行，</span><br><span class="line">// 退出执行可以理解为将 CPU 执行上下文返回到 Qemu。</span><br></pre></td></tr></table></figure><h3 id="Qemu-源码结构"><a href="#Qemu-源码结构" class="headerlink" title="Qemu 源码结构"></a>Qemu 源码结构</h3><hr><p>Qemu 软件虚拟化实现的思路是采用二进制指令翻译技术，主要是提取 guest 代码，然后将其翻译成 TCG 中间代码，最后再将中间代码翻译成 host 指定架构的代码，如 x86 体系就翻译成其支持的代码形式，ARM 架构同理。</p><center><img src="/images/virt/qemu_src.png" alt=""></center><p>所以，从宏观上看，源码结构主要包含以下几个部分：</p><ul><li>/vl.c：最主要的模拟循环，虚拟机环境初始化，和 CPU 的执行。</li><li>/target-arch/translate.c：将 guest 代码翻译成不同架构的 TCG 操作码。</li><li>/tcg/tcg.c：主要的 TCG 代码。</li><li>/tcg/arch/tcg-target.c：将 TCG 代码转化生成主机代码。</li><li>/cpu-exec.c：主要寻找下一个二进制翻译代码块，如果没有找到就请求得到下一个代码块，并且操作生成的代码块。</li></ul><p>其中，涉及的主要几个函数如下：</p><center><img src="/images/virt/qemu_dir.png" alt=""></center><p>知道了这个总体的代码结构，再去具体了解每一个模块可能会相对容易一点。</p><h3 id="Qemu-的使用"><a href="#Qemu-的使用" class="headerlink" title="Qemu 的使用"></a>Qemu 的使用</h3><hr><h4 id="1-源码下载"><a href="#1-源码下载" class="headerlink" title="1. 源码下载"></a>1. 源码下载</h4><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">centos：sudo apt-get install qemu</span><br><span class="line">ubuntu：sudo yum install qemu -y</span><br><span class="line">安装包：</span><br><span class="line">$wget http://wiki.qemu-project.org/download/qemu-2.0.0.tar.bz2</span><br><span class="line">$tar xjvf qemu-2.0.0.tar.bz2</span><br><span class="line">Git：</span><br><span class="line">$git clone git://git.qemu-project.org/qemu.git</span><br></pre></td></tr></table></figure><h4 id="2-编译及安装"><a href="#2-编译及安装" class="headerlink" title="2. 编译及安装"></a>2. 编译及安装</h4><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$cd qemu-2.0.0 //如果使用的是git下载的源码，执行cd qemu</span><br><span class="line">$./configure --enable-kvm --enable-debug --enable-vnc --enable-werror  --target-list=&quot;x86_64-softmmu&quot;</span><br><span class="line">$make -j8</span><br><span class="line">$sudo make install</span><br></pre></td></tr></table></figure><p>configure 脚本用于生成 Makefile，其选项可以用 ./configure –help 查看。</p><p>这里使用到的选项含义如下：</p><ul><li>–enable-kvm：编译 KVM 模块，使 Qemu 可以利用 KVM 来访问硬件提供的虚拟化服务。</li><li>–enable-vnc：启用 VNC。</li><li>–enalbe-werror：编译时，将所有的警告当作错误处理。</li><li>–target-list：选择目标机器的架构。默认是将所有的架构都编译，但为了更快的完成编译，指定需要的架构即可。</li></ul><p>安装好之后，会生成如下应用程序：</p><center><img src="/images/virt/qemu_app.png" alt=""></center><ul><li>ivshmem-client/server：这是一个 guest 和 host 共享内存的应用程序，遵循 C/S 的架构。</li><li>qemu-ga：这是一个不利用网络实现 guest 和 host 之间交互的应用程序（使用 virtio-serial），运行在 guest 中。</li><li>qemu-io：这是一个执行 Qemu I/O 操作的命令行工具。</li><li>qemu-system-x86_64：Qemu 的核心应用程序，虚拟机就由它创建的。</li><li>qemu-img：创建虚拟机镜像文件的工具，下面有例子说明。</li><li>qemu-nbd：磁盘挂载工具。</li></ul><p>下面通过创建虚拟机操作来对这些工具有个初步的认识。</p><h4 id="3-创建虚拟机"><a href="#3-创建虚拟机" class="headerlink" title="3. 创建虚拟机"></a>3. 创建虚拟机</h4><hr><ul><li>使用qemu-img创建虚拟机镜像</li></ul><p>虚拟机镜像用来模拟虚拟机的硬盘，在启动虚拟机之前需要创建镜像文件。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-img create -f qcow2 test-vm-1.qcow2 10G</span><br></pre></td></tr></table></figure></p><p>-f 选项用于指定镜像的格式，qcow2 格式是 Qemu 最常用的镜像格式，采用来写时复制技术来优化性能。test-vm-1.qcow2 是镜像文件的名字，10G是镜像文件大小。镜像文件创建完成后，可使用 qemu-system-x86 来启动x86 架构的虚拟机：</p><ul><li>使用 qemu-system-x86 来启动 x86 架构的虚拟机<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 test-vm-1.qcow2</span><br></pre></td></tr></table></figure></li></ul><p>因为 test-vm-1.qcow2 中并未给虚拟机安装操作系统，所以会提示 “No bootable device”，无可启动设备。</p><ul><li>启动 VM 安装操作系统镜像<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -m 2048 -enable-kvm test-vm-1.qcow2 -cdrom ./Centos-Desktop-x86_64-20-1.iso</span><br></pre></td></tr></table></figure></li></ul><p>-m 指定虚拟机内存大小，默认单位是 MB， -enable-kvm 使用 KVM 进行加速，-cdrom 添加 fedora 的安装镜像。可在弹出的窗口中操作虚拟机，安装操作系统，安装完成后重起虚拟机便会从硬盘 ( test-vm-1.qcow2 ) 启动。之后再启动虚拟机只需要执行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -m 2048 -enable-kvm test-vm-1.qcow2</span><br></pre></td></tr></table></figure></p><p>qemu-img 支持非常多种的文件格式，可以通过 qemu-img -h 查看<br>其中 raw 和 qcow2 是比较常用的两种，raw 是 qemu-img 命令默认的，qcow2 是 qemu 目前推荐的镜像格式，是功能最多的格式。这些知识后面会有文章来专门讲述。</p><p>这篇文章写得有点长，可能是 Qemu 唯一一篇文章，这并不是说 Qemu 不重要，而是我们平时在使用过程中主要把它当工具用，遇到不懂的查就行了，当然，如果你觉得看代码爽一点，非常鼓励，如果看了有什么心得，我们可以一起交流交流。好了，老铁们，看在我深夜一点还在写干货给你们，就给我点个赞吧。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 03 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
            <tag> Qemu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>虚拟化技术总览</title>
      <link href="/2017/11/18/tech/cloud/virt/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF%E6%80%BB%E8%A7%88/"/>
      <url>/2017/11/18/tech/cloud/virt/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF%E6%80%BB%E8%A7%88/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>说起虚拟化，相信大家应该都不陌生，像虚拟内存、Java 虚拟机、Android 模拟器这些都是虚拟化技术的体现，为什么这样说，这个就要回到虚拟化技术的本质上——<strong>虚拟化就是由位于下层的软件模块，根据上层的软件模块的期待，抽象（虚拟）出一个虚拟的软件或硬件模块，使上一层软件直接运行在这个与自己期待完全一致的虚拟环境上</strong>。从这个意义上来看，虚拟化既可以是软件层的抽象，又可以是硬件层的抽象。</p><center><img src="/images/virt/virt_abs.png" alt=""><br>虚拟化技术本质上是软/硬件层的抽象</center><p>所以说，像虚拟内存、Java 虚拟机、Android 模拟器这些都属于是软件虚拟化技术，而硬件虚拟化技术更多的应用就是在云计算领域。从提出至今，虚拟化技术已经出现了多种实现方式，这些不同的方式其实就是软件和硬件的不同组合。本文主要就是对这些实现方式进行一个总览，形成一个总体认识，方便后面的学习。</p><h3 id="VMM"><a href="#VMM" class="headerlink" title="VMM"></a>VMM</h3><hr><p>VMM 全称是 Virtual Machine Monitor，虚拟机监控系统，也叫 Hypervisor，是虚拟化层的具体实现。主要是以软件的方式，实现一套和物理主机环境完全一样的虚拟环境，物理主机有的所有资源，包括 CPU、内存、网络 IO、设备 IO等等，它都有。这样的方式相当于 VMM 对物理主机的资源进行划分和隔离，使其可以充分利用资源供上层使用。虚拟出的资源以虚拟机的形式提供服务，一个虚拟机本质上和一台物理机没有什么区别，可以跑各种操作系统，在之上再跑各种应用。这种方式无疑是计算机历史上非常里程碑的一步，你想想，以前可能要买多台服务器才能解决的事，现在只用一台就解决了。</p><p>虚拟机通常叫做客户机（guest），物理机叫宿主机（host），VMM 处在中间层，既要负责对虚拟资源的管理，包括虚拟环境的调度，虚拟机之间的通信以及虚拟机的管理等，又要负责物理资源的管理，包括处理器、中断、内存、设备等的管理，此外，还要提供一些附加功能，包括定时器、安全机制、电源管理等。</p><center><img src="/images/virt/vmm.png" alt=""><br>VMM</center><h3 id="VMM-分类"><a href="#VMM-分类" class="headerlink" title="VMM 分类"></a>VMM 分类</h3><hr><p>VMM 根据平台类型和实现结构有两种不同的分类，按平台类型可以分为完全虚拟化和类虚拟化，完全虚拟化就是 VMM 完全模拟出一个跟物理主机完全一样的环境。但是这个是非常困难的，首先，这需要硬件的支持，而硬件在初期设计的时候，没有那么远的前瞻性，可以预想到为虚拟化提供支持，前次，指令的复杂性，即使通过模拟的方式也很难做到全部指令都模拟。所以，就需要借助其他的一些技术来辅助虚拟化。</p><center><img src="/images/virt/vmm_type.png" alt=""><br>VMM 分类</center><p>软件辅助虚拟化是通过优先级压缩（Ring Compression）和二进制代码翻译（Binary Translation）这两个技术来完成的。简单讲，RC 基于 CPU 特权级的原理，也就是 guest、VMM 和 host 分别处于不同的特权级上（这个后面讲 CPU 虚拟化的时候会详述），guest 要访问 host 就属于越级访问，会抛异常，这时 VMM 会截获这个异常，并模拟出其可能的行为，从而进行相应处理。但这个问题很明显，就是由于硬件设计的缺陷，有些指令并不能截获，从而导致“漏洞”。</p><p>BT 可以弥补这个缺陷，它通过去扫描 guest 的二进制的代码，将难以虚拟化的指令转为支持虚拟化的指令，从而可以配合 VMM 完成虚拟化功能。这两种方式都是通过「打补丁」的方式来辅助虚拟化，很难再架构上保证完整性。</p><p>所以，后期的硬件厂商就在硬件上对虚拟化提供了支持，有了硬件辅助的虚拟化。通过对硬件本身加入更多的虚拟化功能，就可以截获更多的敏感指令，填补上漏洞。在这一块，Intel 的 VT-x/d 技术和 AMD 的 AMD-V 技术是其中的代表。</p><p>而类虚拟化则是另外一种通过软件来避免漏洞的方式，就是通过修改 guest 操作系统内核代码（API 级）来避免漏洞，这种方式好处就是可以自定义内核的执行行为，某种程度上对性能进行优化。</p><p>上面这种分类仅供了解即可，重点掌握下面这种分类，就是根据 VMM 的实现结构分类，主要分类 Hypervisor 模型（1 型）和宿主模型（2 型）。</p><p>Hypervisor 模型中 VMM 既是操作系统，也是虚拟化软件，也就是集成了虚拟化功能的操作系统，对上为 guest 提供虚拟化功能，对下管理着所有物理资源，它的优点就是效率高，虚拟机的安全性只依赖于 VMM，缺点就是管理所有的物理资源，意味着 VMM 要承担很多的开发工作，特别是驱动层面的开发，我们知道硬件的 I/O 设备是很多的，这些设备都要有对应的驱动来设配才能为虚拟机提供功能。</p><center><img src="/images/virt/hyper.png" alt=""><br>Hypervisor 模型或 1 型模型</center><p>宿主模型剥离了管理功能和虚拟化功能，虚拟化功能只是作为内核的一个模块来加载，比如 KVM 技术就是其中的佼佼者，KVM 技术可以说是云计算最核心的技术了，后面会经常用到。一般 KVM 只负责 CPU 和内存的虚拟化，I/O 的虚拟化则由另外一个技术来完成，即 Qemu。这些技术都是后面的重点，在这里只是提一下。</p><center><img src="/images/virt/hyper1.png" alt=""><br>宿主模型或 2 型模型</center><h3 id="典型的虚拟化产品"><a href="#典型的虚拟化产品" class="headerlink" title="典型的虚拟化产品"></a>典型的虚拟化产品</h3><hr><ul><li><strong>VMware</strong></li></ul><p>VMware 可以说是虚拟化的鼻祖，现在很多公司都是在模仿 VMware 的产品，相应用过 VMware 虚拟机的朋友应该不陌生了，VMware 提供了很多的虚拟化产品，从服务器到桌面都有很多应用。主要有面向企业级应用的 ESX Server，面向服务端的入门级产品 VMware Server，面向桌面的主打产品 VMware Workstation（这个相信大家经常用），面向苹果系统的桌面产品 VMware Fusion，还有提供整套虚拟应用产品的 VMware vSphere，细分的话还有 VMware vStorage（虚拟存储），VMware vNet（虚拟网络）等。</p><ul><li><strong>Xen</strong></li></ul><p>Xen 是一款开源虚拟机软件，Xen 结合了 Hypervisor 模型和宿主模型，属于一种混合的虚拟化模型，基于 Xen 的虚拟化产品也有很多，比如 Ctrix、VirtualIron、RedHat 和 Novell 等都有相应的产品。这个一般是研究机构用得多一些，生产环境中大部分用的是 KVM。</p><ul><li><strong>KVM</strong></li></ul><p>KVM 也是一款开源软件，于 2007 年 2 月被集成到了 Linux 2.6.20 内核中，成为了内核的一部分。KVM 采用的是基于 Intel VT 的硬件辅助虚拟化技术，以及结合 Qemu 来提供设备虚拟化，从实现上看，属于宿主模型。使用 KVM 的厂商很多啊，像我们比较熟悉 VMware Workstation 和 VirtualBox 都在使用，在此就不一一列举了。</p><p>OK，有了这些基本认识，对于学后面的高深内容可能会好理解一些，大家如果觉得写得不错，可以给我个赞，你的鼓励是我不断输出好内容的动力。现在我开始写公众号，才发现这种看上去很虚的东西其实蛮鼓励人的，我现在看到那些坚持原创的作者都会感同身受，都会忍不住想给他们赞。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 03 虚拟化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算技能图谱</title>
      <link href="/2017/11/17/tech/cloud/%E4%BA%91%E8%AE%A1%E7%AE%97%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1/"/>
      <url>/2017/11/17/tech/cloud/%E4%BA%91%E8%AE%A1%E7%AE%97%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><p>云计算领域是一个很庞大的技术领域，技术分支很多，从底层的虚拟化技术，到各种框架，再到上层各种应用服务，都涉及非常多的技能点，<a id="more"></a>按照10000小时天才理论，即使能成为某一板块的技术专家，个人觉得也很难吃透所有的东西。下面是 infoQ 整理的一份技能图谱，我重新画了图，并增删了一部分。</p><p><img src="/images/cloud/cloud_ability.jpg" alt=""></p><p>可以看到，里面涉及的知识点太多了，这个公众号我会重点关注基础设施的计算和网络部分，架构的 OpenStack 、Docker 部分，开发语言重点关注 C/C++、Python，其他可能也会涉及，不管怎么样，希望能坚持下去吧。</p><ul><li><p>基础设施<br>包括计算、存储、网络、安全四大板块所涉及到的基础知识，现在比较火的容器也在列，隶属于计算部分。</p></li><li><p>架构<br>涉及很多的框架，包括分布式消息、微服务、OpenStack 生态、Docker 生态等，这些都是一个云平台为了满足高可用、高可靠、和性能不可缺少的组件。</p></li><li><p>开发<br>涉及到流程和语言方面，其中 Python 和 Go 是云计算时代比较常用的语言工具。</p></li><li><p>平台<br>平台部分展示的更多的是管理和维护一个云平台的工具，其中最为重要的是数据管理部分，光这个部分都可以再生出一个庞大的技能图谱——大数据开发工程师必备技能。</p></li><li><p>应用<br>主要是基于云平台开发的前后端应用和行业相关的应用。</p></li><li><p>运维<br>运维也可以再生一个技能图谱，这部分仅当了解。</p></li></ul><p>如果有志于找这个领域的开发工作，注意是开发，可以参考下面这个建议路径：</p><p><img src="/images/cloud/cloud_path.jpg" alt=""></p><p>注意，上面是应用开发工程师，当然还有很多发展路径，比如测试、产品、运维等，就开发这个路径来说，还有系统工程师，比较注重底层，OpenStack 开发工程师，主要基于 OpenStack 开发，等等。</p><p>好了，有了大概的一个方向，接下来就开干吧。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 01 云计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 技能图谱 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识云计算</title>
      <link href="/2017/11/16/tech/cloud/%E5%88%9D%E8%AF%86%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
      <url>/2017/11/16/tech/cloud/%E5%88%9D%E8%AF%86%E4%BA%91%E8%AE%A1%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/cloud/2.png" alt=""></p><blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="云计算的发展历史"><a href="#云计算的发展历史" class="headerlink" title="云计算的发展历史"></a>云计算的发展历史</h3><hr><p>我们主要从用户对云计算的认知角度来谈云计算的发展史，至于它从提出到发扬光大的那些大事件，网上搜下就知道了，而且我觉得去谈那些发展事件意义也不大，<a id="more"></a>倒不如说说我们对云计算的一个认知变化过程，我觉得任何事物存在必有其可循的迹象，可能在很久以前这东西就存在了，只不过在当时的情境下不叫这个名字而已，随着技术的发展和环境的变化，自然就演变成今天的样子。</p><p>就云计算这个话题，我就拿我们个人来举个例子，假设一个开发者闭关数日，不分白天与黑夜，倾心 Coding，终于开发出一款自认为很牛逼的产品，准备上线了，但是苦于没有服务器（资源）来承载他的产品，于是他勒紧裤腰带买了一台。上线没几天，这款产品出人意料的火爆，完全超出这名开发者的预想，于是，为了支撑流量的上涨，他又买了更多的服务器，为了维护这些服务器集群，他又开始自建机房，自己部署业务系统并运维。看着眼前这小有成就的一切，这名开发者却漏出了难色，这些支离破碎的机房运维管理工作搞得人焦头烂额，都没有心思 Coding 了。这个时候他听说有一些 IDC （Internet Data Center）运营商可以帮助企业或个人托管服务器资源，于是便当机立断把自己的服务器集群转移到 IDC 托管。正当他准备撸起袖子 Coding 之时，想到要定期去采购设备，还要跟 IDC 那边交接托管的事情就头大。这时，他又听朋友说云计算可以解决他这个问题，于是他又跑到一家云计算厂商，租用了一些资源就把这一切都搞定了。忙活了一通，终于可以安心 Coding 了。^=^</p><p>上面这个小故事是我杜撰的，或许能在一定程度上帮助你了解云计算的一个发展历史，从自建机房，到 IDC 托管，再到云计算，整体经历过这么三个阶段。当然，从技术的角度来说，云计算的思想也是早已有之，从早期的网格计算，分布式计算，再到虚拟化技术，无不跟云计算息息相关。云计算的出现，离不开人们的需求日益变化和技术人对技术的倾心专研。</p><h3 id="云计算服务类型"><a href="#云计算服务类型" class="headerlink" title="云计算服务类型"></a>云计算服务类型</h3><hr><p>最终，云计算反过来为人们提供更好的服务。从人们的需求来看，云计算总体上提供三类服务：IaaS、PaaS 和 SaaS。</p><p>关于这三类服务，我在前文中也说了一些，在这里总结一下，IaaS 主要提供的是底层的资源服务，比如服务器、存储、网络，企业或个人租用了这些资源之后，可以根据自己的需求定制自身的业务系统，如采用什么部署环境，开发环境等，这种一般比较适合于中大规模企业。</p><p>PaaS 则是在此基础上事先构建好了所有和开发、测试、运维等相关的环境，个人或企业可以专注在自身的业务逻辑上，不必去关心底层的运行环境，因为它一般能给你提供一个高可用，高可靠，可扩展的环境，这种一般适合于个人或小规模的企业。</p><p>SaaS 就更直接一些，提供的是现成的软件或应用的服务，如 email  服务等，这种比较具有普适性，不管是个人，还是任何规模的企业，都有使用现成的各类软件的需求。</p><p><img src="/images/cloud/cloud_srv.png" alt="图1 云服务类型"></p><center>图1 云服务类型</center><h3 id="云计算的分层架构"><a href="#云计算的分层架构" class="headerlink" title="云计算的分层架构"></a>云计算的分层架构</h3><hr><p>上面科普完了，下面从技术的角度简要说说云计算的架构。任何技术，总少不了会采用分层的架构（貌似是这样的~），这也验证了某位科学先驱所说的，任何问题，都可以通过增加一个间接的中间层来解决。云计算的分层架构可以从技术和使用者，也就是租户的角度，分为两种不同的架构，如下图，左边是技术视角架构，右边租户视角架构。</p><p><img src="/images/cloud/cloud_level.jpg" alt="图2 云计算的分层架构"></p><center>图2 云计算的分层架构</center><p>从技术视角看，计算、存储、网络等底层基础设施构成硬件资源层，虚拟化层通过虚拟化技术，并根据上层应用需求分配、编排和管理着这些资源，为了让资源具备高可用、高可靠，以及可扩展等特性，增加相应的中间层来支持，最上层则提供 Web 等友好控制台给用户，以 RESTful API 的方式展示资源，提高用户体验。</p><p>而从租户视角来看，租户根据自身业务的需求，如高可用，和所属业务领域，如游戏，依据不同的业务逻辑，来申请使用资源。这种方式能够很好隔离资源的提供者和使用者，提高了灵活性，让资源能够得到最大化的利用。 </p><p>本文的目的是希望对云计算形成一个总体的认识，因为我觉得学习任何知识，按照总-分-总这条路线来进行的话，对后面深入学习会有很大的帮助，但是这得花一些时间去整理、总结，才能比较好的输出，但你们很幸运，可以直接看我总结好的，为了鼓励我更好的输出给你们，动动手指给我个赞吧，另外，由于能力有限，难免会有错误，还望留言给我指出。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 01 云计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>云计算术语扫盲</title>
      <link href="/2017/11/15/tech/cloud/%E4%BA%91%E8%AE%A1%E7%AE%97%E6%9C%AF%E8%AF%AD%E6%89%AB%E7%9B%B2/"/>
      <url>/2017/11/15/tech/cloud/%E4%BA%91%E8%AE%A1%E7%AE%97%E6%9C%AF%E8%AF%AD%E6%89%AB%E7%9B%B2/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/cloud/1.png" alt=""></p><blockquote><p>文章首发我的公众号「Linux云计算网络」，欢迎关注，第一时间掌握技术干货！</p></blockquote><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><hr><p>在云计算中，资源和服务本质上是一样的，服务可能更泛一些，资源一般特指 CPU（计算）、Mem（存储）和 IO （网络）三大资源，<a id="more"></a>云计算的做法就是将闲置的这些资源充分利用起来，租给用户使用。我觉得这也有点共享经济的意思，大家把自己身边闲置的资源拿出来，分享给别人使用，我为人人，人人为我。只不过云计算这种资源比较奢侈，一般人还提供不起，只有那些大企业，在满足自己业务需求之余，还空闲着很多资源，所以，与其浪费掉，还不如租给用户去使用，既避免了浪费，还能赚钱。</p><h3 id="云部署类型"><a href="#云部署类型" class="headerlink" title="云部署类型"></a>云部署类型</h3><hr><p>资源多了，就会有一个问题，这些资源该放在哪，以及怎么放，这就涉及到云计算资源的部署类型，根据放的地方不同，可以分为公有云、私有云和混合云。公有云就是放在一个公共的地方，这个地方有个术语叫云服务提供商，这一般都是大公司，小公司还玩不转；私有云则是放在企业内部，一般供自身业务需求；而混合云则是两者融合起来，公有云服务体量大的业务，私有云负责数据的安全。而根据怎么放，近年来已经玩出了很多新花样，譬如把和政务相关的资源放一块，形成政务云，跟金融相关的放一块又形成金融云，类似的还有视频云、音乐云、直播云等。</p><h3 id="云服务类型"><a href="#云服务类型" class="headerlink" title="云服务类型"></a>云服务类型</h3><hr><p>资源整合起来，就需要对外提供服务，用户那么多，可能每个用户的对服务的需求都不一样，该怎么满足用户多样的需求，是一个非常关键的问题。比如用户想要一个开发环境，想立马就上手 Coding，你就不能纯粹给他一个裸机资源，又如用户想用 email 服务，你也必须给他装好相应的软件，用户只需动动手指就可以使用。所以，根据提供的服务类型的不同，可以将云服务分为 IaaS（基础设施即服务）、PaaS（平台即服务） 和 SaaS（软件即服务）。同样，如果再细分的话，类似的还有 DaaS（数据即服务）、SDNaaS（SDN 即服务）、CaaS（容器即服务）等。</p><h3 id="公有云"><a href="#公有云" class="headerlink" title="公有云"></a>公有云</h3><hr><p>上面已经说了一些，这里严格定义一下，公有云一般为云服务器提供商所拥有和运营，包括所有硬件、软件和其他支撑性基础设施资源，通过 Internet 向用户提供其资源，用户可以通过 Web 等方式来访问这些资源。业界比较有名的公有云厂商有：Amazon AWS、Microsoft Azure、Google Cloud、阿里云、腾讯云、百度云、UCloud 等。</p><h3 id="私有云"><a href="#私有云" class="headerlink" title="私有云"></a>私有云</h3><hr><p>私有云是专供一个企业或组织使用的云计算资源，一般部署在自家数据中心上，也可以付费给第三方的提供商托管。在私有云中，通过专用网络来维护其服务和基础结构，因而安全性会比较高。业界比较有名的私有云厂商有：VMWare、Nutanix.、深信服、华为云、青云等。</p><h3 id="混合云"><a href="#混合云" class="headerlink" title="混合云"></a>混合云</h3><hr><p>混合云组合了公有云和私有云，通过技术手段支持数据和应用程序在两者之间迁移，能够为企业提供更大的灵活性和更多的部署选项。</p><h3 id="IaaS"><a href="#IaaS" class="headerlink" title="IaaS"></a>IaaS</h3><hr><p>IaaS 提供的是比较底层的云计算服务，如服务器和虚拟机、存储空间、网络和操作系统，用户可以根据自己的需求租用特定的资源即可，云服务提供商管理和维护着这些资源，用户只需要购买、安装、配置和管理所需的软件，就可以构建自己的业务系统。</p><h3 id="PaaS"><a href="#PaaS" class="headerlink" title="PaaS"></a>PaaS</h3><hr><p>PaaS 则可以按需提供开发、测试、交付和管理应用程序所需的环境，包括中间件和数据库相关的基础结构。用户可以专注在自己的业务逻辑上，无需关心环境的问题，因为一切都就绪，你就开干就行了。</p><h3 id="SaaS"><a href="#SaaS" class="headerlink" title="SaaS"></a>SaaS</h3><hr><p>SaaS 则是提供实在的软件服务，一般用户通过订阅的方式来使用软件，随时随地都可以在云上使用现成的软件，无需下载安装，也无需关心软件升级和维护问题，因为这一切在云端都已经帮你做了。</p><h3 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h3><hr><p>虚拟机是资源的的具象，资源太抽象了，虽然说包括但不限于计算、存储和网络这三大资源，但是这些资源都是统一放在一个“池子”里，如何管理这些资源，并根据用户的需求合理地进行划分，虚拟机就是一种非常好的资源管理方式，它将物理主机上的资源进行细分，一个虚拟机使用一部分，彼此之间不会影响。在外部看来，它就像是一台真实的物理主机一样，拥有和主机该有的一切配置，包括 CPU、内存和 IO，只不过这些都是通过程序虚拟出来的。</p><h3 id="虚拟化"><a href="#虚拟化" class="headerlink" title="虚拟化"></a>虚拟化</h3><hr><p>虚拟化就是将资源进行细分（虚拟）的一门技术，它可以虚拟计算、虚拟存储、虚拟网络，以及虚拟网络功能。它的一个宗旨就是将闲置的资源划分出来，虚构一个和真实物理环境没有差别的虚拟环境，这样，用户在使用资源的时候，就像是在使用一台真实物理机一样。常见的虚拟化技术有 KVM、Xen、Qemu 等。</p><p>PS：文章未经我允许，不得转载，否则后果自负。</p><center>–END–</center><hr><blockquote><p>欢迎扫👇的二维码关注我的微信公众号，后台回复「m」，可以获取往期所有技术博文推送，更多资料回复下列关键字获取。</p></blockquote><p><img src="/images/weichat.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 01 云计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 云计算 </tag>
            
            <tag> 虚拟化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo 不同电脑/终端同步方法</title>
      <link href="/2017/01/08/tech/tools/Hexo-%E4%B8%8D%E5%90%8C%E7%94%B5%E8%84%91-%E7%BB%88%E7%AB%AF%E5%90%8C%E6%AD%A5%E6%96%B9%E6%B3%95/"/>
      <url>/2017/01/08/tech/tools/Hexo-%E4%B8%8D%E5%90%8C%E7%94%B5%E8%84%91-%E7%BB%88%E7%AB%AF%E5%90%8C%E6%AD%A5%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>问题描述：  </p><blockquote><p>在Linux系统上搭建了博客，现想在windows上写博客，如何在两个系统进行文章的同步？</p></blockquote><p>上面是在同一台电脑的不同终端上操作，同样这个问题也适用于不同电脑上，所以这个问题可以扩展成一个比较通用的问题：  </p><blockquote><p>不同电脑，或终端上如何同步Hexo？</p></blockquote><p>Google一下，网友已经给了很好的解决方案，大体上有以下两种方案：<strong>利用分支</strong>和<strong>新建仓库</strong>。<br><a id="more"></a><br>1、利用分支<br>顾名思义，就是新建一个分支来存放hexo网站的原始文件，master分支则用来存放生成的静态网页。具体操作参见以下的链接：<br><a href="https://www.zhihu.com/question/21193762" target="_blank" rel="noopener">使用hexo，如果换了电脑怎么更新博客？</a><br><a href="http://www.jianshu.com/p/6fb0b287f950" target="_blank" rel="noopener">多设备同步hexo搭建的Github博客</a><br>2、新建仓库<br>和建分支同样道理，通过建仓库来存放hexo网站的原始文件，这种方法操作起来比1简单，不易出错，建分支万一发布的时候走神，没有切换到正确的分支，就GG了。方法参见：<br><a href="http://wangmuduo.com/2016/04/02/hexo-change-os/#comments" target="_blank" rel="noopener">Hexo 换终端/换电脑小记</a>  </p>]]></content>
      
      
      <categories>
          
          <category> 05 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git学习笔记——来自廖雪峰的博客</title>
      <link href="/2016/12/29/tech/tools/Git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9D%A5%E8%87%AA%E5%BB%96%E9%9B%AA%E5%B3%B0%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
      <url>/2016/12/29/tech/tools/Git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9D%A5%E8%87%AA%E5%BB%96%E9%9B%AA%E5%B3%B0%E7%9A%84%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="Git-简介"><a href="#Git-简介" class="headerlink" title="Git 简介"></a>Git 简介</h2><p>Git是目前世界上最先进的开源的<strong>分布式</strong>版本控制系统。<br>和<strong>集中式</strong>的版本控制系统，如CVS、SVN等相比，Git有几大优点：<br>1、不像集中式版本控制系统一样，需要联网才能工作；<br>2、安全性较高，避免了单点失效的问题，不会因为某台电脑坏了，文件就丢失了；<br>3、具有强大的分支管理功能。</p><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><pre><code>Linux：sudo apt-get install gitcentos: sudo yum install git-corewindows: git bash</code></pre><p>安装完成后，需要配置机器的Git仓库，即：  </p><pre><code>git config --global user.name &quot;Your Name&quot;git config --global user.email &quot;email@example.com&quot;</code></pre><a id="more"></a><h2 id="Git-基础特性及常用命令"><a href="#Git-基础特性及常用命令" class="headerlink" title="Git 基础特性及常用命令"></a>Git 基础特性及常用命令</h2><h3 id="创建版本库（仓库，repository）"><a href="#创建版本库（仓库，repository）" class="headerlink" title="创建版本库（仓库，repository）"></a>创建版本库（仓库，repository）</h3><pre><code>mkdir learngitcd learngit</code></pre><h3 id="添加文件到Git仓库"><a href="#添加文件到Git仓库" class="headerlink" title="添加文件到Git仓库"></a>添加文件到Git仓库</h3><pre><code>git init: 将当期目录变成Git可以管理的仓库git add filegit commit -m &quot;add file&quot;</code></pre><p>补充几个概念：</p><ul><li>工作区：如learngit</li><li>版本库：.git (包括暂存区和分支)</li><li>暂存区：git add 之后存储的位置</li><li>分支：git commit 之后存储的位置</li></ul><p><img src="/image/git.jpg" alt="">     </p><h3 id="跟踪工作区的状态"><a href="#跟踪工作区的状态" class="headerlink" title="跟踪工作区的状态"></a>跟踪工作区的状态</h3><pre><code>git status: 可以随时掌握工作区的状态git diff file: 如果git status告知file被修改了，该命令告知修改了什么内容</code></pre><h3 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h3><pre><code>git log: 查看提交历史，显示commit id、author、date等git log --pretty=oneline: 清爽版</code></pre><p>回退方法两种：<br>1、HEAD<br>HEAD表示当前版本，HEAD^上一版本，HEAD^^上上一版本，HEAD~100上第100版本。  </p><pre><code>git reset --hard HEAD^</code></pre><p>2、commit id  </p><pre><code>git reset --hard 34343</code></pre><p>如要重返未来的某一个版本，则只能找到commit id，通过以下命令来确定：</p><pre><code>git reflog</code></pre><h3 id="工作区、暂存区的文件修改、撤销修改、删除相关"><a href="#工作区、暂存区的文件修改、撤销修改、删除相关" class="headerlink" title="工作区、暂存区的文件修改、撤销修改、删除相关"></a>工作区、暂存区的文件修改、撤销修改、删除相关</h3><pre><code>git diff HEAD -- readme.txt:查看工作区和版本库里面最新版本的区别git checkout -- file：丢弃工作区的修改git reset HEAD file: 丢弃暂存区的修改，回到工作区</code></pre><p>如果git commit提交到版本库，参考<em>版本回退</em></p><pre><code>git rm filegit commit -m &quot;remove file&quot;:将文件从版本库删除，保证本地工作区和版本库文件同步</code></pre><h3 id="远程仓库（Github）"><a href="#远程仓库（Github）" class="headerlink" title="远程仓库（Github）"></a>远程仓库（Github）</h3><p>本地Git仓库和远程Github仓库之间的传输是通过SSH加密的，所以需要设置SSH Key：  </p><pre><code>ssh-keygen -t rsa -C &quot;youremail@example.com&quot;less ~/.ssh/id_rsa.pub</code></pre><p>注：GitHub允许你添加多个Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的Key都添加到GitHub，就可以在每台电脑上往GitHub推送了。  </p><pre><code>git remote add origin git@github.com:xxx/learngit.git:将本地参考与远程仓库进行关联git push -u origin master:把本地仓库内容推送到远程，-u表示把本地master分支与远程master分支关联，后面如果本地做了修改，只需git push origin master推送修改即可</code></pre><p>克隆远程仓库：  </p><pre><code>git clone git@github.com:xxx/learngit.git:使用ssh协议克隆，也可以使用https，但ssh支持的原生git协议速度更快。</code></pre><h2 id="Git-高级特性：分支、标签"><a href="#Git-高级特性：分支、标签" class="headerlink" title="Git 高级特性：分支、标签"></a>Git 高级特性：分支、标签</h2><h3 id="创建与合并分支"><a href="#创建与合并分支" class="headerlink" title="创建与合并分支"></a>创建与合并分支</h3><pre><code>git checkout -b dev:创建dev分支，并切换到dev分支相当于：git branch devgit checkout devgit branch:查看当前分支git merge dev: 在master上合并dev分支git branch -d dev: 删除dev分支git log --graph: 可以查看合并分支图，常用：git log --graph --pretty=oneline --abbrev-commit</code></pre><h3 id="分支管理策略"><a href="#分支管理策略" class="headerlink" title="分支管理策略"></a>分支管理策略</h3><p>合并分支时一般会使用Fast Forward模式，即快进模式，将master分支直接指向dev分支，删除分支后，丢失分支信息，看不出曾经的合并历史信息，这不利于将来出错回头排查问题，所以，可以禁用Fast forward模式，通过以下方式：</p><pre><code>git merge --no-ff -m &quot;merge with no-ff&quot; dev:Git 在merge时生成一个新的commit id,从分支历史上看出分支信息</code></pre><h3 id="Bug-分支"><a href="#Bug-分支" class="headerlink" title="Bug 分支"></a>Bug 分支</h3><p>在dev分支上开发，突然有了bug，但是dev还没开发好，无法提交，这时，可以：</p><pre><code>git stash:把当前工作现场保存起来，等以后恢复现场后继续工作git stash list: 查看保存的工作现场git stash apply: 恢复工作区，但恢复后stash不删除，需要用：git stash drop:来删除git stash pop:恢复的同时把stash内容也删了</code></pre><h3 id="Feature-分支"><a href="#Feature-分支" class="headerlink" title="Feature 分支"></a>Feature 分支</h3><p>开发一个新feature，最好创建一个分支；<br>如果要丢弃一个没有被合并过的分支，可以通过：</p><pre><code>git branch -D &lt;name&gt;：强行删除</code></pre><h3 id="多人协作：推送分支"><a href="#多人协作：推送分支" class="headerlink" title="多人协作：推送分支"></a>多人协作：推送分支</h3><pre><code>git remote:查看远程库信息git remote -v：显示更详细的信息git push origin master: 推送分支到远程master分支git push origin dev:推送分支到远程dev分支</code></pre><p><strong>分支推送</strong>备注  </p><ul><li>master分支是主分支，因此要时刻与远程同步；</li><li>dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；</li><li>bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；</li><li>feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。</li></ul><h3 id="多人协作：抓取分支"><a href="#多人协作：抓取分支" class="headerlink" title="多人协作：抓取分支"></a>多人协作：抓取分支</h3><pre><code>git checkout -b dev origin/dev：在远程的dev分支上进行开发</code></pre><p>如果两个人对同一个dev分支，会产生冲突，此时先pull，再push。</p><pre><code>git pullgit branch --set-upstream dev origin/dev:指定本地dev分支与远程origin/dev分支的链接如果有冲突，则先处理冲突</code></pre><h3 id="标签管理"><a href="#标签管理" class="headerlink" title="标签管理"></a>标签管理</h3><pre><code>git tag &lt;name&gt;:用于新建一个标签，默认为HEAD，也可以指定一个commit id；git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;:可以指定标签信息；git tag -s &lt;tagname&gt; -m &quot;blablabla...&quot;:可以用PGP签名标签；git tag:可以查看所有标签。git push origin &lt;tagname&gt;:可以推送一个本地标签；git push origin --tags:可以推送全部未推送过的本地标签；git tag -d &lt;tagname&gt;:可以删除一个本地标签；git push origin :refs/tags/&lt;tagname&gt;:可以删除一个远程标签。</code></pre><p><em>更多高级特性，请前往：<a href="http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">廖雪峰git教程</a></em></p>]]></content>
      
      
      <categories>
          
          <category> 05 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LaTeX制作中英简历模板</title>
      <link href="/2015/06/07/tech/tools/LaTeX%E5%88%B6%E4%BD%9C%E4%B8%AD%E8%8B%B1%E7%AE%80%E5%8E%86%E6%A8%A1%E6%9D%BF/"/>
      <url>/2015/06/07/tech/tools/LaTeX%E5%88%B6%E4%BD%9C%E4%B8%AD%E8%8B%B1%E7%AE%80%E5%8E%86%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<p>LaTex用于写文章的快捷与方便之处已是众人周知，然而LaTeX能做的事情还有很多，制作精美的PPT(特别是学术性的PPT)，制作精美的简历，以及做出精美的图片(媲美Python的matplotlib和MATLAB)，等等。LaTeX是一种脚本语言，类似于HTML，非常容易上手。利用LaTeX和JBref的组合来写文章，能够方便科研工作者快速实现自己的需求，而不必要为一些繁琐的排版问题而伤透脑筋。为了方便研究人员，很多会议、期刊、杂志都会提供自己相应的模板，投稿者只需下载模板，往里面增加自己的内容即可。</p><p>本文提供一些利用LaTeX来制作个人简历的模板，以便日后需要。几个比较好的模板：</p><p><a href="http://yixf.name/2012/02/19/%E4%BD%BF%E7%94%A8latex%E5%88%B6%E4%BD%9C%E4%B8%AD%E6%96%87%E7%AE%80%E5%8E%86%E7%9A%84%E6%A8%A1%E6%9D%BF/" target="_blank" rel="noopener">中文模板：</a></p><p><a href="http://rpi.edu/dept/arc/training/latex/resumes/" target="_blank" rel="noopener">英文模板：</a></p>]]></content>
      
      
      <categories>
          
          <category> 05 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTeX </tag>
            
            <tag> 简历 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LaTeX相关的知识盲点</title>
      <link href="/2015/04/14/tech/tools/LaTeX%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%9B%B2%E7%82%B9/"/>
      <url>/2015/04/14/tech/tools/LaTeX%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E7%9B%B2%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>LaTeX是一款高效的论文写作排版工具，除了写论文，它还有其他的一些用途，比如做ppt，写优美的简历，作优美的图画等等。相对于word+endnote的论文写作组合，LaTeX也和JabRef组成一个最佳的组合，但其好用程度比之前一个提升至少10个档次。本文总结几个我在使用的过程中遇到的盲点。</p><h3 id="LaTeX对中文的支持"><a href="#LaTeX对中文的支持" class="headerlink" title="LaTeX对中文的支持"></a>LaTeX对中文的支持</h3><p>要使用中文模板，则LaTeX需要安装CJK库以支持汉字，该库里面已经包含了字体格式，大小等，怎么使用，请见下面一个简单的例子：<br><a id="more"></a>  </p><h4 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h4><p>%要运行该模板，LaTex需要安装CJK库以支持汉字.</p><p> %字体大小为12像素，文档类型为article</p><p> %如果你要写论文，就用report代替article</p><p> %所有LaTex文档开头必须使用这句话</p><p>\documentclass[12pt]{article}  </p><p>%使用支持汉字的CJK包</p><p>\usepackage{CJK}  </p><p>%开始CJK环境,只有在这句话之后,你才能使用汉字</p><p>%另外,如果在Linux下,请将文件的编码格式设置成GBK</p><p> %否则会显示乱码</p><p>\begin{CJK*}{GBK}{song}  </p><p>%这是文章的标题</p><p>\title{LaTex 常用模板}  </p><p>%这是文章的作者</p><p>\author{Kevin}  </p><p>%这是文章的时间</p><p>%如果没有这行将显示当前时间</p><p>%如果不想显示时间则使用 \date{}</p><p> \date{2008/10/12}  </p><p>%以上部分叫做&quot;导言区&quot;,下面才开始写正文</p><p>\begin{document}  </p><p>%先插入标题</p><p>\maketitle</p><p> %再插入目录</p><p>\tableofcontents</p><p> \section{LaTex 简介}</p><p>LaTex是一个宏包,目的是使作者能够利用一个</p><p> 预先定义好的专业页面设置,</p><p>从而得以高质量的排版和打印他们的作品.  </p><p>%第二段使用黑体,上面的一个空行表示另起一段</p><p>\CJKfamily{hei}LaTex 将空格和制表符视为相同的距离.</p><p>多个连续的空白字符 等同为一个空白字符</p><p>\section{LaTex源文件}</p><p> %在第二段我们使用隶书</p><p>\CJKfamily{li}LaTex 源文件格式为普通的ASCII文件,</p><p>你可以使用任何文本编辑器来创建.  </p><p>LaTex源文件不仅包括你要排版的文本, 还包括LaTex</p><p>所能识别的,如何排版这些文本的命令.</p><p> \section{结论}</p><p> %在结论部分我们使用仿宋体</p><p>\CJKfamily{fs}LaTeX, 我看行!  </p><p>\end{CJK*}</p><p> \end{document}  </p><h4 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h4><p>CTeX里提供了GBK编码的六种中文字体（宋体、仿宋、楷体、黑体、隶书和幼圆），如果你安装了CTeX，就可以类似下面来使用这几种字体：</p><p>\documentclass{article}</p><p> \usepackage{CJK}</p><p> \begin{document}</p><p> \begin{CJK}{GBK}{song}</p><p>这是CTeX里的宋体！</p><p>\end{CJK}  </p><p>\begin{CJK}{GBK}{fs}</p><p>这是CTeX里的仿宋体！</p><p>\end{CJK}    </p><p>\begin{CJK}{GBK}{kai}</p><p>这是CTeX里的楷体！</p><p>\end{CJK}  </p><p>\begin{CJK}{GBK}{hei}</p><p>这是CTeX里的黑体！</p><p>\end{CJK}  </p><p>\begin{CJK}{GBK}{li}</p><p>这是CTeX里的隶书！</p><p>\end{CJK}  </p><p>\begin{CJK}{GBK}{you}</p><p>这是CTeX里的幼圆体！</p><p>\end{CJK}</p><p>\end{document}    </p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>参考文献一般显示在文章末尾，可以用数字标号或者文章年份，有的甚至用作者信息来显示，这些显示方式分别对应着不同的模板，也就是不同库，我们常用的一种方式是显示标号，几种设置方式见下文：</p><p><a href="http://zzg34b.w3.c361.com/package/reference.htm" target="_blank" rel="noopener">LaTex常用宏包：</a></p>]]></content>
      
      
      <categories>
          
          <category> 05 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTeX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>十个有好python惯用法</title>
      <link href="/2015/01/09/tech/python/%E5%8D%81%E4%B8%AA%E6%9C%89%E5%A5%BDpython%E6%83%AF%E7%94%A8%E6%B3%95/"/>
      <url>/2015/01/09/tech/python/%E5%8D%81%E4%B8%AA%E6%9C%89%E5%A5%BDpython%E6%83%AF%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Make-a-script-both-importable-and-executable-使你的脚本可输入且可执行"><a href="#1-Make-a-script-both-importable-and-executable-使你的脚本可输入且可执行" class="headerlink" title="1. Make a script both importable and executable(使你的脚本可输入且可执行)"></a>1. Make a script both importable and executable(使你的脚本可输入且可执行)</h2><pre><code>if __name__ == &apos;__main__&apos;:</code></pre><h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><pre><code>def main():    print(&apos;Doing stuff in module&apos;, __name__)if __name__ == &apos;__main__&apos;:    print(&apos;Executed from the command line&apos;)    main()$ python mymodule.pyExecuted from the command lineDoing stuff in module __main__&gt;&gt;&gt; import mymodule&gt;&gt;&gt; mymodule.main()Doing stuff in module mymodule</code></pre><a id="more"></a>  <h2 id="2-Test-for-“truthy”-and-“falsy”-values-测试采用真假判断"><a href="#2-Test-for-“truthy”-and-“falsy”-values-测试采用真假判断" class="headerlink" title="2. Test for “truthy” and “falsy” values(测试采用真假判断)"></a>2. Test for “truthy” and “falsy” values(测试采用真假判断)</h2><pre><code>if x:if not x:</code></pre><h4 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h4><pre><code># GOODname = &apos;Safe&apos;pets = [&apos;Dog&apos;, &apos;Cat&apos;, &apos;Hamster&apos;]owners = {&apos;Safe&apos;: &apos;Cat&apos;, &apos;George&apos;: &apos;Dog&apos;}if name and pets and owners:    print(&apos;We have pets!&apos;)# NOT SO GOODif name != &apos;&apos; and len(pets) &gt; 0 and owners != {}:    print(&apos;We have pets!&apos;)  </code></pre><h2 id="3-Use-in-where-possible-如果有可能尽可能使用in"><a href="#3-Use-in-where-possible-如果有可能尽可能使用in" class="headerlink" title="3. Use in where possible(如果有可能尽可能使用in)"></a>3. Use in where possible(如果有可能尽可能使用in)</h2><pre><code>Contains:if x in items:Iteration:for x in items:</code></pre><h4 id="Example-contains"><a href="#Example-contains" class="headerlink" title="Example (contains)"></a>Example (contains)</h4><pre><code># GOODname = &apos;Safe Hammad&apos;if &apos;H&apos; in name:print(&apos;This name has an H in it!&apos;)# NOT SO GOODname = &apos;Safe Hammad&apos;if name.find(&apos;H&apos;) != -1:    print(&apos;This name has an H in it!&apos;)</code></pre><h4 id="Example-iteration"><a href="#Example-iteration" class="headerlink" title="Example (iteration)"></a>Example (iteration)</h4><pre><code># GOODpets = [&apos;Dog&apos;, &apos;Cat&apos;, &apos;Hamster&apos;]for pet in pets:    print(&apos;A&apos;, pet, &apos;can be very cute!&apos;)# NOT SO GOODpets = [&apos;Dog&apos;, &apos;Cat&apos;, &apos;Hamster&apos;]i = 0while i &lt; len(pets):    print(&apos;A&apos;, pets[i], &apos;can be very cute!&apos;)    i += 1</code></pre><h2 id="4-Swap-values-without-temp-variable-交换两个数不适用temp中间值"><a href="#4-Swap-values-without-temp-variable-交换两个数不适用temp中间值" class="headerlink" title="4. Swap values without temp variable(交换两个数不适用temp中间值)"></a>4. Swap values without temp variable(交换两个数不适用temp中间值)</h2><pre><code>a, b = b, a</code></pre><h4 id="Example-2"><a href="#Example-2" class="headerlink" title="Example"></a>Example</h4><pre><code># GOODa, b = 5, 6print(a, b) # 5, 6a, b = b, aprint(a, b) # 6, 5# NOT SO GOODa, b = 5, 6print(a, b) # 5, 6temp = aa = bb = tempprint(a, b) # 6, 5</code></pre><h2 id="5-Build-strings-using-sequence-使用序列的方式来得到字符串"><a href="#5-Build-strings-using-sequence-使用序列的方式来得到字符串" class="headerlink" title="5. Build strings using sequence(使用序列的方式来得到字符串)"></a>5. Build strings using sequence(使用序列的方式来得到字符串)</h2><pre><code>&apos;&apos;.join(some_strings)</code></pre><h4 id="Example-3"><a href="#Example-3" class="headerlink" title="Example"></a>Example</h4><pre><code># GOODchars = [&apos;S&apos;, &apos;a&apos;, &apos;f&apos;, &apos;e&apos;]name = &apos;&apos;.join(chars)print(name) # Safe# NOT SO GOODchars = [&apos;S&apos;, &apos;a&apos;, &apos;f&apos;, &apos;e&apos;]name = &apos;&apos;for char in chars:    name += char    print(name) # Safe</code></pre><h2 id="6-EAFP-is-preferable-to-LBYL-大概意思是说使用专业的容错机制"><a href="#6-EAFP-is-preferable-to-LBYL-大概意思是说使用专业的容错机制" class="headerlink" title="6. EAFP is preferable to LBYL(大概意思是说使用专业的容错机制)"></a>6. EAFP is preferable to LBYL(大概意思是说使用专业的容错机制)</h2><pre><code>“It&apos;s Easier to Ask for Forgiveness than Permission.”“Look Before You Leap”try: v. if ...:except:</code></pre><h4 id="Example-4"><a href="#Example-4" class="headerlink" title="Example"></a>Example</h4><pre><code># GOODd = {&apos;x&apos;: &apos;5&apos;}try:    value = int(d[&apos;x&apos;])except (KeyError, TypeError, ValueError):    value = None# NOT SO GOODd = {&apos;x&apos;: &apos;5&apos;}if &apos;x&apos; in d and \    isinstance(d[&apos;x&apos;], str) and \    d[&apos;x&apos;].isdigit():    value = int(d[&apos;x&apos;])else:    value = None</code></pre><h2 id="7-Enumerate-常用该函数，得到-index-value"><a href="#7-Enumerate-常用该函数，得到-index-value" class="headerlink" title="7. Enumerate(常用该函数，得到(index, value))"></a>7. Enumerate(常用该函数，得到(index, value))</h2><pre><code>for i, item in enumerate(items):</code></pre><h4 id="Example-5"><a href="#Example-5" class="headerlink" title="Example"></a>Example</h4><pre><code># GOODnames = [&apos;Safe&apos;, &apos;George&apos;, &apos;Mildred&apos;]for i, name in enumerate(names):    print(i, name) # 0 Safe, 1 George etc.# NOT SO GOODnames = [&apos;Safe&apos;, &apos;George&apos;, &apos;Mildred&apos;]count = 0for name in names:    print(i, name) # 0 Safe, 1 George etc.    count += 1</code></pre><h2 id="8-Build-lists-using-list-comprehensions-使用列表合成新的列表"><a href="#8-Build-lists-using-list-comprehensions-使用列表合成新的列表" class="headerlink" title="8. Build lists using list comprehensions(使用列表合成新的列表)"></a>8. Build lists using list comprehensions(使用列表合成新的列表)</h2><pre><code>[i * 3 for i in data if i &gt; 10]</code></pre><h4 id="Example-6"><a href="#Example-6" class="headerlink" title="Example"></a>Example</h4><pre><code># GOODdata = [7, 20, 3, 15, 11]result = [i * 3 for i in data if i &gt; 10]print(result) # [60, 45, 33]# NOT SO GOOD (MOST OF THE TIME)data = [7, 20, 3, 15, 11]result = []for i in data:    if i &gt; 10:        result.append(i * 3)        print(result) # [60, 45, 33]</code></pre><h2 id="9-Create-dict-from-keys-and-values-using-zip-尽可能使用zip-函数创建字典"><a href="#9-Create-dict-from-keys-and-values-using-zip-尽可能使用zip-函数创建字典" class="headerlink" title="9. Create dict from keys and values using zip(尽可能使用zip()函数创建字典)"></a>9. Create dict from keys and values using zip(尽可能使用zip()函数创建字典)</h2><pre><code>d = dict(zip(keys, values))</code></pre><h4 id="Example-7"><a href="#Example-7" class="headerlink" title="Example"></a>Example</h4><pre><code># GOODkeys = [&apos;Safe&apos;, &apos;Bob&apos;, &apos;Thomas&apos;]values = [&apos;Hammad&apos;, &apos;Builder&apos;, &apos;Engine&apos;]d = dict(zip(keys, values))print(d) # {&apos;Bob&apos;: &apos;Builder&apos;,            &apos;Safe&apos;: &apos;Hammad&apos;,            &apos;Thomas&apos;: &apos;Engine&apos;}# NOT SO GOODkeys = [&apos;Safe&apos;, &apos;Bob&apos;, &apos;Thomas&apos;]values = [&apos;Hammad&apos;, &apos;Builder&apos;, &apos;Engine&apos;]d = {}for i, key in enumerate(keys):    d[keys] = values[i]    print(d) # {&apos;Bob&apos;: &apos;Builder&apos;,                &apos;Safe&apos;: &apos;Hammad&apos;,                &apos;Thomas&apos;: &apos;Engine&apos;}</code></pre><h2 id="10-And-the-rest-…"><a href="#10-And-the-rest-…" class="headerlink" title="10. And the rest … !"></a>10. And the rest … !</h2><pre><code>● while True:break # This will spark discussion!!!● Generators and generator expressions.● Avoid from module import *Prefer: import numpy as np; import pandas as pd● Use _ for “throwaway” variables e.g.:for k, _ in [(&apos;a&apos;, 1), (&apos;b&apos;, 2), (&apos;c&apos;, 3)]● dict.get() and dict.setdefault()● collections.defaultdict● Sort lists using l.sort(key=key_func)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python查缺补漏</title>
      <link href="/2015/01/07/tech/python/python%E6%9F%A5%E7%BC%BA%E8%A1%A5%E6%BC%8F/"/>
      <url>/2015/01/07/tech/python/python%E6%9F%A5%E7%BC%BA%E8%A1%A5%E6%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>python语法非常灵活，其宗旨在与指导程序员快速开发，所以，只有想不到，没有它做不到的。最近偶然看见博客园一位大神Vamei的博客，当时就感觉相见他的博客恨晚，因为他的文字幽默，流畅，有深度，从设计的角度剖析技术，把死板板的技术写得非常有文艺范。做一个科技的文艺范是我读大学以来明确自己的一个终极目标，看到Vamei的博文，我瞬间感觉到这就是我想要的状态，一个科技的文艺范。Vamei是一个非常努力的人，兴趣跟我惊人的相似，他是大神，读书无数，而我现在屌丝，所以，必须向他看齐，并超越。他写了诸多教程，精辟流畅，比太多所谓的教程个人感觉要好几十倍。以前就学过python，但都是浅于表面，没有深度。现在看过Vamei写的教程，觉得有些知识点自己还是很模糊。所以，决心还是把那些知识点记录下来，免得以后又要重新回过头再查阅，浪费时间。站在巨人的肩膀上学习，不过我会从自己的角度剖析这些知识点，加深印象。  </p><h3 id="1、print-很灵活，打印多个对象没问题"><a href="#1、print-很灵活，打印多个对象没问题" class="headerlink" title="1、print 很灵活，打印多个对象没问题"></a>1、print 很灵活，打印多个对象没问题</h3><pre><code>&gt;&gt;&gt; a = 1.4&gt;&gt;&gt; print a, type(a)1.4 &lt;type &apos;float&apos;&gt;</code></pre><a id="more"></a>  <h3 id="2、序列分为两种：tuple和list"><a href="#2、序列分为两种：tuple和list" class="headerlink" title="2、序列分为两种：tuple和list"></a>2、序列分为两种：tuple和list</h3><p>它们所存的对象灵活，如： </p><pre><code>s1 = (1, 2.3, &apos;love&apos;, true)  s2 = [false, 3.4, &apos;you&apos;]</code></pre><p>tuple所存的各个对象可变，而list不可变。 字符串是一种tuple。<br>对象的引用方式非常灵活，记住这一种方式——切片：<br><strong>基本样式：[下限：上限：步长]</strong>  </p><h3 id="3、运算符"><a href="#3、运算符" class="headerlink" title="3、运算符"></a>3、运算符</h3><p>数学：+ - * \<br>判断：== &lt; &gt; &lt;= &gt;= != in(表某个对象在某个序列中)<br>逻辑：and or not  </p><h3 id="4、字典：由键和值组成"><a href="#4、字典：由键和值组成" class="headerlink" title="4、字典：由键和值组成"></a>4、字典：由键和值组成</h3><p>键和值可以是任意对象，且一一对应。词典中的对象没有顺序，所以不能通过<em>下标</em>来引用词典中的对象。与可变对象列表不同，词典中的对象无重复。  </p><pre><code>&gt;&gt;&gt; c = {&apos;h&apos;:1, &apos;y&apos;:2, &apos;ki&apos;:4.5}&gt;&gt;&gt; c[&apos;h&apos;] = 1&gt;&gt;&gt; c {&apos;y&apos;: 2, &apos;h&apos;: 1, &apos;ki&apos;: 4.5}  </code></pre><h3 id="5、文件对象"><a href="#5、文件对象" class="headerlink" title="5、文件对象"></a>5、文件对象</h3><p>创建文件对象：f = open(文件名，模式(常用 ‘w’、’r’))<br>文件对象的方法：  </p><pre><code>flist = f.read(N) #读取N bytes的数据flist = f.readline() #读取一行  flist = f.readlines() #读取所有行</code></pre><h3 id="6、模块的导入"><a href="#6、模块的导入" class="headerlink" title="6、模块的导入"></a>6、模块的导入</h3><p>python之所以被称为胶水语言，就是因为模块化的设计机制，可以导入用其他语言写的模块到python运行环境中，当然这种情况使用的比较少，大多是一些大型的应用程序才会应用到。关于模块，更多的是python自身的模块导入。  </p><pre><code>import a as b             # 引入模块a，并将模块a重命名为bfrom a import function1   # 从模块a中引入function1对象。调用a中对象时，我们不用再说明模块，即直接使用function1，而不是a.function1。from a import *           # 从模块a中引入所有对象。调用a中对象时，我们不用再说明模块，即直接使用对象，而不是a.对象。</code></pre><h3 id="7、参数传递"><a href="#7、参数传递" class="headerlink" title="7、参数传递"></a>7、参数传递</h3><p>在python中，除了和其他语言共有的几种参数传递（如关键字参数、默认参数等）之外，还多了一种新的方式，这种方式来源于python中视一切皆为对象的特性。有时我们想要传递多个对象，为了减少代码的重复率，增强灵活性，就需要将多个对象合并成一个对象，比如，可以是序列，可以是字典等结构。这个合并参数的过程，换个说法就是对参数打包，即包裹参数传递。 这里要注意，规定如果传递的是元组，则函数的参数需要加*，如果传递的是字典，则加**。和包裹对应的解包裹，并不是包裹的反义，而是两个相互独立的过程，包裹对应定义参数时，而解包裹对应调用函数时。如下：  </p><pre><code># -*-coding:utf-8 -*-#包裹传递def func(*args):  # 对象是元组    print type(args)    print argsfunc(1,2,3)b = (1,2,3)func(b)def func1(**args):  # 对象是字典    print type(args)    print argsfunc1(d=1, e=2)#解包裹def func2(a,b,c):    print a, b, cargs = (1,2,3)func2(*args)dic = {&apos;a&apos;:1, &apos;b&apos;:2, &apos;c&apos;:3}func2(**dic)</code></pre><h3 id="8、实用的循环设计"><a href="#8、实用的循环设计" class="headerlink" title="8、实用的循环设计"></a>8、实用的循环设计</h3><p><strong>range()：</strong> for i in range(n)<br><strong>enumerate():</strong> for (index, value) in enumerate(n) 得到(下标，值)的元组<br><strong>zip():</strong>对于多个等长的序列，如果在每次循环时都要取每个序列中的一个，则用zip快速方便。  </p><pre><code>ta = [1,2,3]tb = [9,8,7]tc = [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;]for (a,b,c) in zip(ta,tb,tc):    print(a,b,c)  </code></pre><h3 id="9、函数对象"><a href="#9、函数对象" class="headerlink" title="9、函数对象"></a>9、函数对象</h3><p>python中一切皆可以看成是对象，函数有时候也看成是对象进行相应的操作，比如将函数作为参数传递给另一个函数。其中，有三个函数被定义成全局函数来使用，map()、filter()、reduce()。map、reduce功能有些类似大数据计算中用的map_reduce。这三个函数就接受一个函数对象作为参数。我们为了方便，常常用lambda函数来生成函数对象。lambda函数是一个匿名函数，如果有时候纠结与不知道为函数取什么名字，同时函数需要实现的功能又比较简单时，用lambda函数是最合适的。下面的例子举一反三。</p><pre><code>filter(lambda x: x % 2 == 0, [1,2,3,4,5]) #过滤列表中\2不为0的数  map(lambda x: x + 2, [1,2,3,4,5]) #列表中每一项分别加2  reduce(lambda x, y: x + y, [1,2,3,4,5]) #列表中两项相加和再和第三项相加，依次下去。  </code></pre><h3 id="10、python中不支持的类型"><a href="#10、python中不支持的类型" class="headerlink" title="10、python中不支持的类型"></a>10、python中不支持的类型</h3><p>char和byte，可以用长度为1的字符串来表示<br>指针：python中有个类似指针的东西，就是对象的身份标识id()，其实在python中一切皆为指针。<br>int short 和long，python中有标准整形Integer，当需要长整型时，python会自动返回长整型，非常灵活。<br>float和double，python中float实际上就是双精度浮点型，没有单精度浮点型。当需要更高的精度时，可以使用python中的十进制浮点型类型Decimal，这是一个外部模块，拥有任意的精度，足够用户任意精度的要求使用了。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python的设计哲学</title>
      <link href="/2015/01/06/tech/python/python%E8%AE%BE%E8%AE%A1%E5%93%B2%E5%AD%A6/"/>
      <url>/2015/01/06/tech/python/python%E8%AE%BE%E8%AE%A1%E5%93%B2%E5%AD%A6/</url>
      
        <content type="html"><![CDATA[<p>我始终相信任何事物（包括抽象的和具象的）的诞生都是有一定的原因的，在讯息快速更新迭代的今天，我们在接受新事物的同时，不当当要知道该事物是什么，而且有必要知道该事物的起源是什么，这样有助于我们应对这种变化，拥抱变化。<br><img src="/images/next/simple.jpg" alt=""><br><a id="more"></a></p><h2 id="扒一扒python的使用现状"><a href="#扒一扒python的使用现状" class="headerlink" title="扒一扒python的使用现状"></a>扒一扒python的使用现状</h2><p>我们都知道几个比较大的项目，如Google engine的很大部分代码，YouTube以及国内的豆瓣等，都是使用的python。python从1991年正式发布，到至今已经和我同岁了，在这段时间里，已经发生了太多的变化，特别是科技的变化，科技的变化永远都是要比其他的领域来得要快。同样，对于语言的发展来说，同样也是发生着翻天覆地的变化，因为语言是构成科技的重要元素之一，科技的发展离不开语言的发展变迁。python经过这几年的时间，也日趋成熟，其优雅简洁，便于快速开发的优点，逐渐成为各大IT公司和科研机构所青睐的语言。我们来扒一扒两个主流的编程语言社区对现今流行的编程语言的使用情况的一个调查结果。一个是TIOBE，该社区的调查主要是基于Internet上有经验的程序员、课程和第三方厂商对各类语言的使用情况，然后使用搜索引擎技术来进行计算得出；另外一个是CodeForge，不同于TIOBE，该社区的调查则是来源于五万六千多名软件工程师的问卷调查。下面是TIOBE的调查结果。   </p><p><img src="/images/next/langugerank.PNG" alt="">  </p><p><img src="/images/next/languge.PNG" alt="">   </p><p>下面是CodeForge的调查结果。  </p><p><img src="/images/next/pythonchina.PNG" alt="">  </p><p><img src="/images/next/pythonworld.PNG" alt="">   </p><p>可见，python对于业界来说，并不是特别主流的语言，更主流还要属C、C++、java这些老牌语言，因为学习这些语言便于理解计算机底层的执行流程，所以这些语言基本上都是每个学计算的人的入门语言。而对于python而言，其优点在于简洁高效，可以让开发者不必过多的关注底层的执行机制，而专注在更高层的框架设计上。所以，python相对于这些老牌语言，使用率不高也是可以理解，但是当要做一些更高端的操作，如科学计算，使用python将会大大减少开发时间，提高开发效率。因此，这些榜单只能反映某个编程语言的热门程度，并不能说明一门编程语言的好与不好，或者一门语言所编写的代码量的多少。  </p><h2 id="python的起源"><a href="#python的起源" class="headerlink" title="python的起源"></a>python的起源</h2><p>新事物的出现离不开旧有事物的缺陷，python的设计灵感来源自然少不了旧有的语言缺陷。python是由一位荷兰的数学兼计算机专家Guido Van Rossum于1989年底发明的。当时老爷子在一家IT公司工作，他在这家公司参与设计了一种用于教学的语言，称为ABC语言。这种语言语法非常接近于自然语言，几乎和自然语言同等，不同之处也许只有那些标号等细小的点。但是由于这种语言一个不开放（也许比较臃肿），另外一个对机器性能要求极高。这对于当时的几KB RAM的机器配置来说，想要完成一些基本任务都是很难的。为了满足现有硬件的性能要求，只能从语言本身去做优化，才能满足更多的需求。之后，Guido就继承了ABC语言简洁之道，开发了python，为了继续优化性能，Guido也借鉴了shell的设计思想，并去除了ABC语言所特有的语法臃肿的特性，加入C、C++等这些常用语言语法简洁的特性。可以说，python集百家之长，完成了华丽的崛起。  </p><p><img src="/images/next/pythonfarther.PNG" alt="">   </p><p>正因为此，python也被称为胶水语言（Glue Language），它能够将用其他语言写成的模块（尤其是C\C++）很轻松的粘合在一起。常见的一种应用情形是，使用python快速生成程序的框架，然后对其中要求特别高的部分，用更合适的语言写，比如3D游戏中的图形渲染模块，对性能要求极高，就可以用C++重写，还有Google Engine也是这样做的。至于为什么Guido会取名python，据称是因为Monty Python’s Flying Circus(蒙提*派森飞行马戏团)这个剧，Guido是这个剧的狂热粉丝，名字里面就有一个Python字样，又叫做大蟒蛇，所以才会有了python的logo是一条可爱的蟒蛇。后来据说是1989年的圣诞，这个剧停播，Guido为了打发圣诞假期，才动手写的python，听起来好传奇，牛人就是这样，没有做不到，只有想不到。  </p><h2 id="设计宗旨"><a href="#设计宗旨" class="headerlink" title="设计宗旨"></a>设计宗旨</h2><p>python的设计哲学总结为六个字，就是：优雅、明确、简单。为了能够让广大python爱好者全面了解python的设计思想，python社区的人每天都在源源不断的贡献自己的智慧和精力。其中有一位叫Peter的开发者用及其精辟的话整理总结了python的特性，并将之加入到python的模块中，成为了一个小彩蛋。想见这个彩蛋，只需在shell下<em>import this</em>即可。  </p><pre><code>&gt;&gt;&gt; import thisThe Zen of Python, by Tim PetersBeautiful is better than ugly. 优美胜于丑陋Explicit is better than implicit.明确胜于晦涩Simple is better than complex.简单胜于复杂Complex is better than complicated.复杂胜于凌乱Flat is better than nested.扁平胜于嵌套Sparse is better than dense.稀疏胜于稠密Readability counts.可读性需要考虑Special cases aren&apos;t special enough to break the rules.即使情况特殊，也不应打破规则Although practicality beats purity.尽管使用胜于纯净Errors should never pass silently.错误不应该悄无声息的忽略Unless explicitly silenced.除非特意这么做In the face of ambiguity, refuse the temptation to guess.面对混淆是，拒绝猜测（深入搞明白问题）There should be one-- and preferably only one --obvious way to do it.总有一个，且仅有一个，明显的方法来处理问题Although that way may not be obvious at first unless you&apos;re Dutch.Now is better than never.现在开始胜过永远不开始Although never is often better than *right* now.尽管永远不开始经常比仓促立即开始好If the implementation is hard to explain, it&apos;s a bad idea.如果程序的实现很难解释，那么它不是一个很好的实现If the implementation is easy to explain, it may be a good idea.反之Namespaces are one honking great idea -- let&apos;s do more of those!命名空间是个绝好的注意，让我们多利用它  </code></pre><p>这就是python的设计之禅，python社区的这群人都是非常幽默的，将python的设计思想放在解释器中，让人怎么也想不到，这还真是一番人生哲学啊。短短的几行字，借用一个网友说的，每个点都千锤百炼；每一点都直指人内心的感觉；既有指导大是大非的一年，又有指导细节操作的原则；既有谆谆教诲的推荐，也有声色俱厉的禁止。看N遍，每一遍都会让人深思，这就是哲学。     </p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一款值得信赖的编辑器——Sublime Text</title>
      <link href="/2014/12/25/tech/tools/Sublime%20Text/"/>
      <url>/2014/12/25/tech/tools/Sublime%20Text/</url>
      
        <content type="html"><![CDATA[<p>无意中遇到这款编辑器，欲罢不能，先在这里简单记录下，以后在用的过程中在进一步体验。<br>详见：<a href="http://www.cnblogs.com/dolphin0520/archive/2013/04/29/3046237.html" target="_blank" rel="noopener">将sublime Text搭建成一个好用的IDE</a><br><a href="http://www.iplaysoft.com/sublimetext.html" target="_blank" rel="noopener">Sublime Text 2 - 性感无比的代码编辑器</a><br><a href="http://zh.lucida.me/blog/sublime-text-complete-guide/" target="_blank" rel="noopener">用户手册</a><br><a href="http://www.zhihu.com/question/19976788" target="_blank" rel="noopener">知乎讨论</a>  </p>]]></content>
      
      
      <categories>
          
          <category> 05 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sublime Text </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统学习知识点整理</title>
      <link href="/2014/12/17/tech/other/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
      <url>/2014/12/17/tech/other/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<p>本文由知乎<a href="http://www.zhihu.com/question/21251105" target="_blank" rel="noopener">如何学习推荐系统？</a>一文整理而来，如有雷同，纯属正常。^.^<br>推荐系统一直是我最感兴趣的技术，但是苦于课题组不做这块的东西，所以只能借助于课余时间去学习。我觉得兴趣的东西还是不能丢，趁现在还是学生时代，抓紧时间多学些东西，让自己的兴趣尽可能地放大，说白了，就是做自己喜欢做的事，不让将来有后悔的念头。关于如何学习，对于初学者，肯定只能站在巨人的肩膀上才能有所突破，本文就简单地对巨人的工作做一点整理，希望对今后的学习有所帮助。  </p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>推荐系统不算是一个独立的学科，它与机器学习，数据挖掘有天然密不可分的关系。所以，想要学习好这门技术，就要多有所涉略。</p><h3 id="技术博客"><a href="#技术博客" class="headerlink" title="技术博客"></a>技术博客</h3><p>1、<a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy1/index.html#icomments" target="_blank" rel="noopener">探索推荐引擎内部的秘密</a>:对现有的推荐系统技术进行了综述性描述，如下：<br><img src="/image/recomsys.png" alt=""><br><a id="more"></a><br>2、<a href="http://semocean.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E6%96%87%E7%8C%AE%E5%8F%8A%E8%B5%84%E6%96%99/" target="_blank" rel="noopener">推荐系统经典论文文献及业界应用</a>:百度技术专家的技术博客。里面有很多参考资料。<br>3、<a href="http://blog.csdn.net/java060515/article/details/1570243" target="_blank" rel="noopener">个性化推荐技术漫谈</a>：对个性化推荐技术的基本原理进行简要介绍，提出了作者对优秀的个性化推荐的多角度认识。  </p><h3 id="阅读最新Paper"><a href="#阅读最新Paper" class="headerlink" title="阅读最新Paper"></a>阅读最新Paper</h3><p>1、几个重要会议：<strong>recsys</strong>,SIGIR,<strong>KDD</strong>,WSDM,WWW,ICDM…<br>2、 <strong>Recommendation Engines Seminar Paper</strong>, Thomas Hess, 2009: 推荐引擎的总结性文章，Thomas 给出推荐引擎的模型，各种推荐机制的工作原理，并分析了推荐引擎面临的众多问题。<br>3、其余更细致的内容参考本文：<a href="http://semocean.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E6%96%87%E7%8C%AE%E5%8F%8A%E8%B5%84%E6%96%99/" target="_blank" rel="noopener">推荐系统经典论文文献及业界应用</a>，资料非常齐全。  </p><h3 id="相关书籍"><a href="#相关书籍" class="headerlink" title="相关书籍"></a>相关书籍</h3><p>1、项亮《推荐系统实践》<br>2、Shapira B. <strong>Recommender systems handbook[M]</strong>. Springer, 2011. 推荐系统可做枕头，也应该放在枕边的书籍，看了半本多。如果将该书及其中的参考文献都看完并理解，那恭喜你，你已经对这个领域有深入理解了。<br>3、Jannach D, Zanker M, Felfernig A, et al. <strong>Recommender systems: an introduction[M]</strong>. Cambridge University Press, 2010. 可以认为是2010年前推荐系统论文的综述集合。<br>4、Celma O. <strong>Music recommendation and discovery[M]</strong>. Springer, 2010. 主要内容集中在音乐推荐，领域非常专注于音乐推荐，包括选取的特征，评测时如何考虑音乐因素。  </p><h3 id="相关视频"><a href="#相关视频" class="headerlink" title="相关视频"></a>相关视频</h3><p>1、<a href="https://www.coursera.org/course/ml?from_restricted_preview=1&amp;course_id=970311&amp;r=https%3A%2F%2Fclass.coursera.org%2Fml-003%2Fclass" target="_blank" rel="noopener">Stanford-&gt;机器学习</a>   </p><h3 id="相关系统"><a href="#相关系统" class="headerlink" title="相关系统"></a>相关系统</h3><p>1、<a href="www.amazon.com">Amazon</a>：推荐技术的先驱，Amazon 在 B2C 领域的推荐技术值得大家参考。<br>2、<a href="www.douban.com">豆瓣</a>:作为国内社交网络的先驱，豆瓣在推荐技术上也处于领先的位置，同时对于不同内容的推荐策略有深入的研究。<br>3、<a href="http://pan.baidu.com/share/link?shareid=2173369320&amp;uk=1493671608" target="_blank" rel="noopener">淘宝推荐系统</a><br>4、<a href="http://pan.baidu.com/share/home?uk=1493671608#category/type=0" target="_blank" rel="noopener">当当网搜索和推荐 庄洪波</a><br>5、<a href="http://pan.baidu.com/share/link?shareid=2228144324&amp;uk=1493671608" target="_blank" rel="noopener">盛大只能对剑系统的开发与应用</a><br>6、<a href="http://blog.csdn.net/jj12345jj198999/article/details/8821419" target="_blank" rel="noopener">一个入门级的电影推荐系统</a> </p><h3 id="推荐系统工具"><a href="#推荐系统工具" class="headerlink" title="推荐系统工具"></a>推荐系统工具</h3><p>1、<a href="http://mahout.apache.org/" target="_blank" rel="noopener">Mahout</a>：基于hadoop的机器学习，数据挖掘，推荐系统开源工具。<br>2、<a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">scikit-learn</a>：基于python的机器学习，数据挖掘库，方便好用，适合数据量较小的调研任务，不过，一切不支持大数据的机器学习算法，（一定程度上）都是耍流氓。。。。<br>3、<a href="http://www.cs.waikato.ac.nz/ml/weka/" target="_blank" rel="noopener">weka</a>：经典得不能再经典的数据挖掘工具，java版本<br>4、<a href="http://www.r-project.org/" target="_blank" rel="noopener">R</a>：R语言<br>5、<a href="http://glaros.dtc.umn.edu/gkhome/views/cluto" target="_blank" rel="noopener">Cluto</a>：聚类工具，集成了较多聚类算法及相似度度量方法；单机，数据量受限<br>6、<a href="http://rapidminer.com/products/rapidminer-studio/" target="_blank" rel="noopener">RapidMiner</a>：没用过，但据说使用量非常大<br>7、<a href="http://svdfeature.apexlab.org/wiki/Main_Page" target="_blank" rel="noopener">svdfeature</a>: 上交Apex开发的svd工具集，代码质量不错，而且附带（MovieLen数据集）示例，直接下载各MovieLens数据集就能实验效果<br>8、<a href="http://www.libfm.org/" target="_blank" rel="noopener">LibFM:Rendle</a> S. Factorization machines with libFM[J]. ACM Transactions on  </p><h3 id="经典推荐算法大赛数据"><a href="#经典推荐算法大赛数据" class="headerlink" title="经典推荐算法大赛数据"></a>经典推荐算法大赛数据</h3><p>1、<a href="http://pan.baidu.com/s/1hqilwcW" target="_blank" rel="noopener">netflix大赛数据</a>：netflix大赛数据，想尝试各种算法效果，可以用该数据做实验；netflix已经不再发布数据，如有需要可从该链接下载。<br>2、<a href="http://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">Movielen数据集</a>   </p><h3 id="相关参考资料"><a href="#相关参考资料" class="headerlink" title="相关参考资料"></a>相关参考资料</h3><p>1、<a href="https://groups.google.com/group/resys" target="_blank" rel="noopener">Google Recommender System Group</a>：推荐系统的 Google 讨论组，有很多关于推荐引擎的有趣讨论。<br>2、<a href="http://www.deitel.com/ResourceCenters/Web20/RecommenderSystems/RecommenderSystemAlgorithms/tabid/1317/Default.aspx" target="_blank" rel="noopener">Recommender System Algorithms</a>：关于推荐引擎算法的资源。<br>3、<a href="http://www.slideshare.net/rashmi/design-of-recommender-systems" target="_blank" rel="noopener">Design of Recommender System</a>：关于推荐引擎的设计方法的介绍。<br>4、<a href="http://www.slideshare.net/blueace/how-to-build-a-recommender-system-presentation" target="_blank" rel="noopener">How to build a recommender system</a>：这个演示给出了如何构建一个推荐引擎，并结合例子详细介绍了基于协同过滤的推荐策略。<br>5、<a href="http://wuchong.me/blog/2014/04/19/recsys-cf-study/" target="_blank" rel="noopener">协同过滤技术的实现</a></p><h3 id="后序"><a href="#后序" class="headerlink" title="后序"></a>后序</h3><p>其余关于数据挖掘和机器学习的资料请参见<a href="http://semocean.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E6%96%87%E7%8C%AE%E5%8F%8A%E8%B5%84%E6%96%99/" target="_blank" rel="noopener">推荐系统经典论文文献及业界应用</a>一文。不管怎么说，最终想要在这方面取得点成果，最重要的是动手实践，实际参加开发一个推荐系统就好了，而这正是我想做的事，也是我的一点小小的理想吧。  </p><p>附：资料大合集（哇塞，有福了！）<br><a href="https://github.com/Flowerowl/Big-Data-Resources" target="_blank" rel="noopener">呵呵哈哈</a><br><a href="http://rec-sys.net/forum.php" target="_blank" rel="noopener">论坛</a>  </p>]]></content>
      
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>程序员必须知道的10个算法和数据结构？（转自伯乐在线）</title>
      <link href="/2014/11/25/tech/algo/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E7%9F%A5%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
      <url>/2014/11/25/tech/algo/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E7%9F%A5%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="Arjun-Nayini的推荐"><a href="#Arjun-Nayini的推荐" class="headerlink" title="Arjun Nayini的推荐"></a>Arjun Nayini的推荐</h2><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ul><li>1、图搜索 （广度优先、深度优先）深度优先特别重要  </li><li>2、排序</li><li>3、动态规划</li><li>4、匹配算法和网络流算法</li><li>5、正则表达式和字符串匹配</li></ul><a id="more"></a><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><ul><li>1、图（树尤其重要）</li><li>2、Map</li><li>3、栈/队列</li><li>5、Tries | 字典树</li></ul><h3 id="额外推荐"><a href="#额外推荐" class="headerlink" title="额外推荐"></a>额外推荐</h3><ul><li>贪婪算法</li><li>概率方法</li><li>近似算法</li></ul><h2 id="Ken-George的推荐"><a href="#Ken-George的推荐" class="headerlink" title="Ken George的推荐"></a>Ken George的推荐</h2><h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul><li>三路划分-快速排序</li><li>合并排序（更具扩展性，复杂度类似快速排序）</li><li>DF/BF搜索（要知道使用场景）</li><li>Prim/Kruskal(最小生成树)</li><li>Dijkstra(最短路径算法)</li></ul><h3 id="数据结构-1"><a href="#数据结构-1" class="headerlink" title="数据结构"></a>数据结构</h3><ul><li>HashMap（真的要知道所有的哈希结构）</li><li>图和树（红黑树很好学）</li><li>堆（优先级队列）</li><li>栈/队列</li><li>Tries | 字典树</li><li>A*和遗传算法也很有趣</li></ul>]]></content>
      
      
      <categories>
          
          <category> 04 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python Django第一步</title>
      <link href="/2014/10/27/tech/python/Python%20Django%E7%AC%AC%E4%B8%80%E6%AD%A5/"/>
      <url>/2014/10/27/tech/python/Python%20Django%E7%AC%AC%E4%B8%80%E6%AD%A5/</url>
      
        <content type="html"><![CDATA[<p>几经深思和犹豫之后，终于下定了决心要学Django，这是一个如何规划效用成本的问题，有得必有吧，但这是理想的东西，即使是蜗牛的速度，不管什么时候都应该去尝试一下。  </p><p>在正式入门之前，首先要解决的就是开发环境部署的问题，我选择的是Linux，原因是Django依附于多个相关的python库，在windows上安装着实不方便，而在Linux上就显得格外简单，一行sudo apt-get install XXX的命令就可以搞定。本文主要记录我在安装Django的过程中所遇到的一些问题。 </p><a id="more"></a> <h3 id="安装Django"><a href="#安装Django" class="headerlink" title="安装Django"></a>安装Django</h3><p>首先在<a href="http://www.djangoproject.com/download/" target="_blank" rel="noopener">python django</a>下载Django，完了之后执行以下命令安装：  </p><pre><code>1、tar xzvf Django-\*.tar.gz   2、cd Django-\*  3、sudo python setup.py install    </code></pre><p>这时会提示no module named steuptools，这说明在安装Django需要先安装setuptools库，打开<a href="https://pypi.python.org/pypi/setuptools" target="_blank" rel="noopener">setuptools的python官网</a>看看setuptools该如何安装，如下：<br><img src="/images/next/steuptools.PNG" alt=""><br>除了这种安装方法之外，也可以直接下载setuptools软件包后安装：<br>(1)下载setuptools包  </p><pre><code># wget http://pypi.python.org/packages/source/s/  setuptools/setuptools-2.0.tar.gz  </code></pre><p>(2)解压setuptools包  </p><pre><code># tar zxvf setuptools-2.0.tar.gz  # cd setuptools-2.0  </code></pre><p>(3)编译setuptools  </p><pre><code># python setup.py build  </code></pre><p>(4)开始执行setuptools安装  </p><pre><code># python setup.py install  </code></pre><p>除此之外，还有一种安装方法是使用pip，pip是什么，打开<a href="https://pypi.python.org/pypi/pip" target="_blank" rel="noopener">pip的python官网</a>，我们看到，pip是’A tool for installing and managing Python packages’，也就是说pip是python的软件安装工具，下面是pip的使用方法：<br>安装包：       </p><pre><code>pip install SomePackage  </code></pre><p>查看安装包时安装了哪些文件：  </p><pre><code>pip show --files SomePackage  </code></pre><p>查看哪些包有更新：  </p><pre><code>pip show --files SomePackage  </code></pre><p>更新一个软件：  </p><pre><code>pip install --upgrade SomePackage  </code></pre><p>卸载软件：  </p><pre><code>pip uninstall SomePackage  </code></pre><p>但是用pip，首先得安装，同样的方法，在上面的pip python首页下载pip 包(pip-1.4.1.tar.gz)，使用 “ tar -xvf pip-1.4.1.tar.gz” 解压，cd 进文件夹，使用 “python setup.py install” 命令安装软件。（如果你不想使用pip安装软件包，也可以用此方法下载、解压后使用 “python setup.py install”安装！安装完pip之后，就可以用pip直接安装setuptools了，如下命令：  </p><pre><code>sudo pip install steuptools      </code></pre><p>安装完setuptools之后在重新运行sudo python setup.py install，等待其执行完之后，Django 就安装完成了。接下来，我们进入python的交互命令行，测试一下Django是否安装成功，如果出现以下代码，则表示安装成功：  </p><pre><code>&gt;&gt;&gt; import django  &gt;&gt;&gt; django.VERSION  (1, 7, 1, &apos;final&apos;, 0)    </code></pre><h3 id="安装数据库"><a href="#安装数据库" class="headerlink" title="安装数据库"></a>安装数据库</h3><p>django只要求python正确安装后就可以跑起来了。 不过，如果想开发一个数据库驱动的web站点时，你应当需要配置一个数据库服务器。<br>如果你只想玩一下，可以不配置数据库，直接跳到 开始一个project 部分去，不过你要注意本书的例子都是假设你配置好了一个正常工作的数据库。<br>Django支持四种数据库：</p><pre><code>PostgreSQL (http://www.postgresql.org/)SQLite 3 (http://www.sqlite.org/)MySQL (http://www.mysql.com/)Oracle (http://www.oracle.com/)  </code></pre><p>大部分情况下，这四种数据库都会和Django框架很好的工作。 （一个值得注意的例外是Django的可选GIS支持，它为PostgreSQL提供了强大的功能。）如果你不准备使用一些老旧系统，而且可以自由的选择数据库后端，我们推荐你使用PostgreSQL，它在成本、特性、速度和稳定性方面都做的比较平衡。  </p><p>如果只是玩一下，不想安装数据库服务，那么可以考虑使用SQLite。 如果你用python2.5或更高版本的话，SQLite是唯一一个被支持的且不需要以上安装步骤的数据库。 它仅对你的文件系统中的单一文件读写数据，并且Python2.5和以后版本内建了对它的支持。  </p><h3 id="开始一个项目"><a href="#开始一个项目" class="headerlink" title="开始一个项目"></a>开始一个项目</h3><p>如果安装好了python，django和（可选的）数据库及相关库，你就可以通过创建一个project，迈出开发django应用的第一步。</p><p>首先得为项目新建一个工作区，如/home/username/django/djcode，进入该目录，运行命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">django-admin.py startproject mysite</span><br></pre></td></tr></table></figure><p>则会在当前目录下创建一个目录mysite。如果没有对django-admin.py该命令的环境进行配置，则会报错。所以，首先得对django的环境进行配置，如何在linux下配置环境变量，网上相关的配置方法有好几种，我们选择最简单的一种，即修改用户目录下的.bashrc文件，利用命令$gedit ~/.bashrc打开.bashrc，转到最后，添加如下的命令：</p><pre><code># set Djangoexport PATH=$PATH:/home/bycer/Django/Django-1.7.1/django/bin  </code></pre><p>此时，重启终端，再次进入项目目录，重新运行：django-admin.py startproject mysite,就可以生成项目文件。<br>其中包含几个文件：  </p><pre><code>\_\_init__.py  manage.py  settings.py  urls.py  wsgi.py   </code></pre><h3 id="运行开发服务器"><a href="#运行开发服务器" class="headerlink" title="运行开发服务器"></a>运行开发服务器</h3><p>django开发服务器是可用在开发期间的，一个内建的，轻量级的web服务。无需进行产品级的web服务器（如Apache）的配置工作，开发服务器会监视代码并自动加载它。进入mysite，运行以下命令：  </p><pre><code>python manage.py runserver  </code></pre><p>将会看到：  </p><pre><code>Validating models...  0 errors found.  Django version 1.0, using settings &apos;mysite.settings&apos;  Development server is running at http://127.0.0.1:8000/  Quit the server with CONTROL-C.  </code></pre><p>此时在浏览器中访问<a href="http://127.0.0.1:8000/就可以看到django的欢迎界面了。此时就说明一个简单的基于Django的web应用成功部署完成。" target="_blank" rel="noopener">http://127.0.0.1:8000/就可以看到django的欢迎界面了。此时就说明一个简单的基于Django的web应用成功部署完成。</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>上传代码到GitHub的初尝试</title>
      <link href="/2014/10/02/tech/tools/%E4%B8%8A%E4%BC%A0%E4%BB%A3%E7%A0%81%E5%88%B0GitHub/"/>
      <url>/2014/10/02/tech/tools/%E4%B8%8A%E4%BC%A0%E4%BB%A3%E7%A0%81%E5%88%B0GitHub/</url>
      
        <content type="html"><![CDATA[<p>Github是Git旗下的分布式版本管理系统的库，主要用于代码的托管、展示和分享，方便程序员合作开发，所以，Github相当于一个远程的仓库或者是托管代码的云服务器，也可以把其看成是分享和展示代码的网站。<br>这个库早早就听说过，但是一直没有使用，现在随着写的代码越来越多，而且越来越大，代码的管理和维护成为了不得不面对的问题，但作为一个IT爱好者或是即将的IT从业者（我不想用程序员这个词，总感觉这个职业处在压迫之中），不应该把精力放在后期频繁而琐碎的代码维护上，而应该专注于前期的代码开发上。所以，Github这个工具非常合适。但是其入门并不是那么傻瓜式的，因为它依托于Git，需要借助于Git的各种命令来实现代码上传与托管，网上各种入门级的教程，眼花缭乱，与其看别人的，不如自己写一篇，也供以后自己的查阅，若是能够帮助到一两个网友，也不枉费我的辛苦。  </p><h3 id="提交代码的方式"><a href="#提交代码的方式" class="headerlink" title="提交代码的方式"></a>提交代码的方式</h3><p>通过看网友的教程，可以大致分为三种方式，因为代码的提交都是要通过本地客户端来完成，所以这三种方式都是依据不同的客户端来分的：<br>1、使用msysgit客户端，关于如何使用msysgit上传代码，请参见这篇博文：<a href="http://blog.csdn.net/hcbbt/article/details/11651229" target="_blank" rel="noopener">初识Github</a><br>2、直接使用Github的客户端，对应不同的OS，有不同的版本，如windows平台就有Github for Windows，其为windows用户提供了一个基本的图形前端去处理大部分常用版本控制任务，可以创建版本库，向本地版本库提交补丁，在本地和远程版本库之间同步。使用Github提交代码，参见这篇：<a href="http://www.freair.com/bbs/read.php?tid=892" target="_blank" rel="noopener">从不会到会用Github</a><br>3、使用Git客户端，配有Git bash和Git GUI,本文采用这种方法。 </p><h3 id="上传自己的项目到Github"><a href="#上传自己的项目到Github" class="headerlink" title="上传自己的项目到Github"></a>上传自己的项目到Github</h3><p>主要遵循以下两个原则： </p><p><strong>将Github上新建的库clone到本地</strong><br><strong>修改或更新之后上传到Github</strong></p><p>基于以上两个原则，按照以下几步进行操作：<br>1、在Github上建立项目<br>登录Github之后，找到并点击按钮“New Repository”，即可新建一个项目，记住项目地址，以便后面使用。（PS：项目地址在Code一栏处HTTPS的地方，复制即可）。如果说不是第一次上传代码或Git已经配置过，请跳转到第4步。<br>2、配置Git以及上传代码<br>初次使用Git需要对其进行配置，主要就是让Git记录你Github上的账号名邮件地址，输入如下两行即可：  </p><pre><code>git config --global user.name(&quot;your real name&quot;)  git config --global user.email (&quot;you@email.address&quot;)  </code></pre><p>3、认证Github<br>这一步比较麻烦，需要利用Git生成一个SSH密钥，并提交该密钥到Github上。具体生成密钥和提交密钥的步骤请见<a href="http://serholiu.com/github-share-code" target="_blank" rel="noopener">在GitHub上分享和展示你的代码</a><br>4、上传代码到Github<br>秉承下载又上传的原则，刚开始新建的库需要先克隆到本地，然后在上传，所以，首先，通过Git Bash进入需要保存项目的地方，命令的操作和Linux相似。然后执行下面克隆命令操作：  </p><pre><code>git clone https://github.com/XXX/XXX.git  </code></pre><p>上面的地址就是前面所记录的地址。如果说该库在本地已存在，就不用克隆，直接上传文件即可。下面就是上传操作：  </p><pre><code>a、git add .   </code></pre><p>//该操作是上传当前目录下的全部文件，如果只上传单个文件，则如：git add test.md  </p><pre><code>b、git commit -am &apos;commit&apos;   </code></pre><p>//提交，让上条增加文件命令生效，同时显示代码文件的说明，即代码提交到Github上的提示说明，说明该代码是干嘛的。  </p><pre><code>c、git remote add origin https://github.com/XXX/XXX.git  </code></pre><p>//向本地仓库中添加远程仓库地址，远程仓库地址别名为origin，如果出现下面的错误：<br>fatal: remote origin already exists<br>则执行如下语句：<br>git remote rm origin  </p><pre><code>d、git pull origin master  </code></pre><p>//将origin所代表的远程仓库地址里的Master主干下载到本地仓库，即上传之前先进行一次同步  </p><pre><code>e、git push -u origin master </code></pre><p>//将本地仓库上传到origin所代表的远程仓库的master分支上<br>到此，就可以到Github页面上看，就会看到本地的代码文件已经同步到远程仓库中，这里，只要记住一点：<br><strong>先把远程服务器Github上面的文件先pull下来，在push上去</strong><br>关于Github的操作还有很多，在这里，我们只做简单代码上传和托管操作，我觉得只用记住这几条命令即可，后期在做进一步深入的时候，可以在继续学习。mark~~</p>]]></content>
      
      
      <categories>
          
          <category> 05 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git的学习资源</title>
      <link href="/2014/10/01/tech/tools/github/"/>
      <url>/2014/10/01/tech/tools/github/</url>
      
        <content type="html"><![CDATA[<p>最近看到一个工具，用于托管代码的仓库，Github，其相当于为本机客户端提供了一个远程管理代码的云端服务器，本机的代码可以克隆到云端，也可以从云端同步到客户端，对于程序员来说是一个非常强大的工具，这样程序员就可以不用担心代码的累积给自己造成的困扰，可以安心专注于算法，程序的设计上，而不用为其他的一些琐事儿烦恼，如后期的代码整理，查询和修改。Github是Git下面的一个子项目，Git是一款自由和开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目，类似的系统，我们可能比较熟悉的是像微软的CVS、SVN等这些免费的版本控制系统，但是这些系统是集中式的，不但速度慢，而且必须联网才能使用。由于Git是基于Linux社区建立起来的一个系统，所以其拥有开源和分布式等的优点，现在已经成为业界最为流行的分布式版本控制系统。写这篇文章只是作为简单的了解，并列举网上几个入门级的资料，方便自己以后进行更为深入的学习。见下面：  </p><p><a href="http://www.liaoxuefeng.com/" target="_blank" rel="noopener">廖雪峰的官方网站</a><br><a href="http://www.freair.com/bbs/read.php?tid=892" target="_blank" rel="noopener">从不会到会使用Github需要几步？</a><br><a href="http://www.yangzhiping.com/tech/github.html" target="_blank" rel="noopener">如何高效利用Github</a><br><a href="http://www.eoeandroid.com/thread-274556-1-1.html" target="_blank" rel="noopener">史上最全github使用方法：github入门到精通</a></p>]]></content>
      
      
      <categories>
          
          <category> 05 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matplotlib初探</title>
      <link href="/2014/09/25/tech/python/matplotlib%E5%88%9D%E6%8E%A2/"/>
      <url>/2014/09/25/tech/python/matplotlib%E5%88%9D%E6%8E%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近摸索了Python的画图库<a href="http://matplotlib.org/index.html" target="_blank" rel="noopener">Matplotlib</a>，其画的图要多炫丽有多炫丽，要多优雅有多优雅，真的有被震撼到，最近项目要求需要画图，考虑到matlab虽然博大精深，但却不易上手，貌似网上最近也在盛传“是否matplotlib能取代matlab”的诸多言论，对于一个旁观者和门外汉，我不做评论，我只有一个理念：什么好学用什么。另外，对于语言的学习和掌握，我现在有一个特别明晰的目标，就是以后以Python为主轴，C\C++作为辅助，算法为核心来展开。  </p><p>半年前就接触Python，但期间由于没有一个特定的目标，东捡一点西捡一点，到头来总是在重复同样的事情。现在觉得需要什么学什么，带有目标地去学习才能提高效率，也才能记忆犹新，废话不多说，本文就简单的说说matplotlib。由于刚入门，也没有什么心得，仅仅是看了<a href="http://www.loria.fr/~rougier/teaching/matplotlib/" target="_blank" rel="noopener">matplotlib tutorial</a>的一点笔记记录。由于手册是英文版，所以，本文算是对原文的翻译，但我会完全脱离原文的思路，用自己的思路来写，外加自己的一些引申的东西。在进入主题之前，先看下这个知乎大牛关于<a href="http://www.zhihu.com/question/21664179" target="_blank" rel="noopener">如何在论文中画出漂亮的插图</a>的探讨，先目睹一下matplotlib所体现出炫丽与优雅。</p><a id="more"></a>  <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>先来看看matplotlib的Wikipedia介绍，懒得翻译了。  </p><p><a href="http://en.wikipedia.org/wiki/Matplotlib" target="_blank" rel="noopener">matplotlib</a> is a plotting library for the Python programming language and its <strong>NumPy</strong> numerical mathematics extension. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like wxPython, Qt, or GTK+. There is also a procedural <strong>pylab</strong> interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB. <strong>SciPy</strong> makes use of matplotlib. </p><p>matplotlib 一个用来画2D图形的Python库(package)，它同时支持交互式和非交互式的画图方式，能够把画出的图形保存为PNG，PS等多种格式，可以使用多种窗口的工具包，如GTK+，wxWidgets,Qt等，并且可以画出多种类型的图形，如线、条形图、饼状图、柱状图等，主要用于科学计算领域，像常见的<strong>gnuplot</strong>和<strong>MATLAB</strong>一样，它是由John Hunter开发的，他说过一句非常经典的话：<br><strong>Matplotlib tries to make easy things easy and hard things possible.</strong>    </p><p>可见，matplotlib是多么令人心动不已。  </p><p>matplotlib非常的灵活和友善，提供多种画图的方式，在<a href="http://www.packtpub.com/matplotlib-python-development/book?utm_source=matplotlib.sourceforge.net&amp;utm_medium=link&amp;utm_content=pod&amp;utm_campaign=mdb_002124" target="_blank" rel="noopener">《Matplotlib for python developers》</a>中总结了三种使用matplotlib画图的方式：  </p><ul><li><em>pyplot</em>: matplotlib.pyplot提供类似于MATLAB的命令集合，使其表现得像MATLAB。</li><li><em>pylab</em>:结合了matplotlib.pyplot和Numpy的功能，既可以画图，又可以做科学计算，说白了，它就是一个使matplotlib表现得像MATLAB的一个接口，但是John不推荐使用这种方式，原因就是它的封装性覆盖了matplotlib的一些基本的东西，不适合开发者学习，往往造成知其然而不知其所以然的状态。</li><li><em>Object-oriented way（OO）</em>：面向对象方法，以python的方式使用，更加的pythonic，这种方法对于开发者来说可以完全控制matplotlib的执行过程，是最好的，但同时也是最复杂的。</li></ul><p>下面分别以同一个画图例子来说明这三种方式：<br><strong>pyplot</strong>:  </p><pre><code>import matplotlib.pyplot as pltimport numpy as npx = np.arange(0, 10, 0.1)y = np.random.randn(len(x))plt.plot(x, y)plt.title(&apos;random numbers&apos;)plt.show()</code></pre><p>得到如下的图示：  </p><p><img src="/images/next/plt.png" alt="">  </p><p><strong>pylab</strong>:  </p><pre><code>from pylab import *x = arange(0, 10, 0.1)y = randn(len(x)) plot(x, y) title(&apos;random numbers&apos;)show()</code></pre><p><strong>Object-oriented way（OO）</strong></p><pre><code>import matplotlib.pyplot as pltimport numpy as np x = np.arange(0, 10, 0.1)y = np.random.randn(len(x))fig = plt.figure()ax = fig.add_subplot(111)l, = plt.plot(x, y)t = ax.set_title(&apos;random numbers&apos;)plt.show()</code></pre><p>可以看到，面向对象的方式和pyplot需要导入同样的库，才能做相应的操作，在所有代码中，pylab最简单，pyplot其次，OO最复杂，几乎和画图有关的所有元素都由用户自己定义，所以，这种方式更加能够剖析matplotlib的运行机理。因为我们写代码的目标是解决问题，解决问题讲究的是结果和效率，所以，我觉得用pylab是比较清爽的，但每个人都有自己不同的要求，如果一个程序2/3的代码量是依托于Plot，那么用OO的方式是比较合适的，如果只当它是个工具，就应该用最简单的。下面通过一个例子由浅入深地再来matplotlib的美妙之处，这个例子前面说过是翻译，可以找到前面的链接进入原版。  </p><p>这个例子是显示一个cos(x)和sin(x)，由浅入深，层层递进。我们使用pylab来操作。  </p><h3 id="第一步：Using-default"><a href="#第一步：Using-default" class="headerlink" title="第一步：Using default"></a>第一步：Using default</h3><p>使用Plot的默认参数，包括figure size, dpi, line width, color, style, axes, axis, grid properties text and ront properties and so on.代码如下：  </p><pre><code>from pylab import *X = np.linspace(-np.pi, np.pi, 256, endpoint = True)C, S = np.cos(X), np.sin(X)plot(X, C)plot(X, S)show()</code></pre><p><img src="/images/next/plt1.png" alt=""></p><h3 id="第二步：Instantiating-defaults"><a href="#第二步：Instantiating-defaults" class="headerlink" title="第二步：Instantiating defaults"></a>第二步：Instantiating defaults</h3><p>自己定制一个图示，所有参数都自己定制，关于参数的详细定制请见：<a href="http://matplotlib.org/users/customizing.html" target="_blank" rel="noopener">Customizing matplotlib</a>  </p><pre><code># Import everything from matplotlib (numpy is accessible via &apos;np&apos; alias)from pylab import *# Create a new figure of size 8x6 points, using 80 dots per inchfigure(figsize=(8,6), dpi=80)# Create a new subplot from a grid of 1x1subplot(1,1,1)X = np.linspace(-np.pi, np.pi, 256,endpoint=True)C,S = np.cos(X), np.sin(X)# Plot cosine using blue color with a continuous line of width 1 (pixels)plot(X, C, color=&quot;blue&quot;, linewidth=1.0, linestyle=&quot;-&quot;)# Plot sine using green color with a continuous line of width 1 (pixels)plot(X, S, color=&quot;green&quot;, linewidth=1.0, linestyle=&quot;-&quot;)# Set x limitsxlim(-4.0,4.0)# Set x ticksxticks(np.linspace(-4,4,9,endpoint=True))# Set y limitsylim(-1.0,1.0)# Set y ticksyticks(np.linspace(-1,1,5,endpoint=True))# Save figure using 72 dots per inch# savefig(&quot;exercice_2.png&quot;,dpi=72)# Show result on screenshow()</code></pre><p>最后一步保存图示，所用的像素值和上面显示的像素值不一样，需要自己设置。<br><img src="/images/next/plt2.png" alt="">  </p><h3 id="第三步：Changing-colors-and-line-widths"><a href="#第三步：Changing-colors-and-line-widths" class="headerlink" title="第三步：Changing colors and line widths"></a>第三步：Changing colors and line widths</h3><pre><code>...figure(figsize=(10,6), dpi=80)plot(X, C, color=&quot;blue&quot;, linewidth=2.5, linestyle=&quot;-&quot;)plot(X, S, color=&quot;red&quot;,  linewidth=2.5, linestyle=&quot;-&quot;)...</code></pre><p><img src="/images/next/plt3.png" alt="">  </p><h3 id="第四步：Setting-limits"><a href="#第四步：Setting-limits" class="headerlink" title="第四步：Setting limits"></a>第四步：Setting limits</h3><p>上图中的图示感觉太紧凑了，通过对x,y轴设置，可以预留出一些空间，使之看起来清晰一些，如下： </p><pre><code>...xlim(X.min()*1.1, X.max()*1.1)ylim(C.min()*1.1, C.max()*1.1)...</code></pre><p><img src="/images/next/plt4.png" alt="">  </p><p>但是，一个更鲁棒性的版本，我们应该这样写：  </p><pre><code>xmin ,xmax = X.min(), X.max()ymin, ymax = Y.min(), Y.max()dx = (xmax - xmin) * 0.2dy = (ymax - ymin) * 0.2xlim(xmin - dx, xmax + dx)ylim(ymin - dy, ymax + dy)</code></pre><h3 id="第五步：Setting-ticks"><a href="#第五步：Setting-ticks" class="headerlink" title="第五步：Setting ticks"></a>第五步：Setting ticks</h3><p>我们需要将步长设成[+/-pi, +/-pi/2]之间的值，那么需要这样做：  </p><pre><code>...xticks( [-np.pi, -np.pi/2, 0, np.pi/2, np.pi])yticks([-1, 0, +1])...</code></pre><p><img src="/images/next/plt5.png" alt="">  </p><h3 id="第六步：Setting-ticks-labels"><a href="#第六步：Setting-ticks-labels" class="headerlink" title="第六步：Setting ticks labels"></a>第六步：Setting ticks labels</h3><p>上图中已经非常接近了，但还需要将3.142表示成π的形式，这样做：  </p><pre><code>...xticks([-np.pi, -np.pi/2, 0, np.pi/2, np.pi],       [r&apos;$-\pi$&apos;, r&apos;$-\pi/2$&apos;, r&apos;$0$&apos;, r&apos;$+\pi/2$&apos;, r&apos;$+\pi$&apos;])yticks([-1, 0, +1],       [r&apos;$-1$&apos;, r&apos;$0$&apos;, r&apos;$+1$&apos;])...</code></pre><p><img src="/images/next/plt6.png" alt=""></p><h3 id="第七步：Moving-spines"><a href="#第七步：Moving-spines" class="headerlink" title="第七步：Moving spines"></a>第七步：Moving spines</h3><p>这一步需要去掉边框，即形成x,y轴，形成一个坐标系，边框在这里定义成spines,我们把上、右边框移去，左边框右移，下边框上移，即可得到最终的图形。  </p><pre><code>...ax = gca()ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)ax.xaxis.set_ticks_position(&apos;bottom&apos;)ax.spines[&apos;bottom&apos;].set_position((&apos;data&apos;,0))ax.yaxis.set_ticks_position(&apos;left&apos;)ax.spines[&apos;left&apos;].set_position((&apos;data&apos;,0))...</code></pre><p><img src="/images/next/plt7.png" alt="">  </p><h3 id="第八步：adding-legend"><a href="#第八步：adding-legend" class="headerlink" title="第八步：adding legend"></a>第八步：adding legend</h3><p>为图示增加一个图例，我们选择左上角的位置，如下：  </p><pre><code>...plot(X, C, color=&quot;blue&quot;, linewidth=2.5, linestyle=&quot;-&quot;, label=&quot;cosine&quot;)plot(X, S, color=&quot;red&quot;,  linewidth=2.5, linestyle=&quot;-&quot;, label=&quot;sine&quot;)legend(loc=&apos;upper left&apos;)...</code></pre><p><img src="/images/next/plt8.png" alt="">  </p><h3 id="第九步：Annotate-some-points"><a href="#第九步：Annotate-some-points" class="headerlink" title="第九步：Annotate some points"></a>第九步：Annotate some points</h3><p>比如我要在图中注释2/3pi,该怎么做呢？  </p><pre><code>t = 2*np.pi/3plot([t,t],[0,np.cos(t)], color =&apos;blue&apos;, linewidth=2.5, linestyle=&quot;--&quot;)scatter([t,],[np.cos(t),], 50, color =&apos;blue&apos;)annotate(r&apos;$\sin(\frac{2\pi}{3})=\frac{\sqrt{3}}{2}$&apos;,         xy=(t, np.sin(t)), xycoords=&apos;data&apos;,         xytext=(+10, +30), textcoords=&apos;offset points&apos;, fontsize=16,         arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))plot([t,t],[0,np.sin(t)], color =&apos;red&apos;, linewidth=2.5, linestyle=&quot;--&quot;)scatter([t,],[np.sin(t),], 50, color =&apos;red&apos;)annotate(r&apos;$\cos(\frac{2\pi}{3})=-\frac{1}{2}$&apos;,         xy=(t, np.cos(t)), xycoords=&apos;data&apos;,         xytext=(-90, -50), textcoords=&apos;offset points&apos;, fontsize=16,         arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;arc3,rad=.2&quot;))...</code></pre><p><img src="/images/next/plt9.png" alt="">  </p><h3 id="第十步：魔鬼在于细节"><a href="#第十步：魔鬼在于细节" class="headerlink" title="第十步：魔鬼在于细节"></a>第十步：魔鬼在于细节</h3><p>从上幅图中可以看出，整个图示已经够美观了，但是仔细一看，却发现线压字了，所以我们需要把字凸显出来，让线隐没下去。希望我们在这方面能够向处女座的人多学学。^_^  </p><pre><code>...for label in ax.get_xticklabels() + ax.get_yticklabels():    label.set_fontsize(16)    label.set_bbox(dict(facecolor=&apos;white&apos;, edgecolor=&apos;None&apos;, alpha=0.65 ))...</code></pre><p><img src="/images/next/plt10.png" alt="">  </p><p>OK，到这里为止，10步就把一个图完完整整表现出来了，原作者写这个入门手册也就是为了抛砖引玉，后面怎么举一反三就靠自己了，更多的图例可以看原手册，这里不做过多描述。另外，<a href="http://matplotlib.org/gallery.html" target="_blank" rel="noopener">matplotlib gallery</a>有很多精美的图示，点开就可以看见源码，我们以后如果遇到相关图示，可以做一个简单的参考。还有邮件列表，<a href="https://lists.sourceforge.net/lists/listinfo/matplotlib-users" target="_blank" rel="noopener">user mailing</a>。下面看看画图中常用的线的风格、标志符号和颜色值的表示。<br><strong>line style or marker:</strong>  </p><p><img src="/images/next/lines.PNG" alt="">  </p><p><strong>color</strong><br>标准的颜色值共有8种，如下：  </p><p><img src="/images/next/color.PNG" alt="">  </p><p>但是指定颜色值的方式很灵活，《matplotlib for Python developer》中给出了四种方式：  </p><ul><li>用全名，如‘yellow’或缩写（缩写只有上表中的8种）</li><li>十六进制表示，如紫色用#800080表示，更全的信息详见<a href="http://baike.baidu.com/view/644772.htm" target="_blank" rel="noopener">十六进制颜色码</a></li><li>RGB或RGBA元组，如(1,0,1,1)，关于如何转换RGB颜色，我还没搞懂，如有知晓，请告知。</li><li>灰度值表示，如’0.7’，只针对某一种颜色进行变换。</li></ul><p>OK，就记录到这里，更详尽的还请见<a href="http://www.loria.fr/~rougier/teaching/matplotlib/" target="_blank" rel="noopener">matplotlib tutorial</a>。此处有一个非常全的视频讲Numpy和matplotlib &gt;&gt;&gt;&gt; <a href="http://www.youtube.com/watch?v=3Fp1zn5ao2M&amp;feature=plcp" target="_blank" rel="noopener">introductory tutorial on Numpy and matplotlib</a></p><p><strong>参考：</strong><br><a href="http://matplotlib.org/index.html" target="_blank" rel="noopener">matplotlib</a><br>matplotlib for Python developer</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> matplotlib </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Numpy——强大的科学计算器</title>
      <link href="/2014/09/24/tech/python/Numpy%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%99%A8/"/>
      <url>/2014/09/24/tech/python/Numpy%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="简单概念"><a href="#简单概念" class="headerlink" title="简单概念"></a>简单概念</h3><p><strong>Numpy 是Python中用于科学计算的一个基本的包(Package)</strong>，下面来看看Numpy的官方定义：<br>NumPy is the fundamental package for scientific computing with Python. It contains among other things:  </p><ul><li>a powerful N-dimensional array object  </li><li>sophisticated (broadcasting) functions</li><li>tools for integrating C/C++ and Fortran code</li><li>useful linear algebra, Fourier transform, and random number capabilities</li></ul><p><strong>简单的理解Numpy就是</strong>：其为数值定义了特定的结构和规则，方便对不同的数值，甚至复杂的数值进行运算（这里说的数值是广义上的），如多维数组结构等。</p><a id="more"></a>  <p><strong>Numpy提供了两种基本的对象</strong>：ndarray(N-dimensional array object)和ufunc(universal function object)。ndarray是存储单一数据类型的多维数组，而ufunc则是能够对数组进行处理的函数。</p><p>在Python的标准库，已经有相应的结构来处理数组值的计算，如list，array，那么为什么还需要Numpy？原因就是：<br>1）、list中可以存放任何对象，因此list中所保存的是对象的指针，这样为了保存一个简单的[1,2,3]，需要3个指针和三个整数对象，对于数值计算这种结构显然是比较浪费内存和CPU计算时间的。<br>2）、array模块直接保存数值，和C语言的一维数数组比较类似，但其不支持多维数组，也没有各种运算函数，因此不适合做数值计算。<br>3）、Numpy定义的运算函数ufunc可以在数组和矩阵之间互相转换，可以做到和matlab一般计算游刃有余。</p><p>标准的Python库没有Numpy，需要额外安装，这里推荐一个Python shell：<a href="http://ipython.org/" target="_blank" rel="noopener">ipython</a>，其整合了多个库，不用额外去安装，具体都有些什么库，可以看看知乎的一个回答：<a href="Python 常用的标准库以及第三方库有哪些？">Python 常用的标准库以及第三方库有哪些？</a>，一般我们常用的两个库是Numpy和matplotlib，在我的眼中，这两个加起来就是matlab。另外在推荐一个方便进行调试的IDE：JetBrains开发的<a href="http://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">Pycharm</a>，个人感觉非常nice。</p><h3 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h3><p><strong>Numpy的主要对象是同构的多维数组</strong>（homogeneous multidimensional array），每个元素用序号进行标记，在Numpy中，维叫做轴，维中的行列值对应轴中叫做rank，如一个二维数组：[[1,0,1], [0,1,2]]有两个轴，第一个轴的rank是2，第二轴的rank是3。（和矩阵对应，Numpy提供了数组和矩阵之间的转化方式）</p><p><strong>Numpy中的数组类型叫做ndarray</strong>，也可以看成是array，只是环境不同，叫法不一样而已，它和Python标准库中的array不一样，array.array只处理一维的数组和少量的函数，而Numpy.array提供多维数组运算，同时还提供多种函数进行数组相关的数值计算，如下：<br>ndarry.ndim:数组轴(维)的数量，如二维数组就为2<br>ndarray.shape:数组的维，如3行2列就为（3,2）<br>ndarray.size:数组中总的元素个数<br>ndarray.dtype:数组中元素的类型<br>ndarray.itemsize:数组中元素的字节类型，如int类型就为4<br>ndarray.data:数组的缓存，一般是用不到的  </p><p>由于用的不多，且时间有限，尚无法总结出其主要的操作，我们知道它是干什么用的就可以了，以后如果用到，直接查看其用户手册<a href="http://wiki.scipy.org/Tentative_NumPy_Tutorial" target="_blank" rel="noopener">Numpy tutorial</a>即可。这里有一个学习的网址：<a href="http://sebug.net/paper/books/scipydoc/numpy_intro.html" target="_blank" rel="noopener">Numpy introduction</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JabRef、Pycharm显示乱码的解决方法（大同小异）</title>
      <link href="/2014/09/23/tech/python/%E5%87%A0%E4%B8%AA%E6%96%87%E4%BB%B6%E6%98%BE%E7%A4%BA%E4%B9%B1%E7%A0%81%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%88%E5%A4%A7%E5%90%8C%E5%B0%8F%E5%BC%82%EF%BC%89/"/>
      <url>/2014/09/23/tech/python/%E5%87%A0%E4%B8%AA%E6%96%87%E4%BB%B6%E6%98%BE%E7%A4%BA%E4%B9%B1%E7%A0%81%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%EF%BC%88%E5%A4%A7%E5%90%8C%E5%B0%8F%E5%BC%82%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p><strong>JabRef：</strong>一款和CTex配套使用的文献管理软件，就像Endnote和Word配套一样，在写Abstract或review时，其默认只能输入英文，输入中文则乱码，解决办法：<br>first set Options||Preferences||General&gt;&gt;Default Encoding as UTF8<br>then set Options||Preferences||Appearance&gt;&gt;Set table font as simsun (or any other Chinese fonts) </p><p>效果非常好。如果还有问题，可将Entry Preview里改为<font face="simsun">  </font></p><p><strong>Pycharm：</strong>一款方便调试，并集成了多个库的Python IDE，个人感觉非常好用，如C\C++的VS和Codeblock，java的eclipse，安装默认不支持中文编码，在注释的时候出现乱码，解决办法：<br>File-settings-file and code templates-python script中改成：<br># -<em>- coding: utf-8 -</em>-<br>然后File-settings-file encoding改成UTF-8编码  </p><p>设置字体大小，行号<br>File-&gt;Settings-&gt;Editor-&gt;Colors &amp; Font -&gt; Font</p><p>File-&gt;Settings-&gt;Editor-&gt;Apperance -&gt; 选上Show line numbers</p><p>总结：一般这种编码乱码问题，都是由于编码不支持造成的，找到相关地方进行修改就可以了，一般都会在Setting里面。关于中英文编码问题，详见这篇文章，写得非常nice。<br><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000" target="_blank" rel="noopener">字符编码</a></p>]]></content>
      
      
      <categories>
          
          <category> 05 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 乱码 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python编码中从容易出错的两个小问题中看其编码规范</title>
      <link href="/2014/09/21/tech/python/Python%E7%BC%96%E7%A0%81%E4%B8%AD%E4%B8%A4%E4%B8%AA%E5%AE%B9%E6%98%93%E5%87%BA%E9%94%99%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%B0%8F%E9%97%AE%E9%A2%98%E7%9C%8B%E5%85%B6%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/"/>
      <url>/2014/09/21/tech/python/Python%E7%BC%96%E7%A0%81%E4%B8%AD%E4%B8%A4%E4%B8%AA%E5%AE%B9%E6%98%93%E5%87%BA%E9%94%99%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%B0%8F%E9%97%AE%E9%A2%98%E7%9C%8B%E5%85%B6%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83/</url>
      
        <content type="html"><![CDATA[<p>最近在写Python的一些小程序，由于没有系统地学习过Python的语法及编码风格，在编码规范上栽了几个跟头。先看看我遇到的这两个问题，接着引出Python的编码规范。  </p><h3 id="缩进问题"><a href="#缩进问题" class="headerlink" title="缩进问题"></a>缩进问题</h3><p>缩进是Python中最重要的，也是最容易出错的细节，这个对于每个初学者都要牢记于心并养成习惯，我的问题出在，从一个文本文档拷贝一段代码到一个IDE的时候，出现了如下的两个错误。 </p><pre><code>IndentationError: unindent does not match any outer indentation level IndentationError:expected an indented block  </code></pre><a id="more"></a>  <p>这两个问题就是提醒你要缩进，但是我明明缩进了，为什么还不行，原因就是拷贝的代码的缩进是用空格，缩进不统一（用肉眼完全看不出来），另外一个就是IDE它本身加了一些功能让你方便写代码，像自动缩进，换行等，拷贝的代码本身也拷贝了文件的格式，所以拷贝过来的代码IDE不能识别，不知道为什么不能识别，按理说删除空格换成Tab就应该可以了，但是不管怎么弄都不行，后来就只得重新编写代码才解决这个问题。</p><h3 id="Python库import的导入顺序有讲究"><a href="#Python库import的导入顺序有讲究" class="headerlink" title="Python库import的导入顺序有讲究"></a>Python库import的导入顺序有讲究</h3><p>库的导入有两种方式：<br>1）并排导入：</p><pre><code>import module1,module2,module3.......  </code></pre><p>2）顺序导入： </p><pre><code>import module1  import module2  :  import moduleN  </code></pre><p>显然第二种看着要清爽，但是由于Python库众多，什么库应该先导入，就非常有讲究。我的第二个问题就是这个。先来看看一个小例子：  </p><p><img src="/images/next/importA.PNG" alt="">    </p><p><img src="/images/next/importB.PNG" alt="">   </p><p>这上面导入的两个库的顺序不一样，完全出现不同的结果，可以看到，正确的版本是先导入第三方的库，在导入标准库，但是。。。但是Python的官方编码规范中要求的顺序是：     </p><ul><li>python 标准库模块</li><li>python 第三方模块</li><li>应用程序自定义模块  </li></ul><p>这就让人很匪夷所思了，这里先mark一下，以后遇到在进一步补上，但是我决定以后的做法是，用谁，谁最后导入，这里可能存在着覆盖的问题。  </p><p>好了，上面是我遇到的两个小问题，希望自己牢记，不要再犯同样的错误。关于Python的编码规范，更多的详见<a href="http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/" target="_blank" rel="noopener">Google的官方的风格指南:</a><br>其中，我挑几点记录一下：  </p><p>1、不要在行尾加分号<br>2、一行太长，不要加\换行，这和C++这些不同，Python用()支持直接换行，如：  </p><pre><code>foo = long_function_name(      var_one, var_two, var_three,      var_four)  </code></pre><p>3、括号内不要有空格，如：  </p><pre><code>Yes: spam(ham[1], {eggs: 2}, [])No： spam( ham[ 1 ], { eggs: 2 }, [ ] )  </code></pre><p>4、一般’=’或比较符号（’&lt;’’=’’&gt;’）两边我们都会加空格，但这里特别注意一下：<br>当’=’用于指示关键字参数或默认参数值时, 不要在其两侧使用空格，如：  </p><pre><code>Yes: def complex(real, imag=0.0): return magic(r=real, i=imag)No:  def complex(real, imag = 0.0): return magic(r = real, i = imag)</code></pre><p>5、一般在C、C++程序中我们都会这样写，看着舒服极了：  </p><pre><code>foo       = 1000  # commentlong_name = 2     # comment that should not be aligneddictionary = {    &quot;foo&quot;      : 1,    &quot;long_name&quot;: 2,}</code></pre><p>但最好别这样写，这样写就行：  </p><pre><code>foo = 1000  # commentlong_name = 2  # comment that should not be aligneddictionary = {    &quot;foo&quot;: 1,    &quot;long_name&quot;: 2,}</code></pre><p>6、大部分.py文件不必以#!作为文件的开始. 根据 PEP-394 , 程序的main文件应该以 #!/usr/bin/python2或者 #!/usr/bin/python3开始.  </p><p>(#!)叫做Shebang(也叫Hashbang),一般在科学计算中，类Unix操作系统的程序载入器会分析Shebang后的内容，将这些内容作为解释器指令，并调用该指令, 并将载有Shebang的文件路径作为该解释器的参数. 例如, 以指令#!/bin/sh开头的文件在执行时会实际调用/bin/sh程序.)  </p><p>(#!)先用于帮助内核找到Python解释器, 但是在导入模块时, 将会被忽略. 因此只有被直接执行的文件中才有必要加入#!.  </p><p>7、使用文档字符串对模块、函数或方法进行注释，并在必要时增加行内注释，这一条决定着编写大型模块程序时，不会被搞得晕头转向，也不会让别人看自己程序时摸不清头脑。具体怎么做见下面这个模板即可。  </p><pre><code>def fetch_bigtable_rows(big_table, keys, other_silly_variable=None):&quot;&quot;&quot;Fetches rows from a Bigtable.Retrieves rows pertaining to the given keys from the Table instancerepresented by big_table.  Silly things may happen ifother_silly_variable is not None.Args:    big_table: An open Bigtable Table instance.    keys: A sequence of strings representing the key of each table row        to fetch.    other_silly_variable: Another optional variable, that has a much        longer name than the other args, and which does nothing.Returns:    A dict mapping keys to the corresponding table row data    fetched. Each row is represented as a tuple of strings. For    example:    {&apos;Serak&apos;: (&apos;Rigel VII&apos;, &apos;Preparer&apos;),     &apos;Zim&apos;: (&apos;Irk&apos;, &apos;Invader&apos;),     &apos;Lrrr&apos;: (&apos;Omicron Persei 8&apos;, &apos;Emperor&apos;)}    If a key from the keys argument is missing from the dictionary,    then that row was not found in the table.Raises:    IOError: An error occurred accessing the bigtable.Table object.&quot;&quot;&quot;pass</code></pre><p>8、如果一个类不继承自其他类，就应该显示地从object继承，嵌套类也一样，继承自 object 是为了使属性(properties)正常工作, 并且这样可以保护你的代码, 使其不受Python 3000的一个特殊的潜在不兼容性影响. 这样做也定义了一些特殊的方法, 这些方法实现了对象的默认语义, 包括 __new__, __init__, __delattr__, __getattribute__, __setattr__, __hash__, __repr__, and __str__ .如：  </p><pre><code>class SampleClass(object):     pass class OuterClass(object):     class InnerClass(object):         pass class ChildClass(ParentClass):     &quot;&quot;&quot;Explicitly inherits from another class already.&quot;&quot;&quot;</code></pre><p>9、命名，这个应该是比较重要，好的命名在代码编写过程中，或是在后期调试，维护的过程中都会给人特别舒服的感觉。<br>Tip：  </p><pre><code>module_name, package_name, ClassName, method_name, ExceptionName, function_name, GLOBAL_VAR_NAME, instance_var_name, function_parameter_name,local_var_name  </code></pre><p>应该避免的名称</p><ul><li>单字符名称, 除了计数器和迭代器.</li><li>包/模块名中的连字符(-)</li><li>双下划线开头并结尾的名称(Python保留, 例如__init__)</li></ul><p>命名约定  </p><ul><li>所谓”内部(Internal)”表示仅模块内可用, 或者, 在类内是保护或私有的.</li><li>用单下划线(_)开头表示模块变量或函数是protected的(使用import * from时不会包含).</li><li>用双下划线(__)开头的实例变量或方法表示类内私有.</li><li>将相关的类和顶级函数放在同一个模块里. 不像Java, 没必要限制一个类一个模块.</li><li>对类名使用大写字母开头的单词(如CapWords, 即Pascal风格), 但是模块名应该用小写加下划线的方式(如lower_with_under.py). 尽管已经有很多现存的模块使用类似于CapWords.py这样的命名, 但现在已经不鼓励这样做, 因为如果模块名碰巧和类名一致, 这会让人困扰.  </li></ul><p><strong>Python 之父Guido推荐的规范</strong><br><img src="/images/next/Guido.PNG" alt="">  </p><p>10、即使是一个打算被用作脚本的文件, 也应该是可导入的. 并且简单的导入不应该导致这个脚本的主功能(main functionality)被执行, 这是一种副作用. 主功能应该放在一个main()函数中.</p><p>在Python中, pydoc以及单元测试要求模块必须是可导入的. 你的代码应该在执行主程序前总是检查 if __name__ == ‘__main__‘ , 这样当模块被导入时主程序就不会被执行.如：  </p><pre><code>def main():  ...if __name__ == &apos;__main__&apos;:    main()</code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python入门笔记</title>
      <link href="/2014/09/20/tech/python/Python%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/"/>
      <url>/2014/09/20/tech/python/Python%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><strong>特殊符号的打印方法</strong></p><p>打印单引号：print (“‘’”)<br>打印双引号：print (‘“”‘)<br>打印换行符：print (‘\n’)<br>打印反斜杠：print (‘\‘)</p><p><strong>总结：</strong>一般情况下单引号和双引号的作用都是相同的.</p><a id="more"></a>  <p><strong>Python支持复数的运算:</strong><br>其支持两种表示方法：<br>1、a = 1 + 5j<br>2、a = complex(1, 5)</p><p>复数也支持数学运算：<br>a = 2 + 3j<br>b = 4 + 3j<br>a + b = 6 + 6j</p><p>函数 <strong>id</strong> 以值或变量为参数，返回值是一整数，表示值或变量的唯一标识符，每个值或变量在内存中都有唯一的id值，其与在内存中的位置有关。<br>e.g: id(123) = 11602164</p><p>Python 允许定义单行的小函数，如lambda函数，叫做匿名函数<br>labmda  参数：表达式<br>g = labmda x, y : x + y<br>g(3,4) = 7<br>(labmda x,y = 0,z=0:x+y+z)(3,5,6) = 14<br>关于匿名函数，这里有一篇比较好的文章：<br><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/0013868198760391f49337a8bd847978adea85f0a535591000" target="_blank" rel="noopener">http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/0013868198760391f49337a8bd847978adea85f0a535591000</a><br>匿名函数也是一个函数对象，可以将其赋给一个变量，也可以作为函数的返回值。</p><p>Python对大小写比较敏感<br>如果字符串里面有很多字符都需要转义，就需要加很多\，为了简化，Python还允许用r’’表示’’内部的字符串默认不转义</p><p><strong>Python的字符编码：</strong>这里有一篇比较优秀的文章：<br><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000" target="_blank" rel="noopener">http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000</a></p><p>由于Python源代码也是一个文本文件，所以，当你的源代码中包含中文的时候，在保存源代码时，就需要务必指定保存为UTF-8编码。当Python解释器读取源代码时，为了让它按UTF-8编码读取，我们通常在文件开头写上这两行：</p><pre><code>#!/usr/bin/env python# -*- coding: utf-8 -*-</code></pre><p>要定义一个只有1个元素的tuple，如果你这么定义：<br>t = (1)</p><p>定义的不是tuple，是1这个数！这是因为括号()既可以表示tuple，又可以表示数学公式中的小括号，这就产生了歧义，因此，Python规定，这种情况下，按小括号进行计算，计算结果自然是1。</p><p>所以，只有1个元素的tuple定义时必须加一个逗号,，来消除歧义：<br>t = (1,)</p><p>从<strong>raw_input()</strong>读取的内容永远以字符串的形式返回。</p><p>set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。</p><p>在函数调用中，如果参数不对，会抛出”TypeError”的错误，但如果参数类型不对，则不会进行任何操作，所以，需要人为的对可能发生的错误进行处理，使用内置函数isinstance可以解决。如：<br>对参数类型做检查，只允许整数和浮点数类型的参数。数据类型检查可以用内置函数isinstance实现：  </p><pre><code>def my_abs(x):      if not isinstance(x, (int, float)):          raise TypeError(&apos;bad operand type&apos;)      if x &gt;= 0:          return x      else:          return -x  </code></pre><p>函数可以同时返回多个值，但其实就是一个<strong>tuple</strong>。</p><p><strong>Python中的参数组合：</strong><br>在Python中定义函数，可以用必选参数、默认参数、可变参数和关键字参数，这4种参数都可以一起使用，或者只用其中某些，但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数和关键字参数。</p><p><strong>Python中的高阶函数：</strong><br>能够接受函数作为参数的函数，还可以把函数作为结果值返回，函数式编程就是指这种高度抽象的编程范式。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
